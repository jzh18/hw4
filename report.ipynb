{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement distributed training based on the Needle framework in our final project. In distributed training, the workload to train a model is split up and shared among multiple devices like GPUs, called nodes. These nodes work in parallel to speed up model training. The two main types of distributed training are data parallelism and model parallelism. In short, data parallelism divides the training data into partitions; model parallelism segments the model into different parts that can run concurrently in different nodes [1]. This project implmements the data parallism apporach. We'll elaborate a bit more about data parallelism in the following sections.\n",
    "\n",
    "In data parallelism, the training data is divided into partitions, where the number of partitions is equal to the total number of available nodes. The partitions are assigned to the available nodes.\n",
    "The model is copied in each of these nodes and each nodes operates on its own subset of the partition. Each node calculates the gradients of the model parameters independently. The calculated gradients of the nodes are aggragated to obtain the average gradients. Finally, each node updates the model parameters using the average gradients. \n",
    "\n",
    "Here we also give a brief explanation of the mathematical theory of data parallelism. Let $w$ be the parameters of the model; $\\frac{\\delta{L}}{\\delta{w}}$ is the original gradients of the batch of size $n$; $l_i$ is the loss for data point $i$ and $k$ is the number of nodes. Then we have\n",
    "$$\n",
    "\\frac{\\delta{L}}{\\delta{w}}=\\frac{\\delta[\\frac{1}{n}\\sum_{i=1}^{n}l_i]}{\\delta{w}} \\\\\n",
    "                              =\\frac{1}{n}\\sum_{i=1}^{n}\\frac{\\delta{l_i}}{\\delta{w}} \\\\\n",
    "                              =\\frac{m_1}{n}\\frac{\\frac{1}{m_1}\\sum_{i=1}^{m_1}l_i}{\\delta{w}} \n",
    "                               +\\frac{m_2}{n}\\frac{\\frac{1}{m_2}\\sum_{i=m_1+1}^{m_1+m2}l_i}{\\delta{w}}\n",
    "                               + \\dots\n",
    "                               + \\frac{m_k}{n}\\frac{\\frac{1}{m_k}\\sum_{i=m_{k-1}+1}^{m_{k-1}+m_{k}}l_i} {\\delta{w}} \\\\\n",
    "                              =\\frac{m_1}{n}\\frac{\\delta{l_1}}{\\delta{w}}+\\frac{m_2}{n}\\frac{\\delta{l_2}}{\\delta{w}}\n",
    "                              +\\dots+\\frac{m_k}{n}\\frac{\\delta{l_k}}{\\delta{w}}\n",
    "$$\n",
    "where $m_k$ is the number of data points assigned to node $k$, and \n",
    "$$\n",
    "m_1+m_2+\\dots+m_{k}=n\n",
    "$$\n",
    "If $m_1=m_2=\\dots=m_k=\\frac{n}{k}$, we have\n",
    "$$\n",
    "\\frac{\\delta{L}}{\\delta{w}}=\\frac{1}{k}[\\frac{\\delta{l_1}}{\\delta{w}}+\\frac{\\delta{l_2}}{\\delta{w}}+\\dots+\\frac{\\delta{l_k}}{\\delta{w}}]\n",
    "$$\n",
    "where $\\frac{\\delta{l_k}}{\\delta{w}}$ means the gradients calculated by node $k$ based on the data points $\\{m_{k-1}+1,m_{k-1}+2,\\dots,m_{k-1}+m_k\\}$.\n",
    "According to the above equation, we could know that the average gradients of all the nodes are equal to the original gradients [2]. \n",
    "\n",
    "We have two implementations of distributed trainig, which are based on different communication frameworks, i.e., mpi4py [3] and nccl [4]. The source code of the project can be found here: https://github.com/jzh18/hw4/tree/ddp_nccl. We intorduce these two implementations in section 2 and section 3 respectively.\n",
    "\n",
    "To test distributed training, you need to run this notebook with multiple GPUs. Run  `nvidia-smi` to check how many GPUs are available on your machine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jan  9 16:26:47 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.161.03   Driver Version: 470.161.03   CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM...  On   | 00000000:47:00.0 Off |                    0 |\n",
      "| N/A   26C    P0    53W / 400W |      0MiB / 40536MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100-SXM...  On   | 00000000:90:00.0 Off |                    0 |\n",
      "| N/A   27C    P0    52W / 400W |      0MiB / 40536MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A100-SXM...  On   | 00000000:B7:00.0 Off |                    0 |\n",
      "| N/A   28C    P0    52W / 400W |      0MiB / 40536MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clone the cod and install necessary packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# %cd /content/drive/MyDrive/\n",
    "\n",
    "# !git clone -b ddp_nccl https://github.com/jzh18/hw4.git\n",
    "# !pip3 install --upgrade --no-deps git+https://github.com/dlsys10714/mugrade.git\n",
    "# !pip3 install pybind11\n",
    "# !pip3 install mpi4py\n",
    "# %cd /content/drive/MyDrive/hw4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the datasets you will be using for this assignment\n",
    "\n",
    "import urllib.request\n",
    "import os\n",
    "\n",
    "!mkdir -p './data/ptb'\n",
    "# Download Penn Treebank dataset\n",
    "ptb_data = \"https://raw.githubusercontent.com/wojzaremba/lstm/master/data/ptb.\"\n",
    "for f in ['train.txt', 'test.txt', 'valid.txt']:\n",
    "    if not os.path.exists(os.path.join('./data/ptb', f)):\n",
    "        urllib.request.urlretrieve(ptb_data + f, os.path.join('./data/ptb', f))\n",
    "\n",
    "# Download CIFAR-10 dataset\n",
    "if not os.path.isdir(\"./data/cifar-10-batches-py\"):\n",
    "    urllib.request.urlretrieve(\"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\", \"./data/cifar-10-python.tar.gz\")\n",
    "    !tar -xvzf './data/cifar-10-python.tar.gz' -C './data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Found pybind11: /home/x_huzha/.conda/envs/dlsys/lib/python3.8/site-packages/pybind11/include (found version \"2.10.2\")\n",
      "CMake Warning (dev) at CMakeLists.txt:56 (find_package):\n",
      "  Policy CMP0074 is not set: find_package uses <PackageName>_ROOT variables.\n",
      "  Run \"cmake --help-policy CMP0074\" for policy details.  Use the cmake_policy\n",
      "  command to set the policy and suppress this warning.\n",
      "\n",
      "  Environment variable CUDA_ROOT is set to:\n",
      "\n",
      "    /software/sse/manual/CUDA/11.3.1_465.19.01\n",
      "\n",
      "  For compatibility, CMake is ignoring the variable.\n",
      "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
      "\n",
      "-- Determining NCCL version from /software/sse/manual/CUDA/11.3.1_465.19.01/include/nccl.h...\n",
      "-- NCCL version: 2.11.4\n",
      "\n",
      "-- Found NCCL (include: /software/sse/manual/CUDA/11.3.1_465.19.01/include, library: /software/sse/manual/CUDA/11.3.1_465.19.01/lib64/libnccl.so)\n",
      "-- Found cuda, building cuda backend\n",
      "Mon Jan  9 16:26:49 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.161.03   Driver Version: 470.161.03   CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM...  On   | 00000000:47:00.0 Off |                    0 |\n",
      "| N/A   26C    P0    53W / 400W |      0MiB / 40536MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100-SXM...  On   | 00000000:90:00.0 Off |                    0 |\n",
      "| N/A   27C    P0    52W / 400W |      0MiB / 40536MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A100-SXM...  On   | 00000000:B7:00.0 Off |                    0 |\n",
      "| N/A   28C    P0    52W / 400W |      0MiB / 40536MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n",
      "-- Autodetected CUDA architecture(s):  8.0 8.0 8.0\n",
      "-- Configuring done\n",
      "-- Generating done\n",
      "-- Build files have been written to: /proj/berzelius-2022-9-video/users/x_huzha/hw4/build\n",
      "make[1]: Entering directory '/proj/berzelius-2022-9-video/users/x_huzha/hw4/build'\n",
      "make[2]: Entering directory '/proj/berzelius-2022-9-video/users/x_huzha/hw4/build'\n",
      "make[3]: Entering directory '/proj/berzelius-2022-9-video/users/x_huzha/hw4/build'\n",
      "\u001b[35m\u001b[1mConsolidate compiler generated dependencies of target ndarray_backend_cpu\u001b[0m\n",
      "make[3]: Leaving directory '/proj/berzelius-2022-9-video/users/x_huzha/hw4/build'\n",
      "[ 50%] Built target ndarray_backend_cpu\n",
      "make[3]: Entering directory '/proj/berzelius-2022-9-video/users/x_huzha/hw4/build'\n",
      "make[3]: Leaving directory '/proj/berzelius-2022-9-video/users/x_huzha/hw4/build'\n",
      "[100%] Built target ndarray_backend_cuda\n",
      "make[2]: Leaving directory '/proj/berzelius-2022-9-video/users/x_huzha/hw4/build'\n",
      "make[1]: Leaving directory '/proj/berzelius-2022-9-video/users/x_huzha/hw4/build'\n"
     ]
    }
   ],
   "source": [
    "!make"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Distributed training with mpi4py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we introduce the usage and implementation details of distributed trainig based on mpi4py framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we demostrate how to use distributed training.\n",
    "\n",
    "The training process will take place in different process at the same time, and each process would communicate with each other through Message Passing Interface (MPI) protocol.\n",
    "\n",
    "Let's see how it works.\n",
    "\n",
    "In the file `apps/distribute_training.py`, we use distributed training to train a ResNet9 model. Let's walk through the code in `apps/distribute_training.py` briefly to show how to use distributed training.\n",
    "\n",
    "Firstly, we initialize the distributed training functionality with some arguments like number of GPUs and if enable nccl backend.\n",
    "```\n",
    "parser.add_argument('--nccl', action='store_true', default=False, help='Use nccl or not.')\n",
    "args = parser.parse_args()\n",
    "args.comm = num_gpus\n",
    "rank, size, device = ndl.ddp.init(args)\n",
    "\n",
    "```\n",
    "\n",
    "After that, we load the dataset and partition the dataset according the number of GPUs.\n",
    "```\n",
    "dataset = ndl.data.CIFAR10Dataset(\"data/cifar-10-batches-py\", train=True)\n",
    "train_set, bsz = ndl.ddp.partition_dataset(\n",
    "        dataset, batch_size , num_gpus, device=device, dtype='float32')\n",
    "```\n",
    "\n",
    "Then we create a ResNet model and broadcast the model parameters to all the nodes.\n",
    "```\n",
    "model = ResNet9(device=device, dtype=\"float32\")\n",
    "ndl.ddp.broadcast_parameters(model,args)\n",
    "```\n",
    "\n",
    "Finally, we start distributed trainning, just like normal training process.\n",
    "```\n",
    "for batch in dataloader:\n",
    "    opt.reset_grad()\n",
    "    X, y = batch\n",
    "    X,y = ndl.Tensor(X, device=device), ndl.Tensor(y, device=device)\n",
    "    out = model(X)\n",
    "    correct += np.sum(np.argmax(out.numpy(), axis=1) == y.numpy())\n",
    "    loss = loss_fn(out, y)\n",
    "    total_loss += loss.data.numpy() * y.shape[0]\n",
    "    totnum+=y.shape[0]\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run the distributed training for real. Firstly, use pytorch to find how many gpus available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/x_huzha/.local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "num_of_gpus = torch.cuda.device_count()\n",
    "print(num_of_gpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 3 GPUs here. Now, let's train the ResNet model using distributed training with the 3 GPUs!\n",
    "\n",
    "**Note that the following code will fail if your machine has less than 3 GPUs.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use cuda: 2\n",
      "Use mpi4py backend\n",
      "Use cuda: 1\n",
      "Use mpi4py backend\n",
      "Use cuda: 0\n",
      "Use mpi4py backend\n",
      "partitioned dataset length: 16666\n",
      "partitioned dataset length: 16666\n",
      "partitioned dataset length: 16666\n",
      "0  correct: 0.38173526941077646  loss: [1.7241981]\n",
      "0  correct: 0.3717148685947438  loss: [1.7246463]\n",
      "0  correct: 0.37741509660386413  loss: [1.7312943]\n",
      "Time: 44.50049090385437\n",
      "0.2594503780151206 [3.442177]\n",
      "0.25543021720868836 [3.4752069]\n",
      "0.2604104164166567 [3.4539325]\n"
     ]
    }
   ],
   "source": [
    "!mpiexec -np 3 python apps/distribute_training.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training ResNet for 1 epoch with 3 GPUs takes about 43s. Let's train it with one GPU and see how long it would take."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use cuda: 0\n",
      "Use mpi4py backend\n",
      "partitioned dataset length: 50000\n",
      "0  correct: 0.38548  loss: [1.7017473]\n",
      "Time: 59.53646516799927\n",
      "0.20052 [5.709298]\n"
     ]
    }
   ],
   "source": [
    "!mpiexec -np 1 python apps/distribute_training.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, training resnet for 1 epoch with 1 GPU takes about 60s. By distributed trainig, we reduce the training time for 28% ((60-43)/60)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we show the implementation details of distributed training in our project. Most of the code relevant to distributed training are located in the file `python/needle/ddp.py`.\n",
    "\n",
    "\n",
    "The `DataPartitioner` in the file divides a dataset into multiple partitions with the size specified by users. The code are shown as below:\n",
    "```\n",
    "class DataPartitioner(object):\n",
    "    \"\"\" Partitions a dataset into different chuncks. \"\"\"\n",
    "\n",
    "    def __init__(self, data, sizes=[0.7, 0.2, 0.1], seed=1234):\n",
    "        self.data = data\n",
    "        self.partitions = []\n",
    "        rng = Random()\n",
    "        rng.seed(seed)\n",
    "        data_len = len(data)\n",
    "        indexes = [x for x in range(0, data_len)]\n",
    "        rng.shuffle(indexes)\n",
    "\n",
    "        for frac in sizes:\n",
    "            part_len = int(frac * data_len)\n",
    "            self.partitions.append(indexes[0:part_len])\n",
    "            indexes = indexes[part_len:]\n",
    "\n",
    "    def use(self, partition):\n",
    "        return Partition(self.data, self.partitions[partition])\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "The `broadcast_parameters` function broadcasts the model parameters to all the nodes. The following shows the code of `broadcast_parameters` function:\n",
    "```\n",
    "def broadcast_parameters(model, args, rank=0):\n",
    "    if args.nccl:\n",
    "        // use nccl backend\n",
    "        ...\n",
    "    else:\n",
    "        // use mpi4 py backend\n",
    "        comm = args.comm\n",
    "        for p in model.parameters():\n",
    "            p_data = p.numpy()\n",
    "            p_data = comm.bcast(p_data, root=0)\n",
    "            p.data = ndl.Tensor(p_data, device=p.device, dtype=p.dtype)\n",
    "        \n",
    "\n",
    "```\n",
    "\n",
    "The optimizers in the file `python/needle/optim.py` are modified, too. The optimizers use the all-reduce functionality to aggrate the gradients calculated by each nodes and calculate the mean value of these gradients. The model of each node update the model parameters based on the mean gradients. Part of the code of Adam optimizer are shown below.\n",
    "```\n",
    "if self.args.nccl:\n",
    "    // use nccl as backend\n",
    "    ...\n",
    "else:\n",
    "    // use mpi4py as backend\n",
    "    comm = MPI.COMM_WORLD\n",
    "    world_size = comm.Get_size()\n",
    "    for p in self.params:\n",
    "        if p.grad is None:\n",
    "            continue\n",
    "        sendbuf = np.ascontiguousarray(p.grad.numpy())\n",
    "        recvbuf = np.empty_like(sendbuf, dtype=p.dtype)\n",
    "        comm.Allreduce(sendbuf, recvbuf, op=MPI.SUM)\n",
    "        recvbuf = recvbuf / world_size\n",
    "        p.grad.data = ndl.Tensor(recvbuf, device=p.grad.device, dtype=p.grad.dtype)\n",
    "```\n",
    "\n",
    "\n",
    "In order to select the GPU we want to run the training workloads, we add a function named `SetDevice(int32_t device_id)` in `src/ndarray_backend_cuda.cu`. Users need to specify the device_id when invoking `needle.cuda(device_id)`. For example,\n",
    "`needle.cuda(1)` return a device which represents GPU 1. The code of `SetDevice(int32_t device_id)` are shown below.\n",
    "```\n",
    "void SetDevice(int id)\n",
    "{\n",
    "    mess.localRank=id;\n",
    "    cudaSetDevice(id);\n",
    "}\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Distributed training with NCCL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In section 2, we implemented distributed trainging using MPI communication API. This type of communication is very inconvenient, we need to turn the data into numpy, and use CPU to communicate. It doesn't take advantage of multiple GPUs. Therefore it is essential to use the NVIDIA Collective Communication Library(NCCL), which is developed by NVIDIA official."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The usage are quite similar with Section 2.1. So we won't go through the code here. Let's try the distributed training with NCCL backend. It's quite easy as we just need to specify an argument `--nccl`. Then we are using NCCL as our backend. Make sure that have installed NCCL.\n",
    "\n",
    "**Note that the following code will fail if your machine has less than 3 GPUs.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use cuda: 1\n",
      "Use nccl backend\n",
      "Use cuda: 0\n",
      "Use nccl backend\n",
      "Use cuda: 2\n",
      "Use nccl backend\n",
      "partitioned dataset length: 16666\n",
      "partitioned dataset length: 16666\n",
      "partitioned dataset length: 16666\n",
      "0  correct: 0.3768150726029041  loss: [1.7299722]\n",
      "Time: 43.92278861999512\n",
      "0  correct: 0.37873514940597625  loss: [1.7322209]\n",
      "0  correct: 0.3793951758070323  loss: [1.7277248]\n",
      "0.22974918996759872 [4.400443]\n",
      "0.21810872434897396 [4.6936193]\n",
      "0.22224888995559822 [4.6116395]\n"
     ]
    }
   ],
   "source": [
    "!mpiexec -np 3 python apps/distribute_training.py --nccl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To enable direct communication between GPUs in NCCL, we should crreate a communicator first. In terms of concrete implementation, first we need to call the `ncclGetUniqueId()` function, it will return an ID, which will be used by all processees and threads to synchronize and understand they are part of the same communicator. Then we can use `ncclCommInitRank()` to create the communicator objects. The key issue is that we need to broadcast ID to all participating threads and processes using any CPU communication system. In the original MPI with CUDA program, we can call the CUDA-based MPI API to finish the broadcast. But in our project, we call CUDA program via Python, MPI is also based on Python. As a result, we can't use the CUDA-based MPI API but we can use the Python-based. \n",
    "\n",
    "Our solutions are as follows:\n",
    "\n",
    "1. Python program calls CUDA API, CUDA program gets the ID and returns it to Python.\n",
    "2. Python program calls Python-based MPI API to broadcast the ID.\n",
    "3. All processees and threads get the same ID, calls CUDA API to establish a connection.\n",
    "\n",
    "\n",
    "The relevant codes arre as follows:\n",
    "\n",
    "Python code:\n",
    "```\n",
    "def init():\n",
    "    comm = MPI.COMM_WORLD\n",
    "    size = comm.Get_size()\n",
    "    rank = comm.Get_rank() # call MPI API to get world_size and rank\n",
    "    device = ndl.cuda(rank) # choose different GPUs\n",
    "    print(f'Use cuda: {rank}')\n",
    "\n",
    "    if rank==0:\n",
    "        vec = device.get_id() # get ID\n",
    "    else:\n",
    "        vec = None\n",
    "    vec = comm.bcast(vec, root=0) # broadcast ID\n",
    "\n",
    "    device.init_nccl(vec,rank,size) # establish a connection\n",
    "    return rank, size, device\n",
    "```\n",
    "\n",
    "CUDA code:\n",
    "```\n",
    "struct CudaCommAndStream{\n",
    "    int nRanks,localRank,myRank;\n",
    "    ncclUniqueId id;\n",
    "    ncclComm_t comm;\n",
    "    cudaStream_t s;\n",
    "}mess;\n",
    "void SetDevice(int id) # set different device\n",
    "{\n",
    "    mess.localRank=id;\n",
    "    cudaSetDevice(id);\n",
    "}\n",
    "std::vector<uint8_t> GetId()\n",
    "{\n",
    "    ncclGetUniqueId(&mess.id); # get id \n",
    "    auto vec = std::vector<uint8_t>(reinterpret_cast<uint8_t*>(&mess.id),reinterpret_cast<uint8_t*>(&mess.id) + NCCL_UNIQUE_ID_BYTES); # put id into vector\n",
    "    return vec;\n",
    "}\n",
    "\n",
    "void InitNccl(std::vector<uint8_t> vec,int rank,int size) \n",
    "{\n",
    "    mess.nRanks = size;\n",
    "    mess.myRank = rank;\n",
    "    std::memcpy(&mess.id, vec.data(), vec.size()); # change vector to id\n",
    "    ncclCommInitRank(&mess.comm, mess.nRanks, mess.id, mess.myRank); # establish a connection\n",
    "    cudaStreamCreate(&mess.s);\n",
    "}\n",
    "PYBIND11_MODULE(ndarray_backend_cuda, m) {\n",
    "    ...\n",
    "    m.def(\"set_device\", SetDevice);\n",
    "    m.def(\"get_id\", GetId);\n",
    "    m.def(\"init_nccl\", InitNccl);\n",
    "}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] Distributed training. https://learn.microsoft.com/en-us/azure/machine-learning/concept-distributed-training\n",
    "\n",
    "[2] Data Parallelism VS Model Parallelism in Distributed Deep Learning Training. https://leimao.github.io/blog/Data-Parallelism-vs-Model-Paralelism/\n",
    "\n",
    "[3] mpi4py: https://mpi4py.readthedocs.io/en/stable/mpi4py.html\n",
    "\n",
    "[4] NCCL: https://developer.nvidia.com/nccl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "601b499b2199c856df16ec98313964189b457c31a1c25fec9d7f71bee01fcfcc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
