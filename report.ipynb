{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement distributed training based on the Needle framework in our final project. In distributed training, the workload to train a model is split up and shared among multiple devices like GPUs, called nodes. These nodes work in parallel to speed up model training. The two main types of distributed training are data parallelism and model parallelism. In short, data parallelism divides the training data into partitions; model parallelism segments the model into different parts that can run concurrently in different nodes [1]. This project implmements the data parallism apporach. We'll elaborate a bit more about data parallelism in the following sections.\n",
    "\n",
    "In data parallelism, the training data is divided into partitions, where the number of partitions is equal to the total number of available nodes. The partitions are assigned to the available nodes.\n",
    "The model is copied in each of these nodes and each nodes operates on its own subset of the partition. Each node calculates the gradients of the model parameters independently. The calculated gradients of the nodes are aggragated to obtain the average gradients. Finally, each node updates the model parameters using the average gradients. \n",
    "\n",
    "Here we also give a brief explanation of the mathematical theory of data parallelism. Let $w$ be the parameters of the model; $\\frac{\\delta{L}}{\\delta{w}}$ is the original gradients of the batch of size $n$; $l_i$ is the loss for data point $i$ and $k$ is the number of nodes. Then we have\n",
    "$$\n",
    "\\frac{\\delta{L}}{\\delta{w}}=\\frac{\\delta[\\frac{1}{n}\\sum_{i=1}^{n}l_i]}{\\delta{w}} \\\\\n",
    "                              =\\frac{1}{n}\\sum_{i=1}^{n}\\frac{\\delta{l_i}}{\\delta{w}} \\\\\n",
    "                              =\\frac{m_1}{n}\\frac{\\frac{1}{m_1}\\sum_{i=1}^{m_1}l_i}{\\delta{w}} \n",
    "                               +\\frac{m_2}{n}\\frac{\\frac{1}{m_2}\\sum_{i=m_1+1}^{m_1+m2}l_i}{\\delta{w}}\n",
    "                               + \\dots\n",
    "                               + \\frac{m_k}{n}\\frac{\\frac{1}{m_k}\\sum_{i=m_{k-1}+1}^{m_{k-1}+m_{k}}l_i} {\\delta{w}} \\\\\n",
    "                              =\\frac{m_1}{n}\\frac{\\delta{l_1}}{\\delta{w}}+\\frac{m_2}{n}\\frac{\\delta{l_2}}{\\delta{w}}\n",
    "                              +\\dots+\\frac{m_k}{n}\\frac{\\delta{l_k}}{\\delta{w}}\n",
    "$$\n",
    "where $m_k$ is the number of data points assigned to node $k$, and \n",
    "$$\n",
    "m_1+m_2+\\dots+m_{k}=n\n",
    "$$\n",
    "If $m_1=m_2=\\dots=m_k=\\frac{n}{k}$, we have\n",
    "$$\n",
    "\\frac{\\delta{L}}{\\delta{w}}=\\frac{1}{k}[\\frac{\\delta{l_1}}{\\delta{w}}+\\frac{\\delta{l_2}}{\\delta{w}}+\\dots+\\frac{\\delta{l_k}}{\\delta{w}}]\n",
    "$$\n",
    "where $\\frac{\\delta{l_k}}{\\delta{w}}$ means the gradients calculated by node $k$ based on the data points $\\{m_{k-1}+1,m_{k-1}+2,\\dots,m_{k-1}+m_k\\}$.\n",
    "According to the above equation, we could know that the average gradients of all the nodes are equal to the original gradients [2]. \n",
    "\n",
    "We have two implementations of distributed trainig, which are based on different communication frameworks, i.e., mpi4py [3] and nccl [4]. The source code of the project can be found here: [TODO]. We intorduce these two implementations in section 2 and section 3 respectively.\n",
    "\n",
    "To test distributed training, you need to run this notebook with multiple GPUs. Run  `nvidia-smi` to check how many GPUs are available on your machine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jan  9 15:08:52 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.161.03   Driver Version: 470.161.03   CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM...  On   | 00000000:4E:00.0 Off |                    0 |\n",
      "| N/A   25C    P0    50W / 400W |      0MiB / 40536MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100-SXM...  On   | 00000000:B7:00.0 Off |                    0 |\n",
      "| N/A   28C    P0    53W / 400W |      0MiB / 40536MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A100-SXM...  On   | 00000000:BD:00.0 Off |                    0 |\n",
      "| N/A   28C    P0    51W / 400W |      0MiB / 40536MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clone the cod and install necessary packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# %cd /content/drive/MyDrive/\n",
    "\n",
    "# !git clone https://github.com/jzh18/hw4.git\n",
    "# !pip3 install --upgrade --no-deps git+https://github.com/dlsys10714/mugrade.git\n",
    "# !pip3 install pybind11\n",
    "# !pip3 install mpi4py\n",
    "# %cd /content/drive/MyDrive/hw4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the datasets you will be using for this assignment\n",
    "\n",
    "import urllib.request\n",
    "import os\n",
    "\n",
    "!mkdir -p './data/ptb'\n",
    "# Download Penn Treebank dataset\n",
    "ptb_data = \"https://raw.githubusercontent.com/wojzaremba/lstm/master/data/ptb.\"\n",
    "for f in ['train.txt', 'test.txt', 'valid.txt']:\n",
    "    if not os.path.exists(os.path.join('./data/ptb', f)):\n",
    "        urllib.request.urlretrieve(ptb_data + f, os.path.join('./data/ptb', f))\n",
    "\n",
    "# Download CIFAR-10 dataset\n",
    "if not os.path.isdir(\"./data/cifar-10-batches-py\"):\n",
    "    urllib.request.urlretrieve(\"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\", \"./data/cifar-10-python.tar.gz\")\n",
    "    !tar -xvzf './data/cifar-10-python.tar.gz' -C './data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Found pybind11: /home/x_huzha/.conda/envs/dlsys/lib/python3.8/site-packages/pybind11/include (found version \"2.10.2\")\n",
      "CMake Warning (dev) at CMakeLists.txt:56 (find_package):\n",
      "  Policy CMP0074 is not set: find_package uses <PackageName>_ROOT variables.\n",
      "  Run \"cmake --help-policy CMP0074\" for policy details.  Use the cmake_policy\n",
      "  command to set the policy and suppress this warning.\n",
      "\n",
      "  Environment variable CUDA_ROOT is set to:\n",
      "\n",
      "    /software/sse/manual/CUDA/11.3.1_465.19.01\n",
      "\n",
      "  For compatibility, CMake is ignoring the variable.\n",
      "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
      "\n",
      "-- Determining NCCL version from /software/sse/manual/CUDA/11.3.1_465.19.01/include/nccl.h...\n",
      "-- NCCL version: 2.11.4\n",
      "\n",
      "-- Found NCCL (include: /software/sse/manual/CUDA/11.3.1_465.19.01/include, library: /software/sse/manual/CUDA/11.3.1_465.19.01/lib64/libnccl.so)\n",
      "-- Found cuda, building cuda backend\n",
      "Mon Jan  9 15:10:15 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.161.03   Driver Version: 470.161.03   CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM...  On   | 00000000:4E:00.0 Off |                    0 |\n",
      "| N/A   25C    P0    50W / 400W |      0MiB / 40536MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100-SXM...  On   | 00000000:B7:00.0 Off |                    0 |\n",
      "| N/A   28C    P0    53W / 400W |      0MiB / 40536MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A100-SXM...  On   | 00000000:BD:00.0 Off |                    0 |\n",
      "| N/A   28C    P0    51W / 400W |      0MiB / 40536MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n",
      "-- Autodetected CUDA architecture(s):  8.0 8.0 8.0\n",
      "-- Configuring done\n",
      "-- Generating done\n",
      "-- Build files have been written to: /proj/berzelius-2022-9-video/users/x_huzha/hw4/build\n",
      "make[1]: Entering directory '/proj/berzelius-2022-9-video/users/x_huzha/hw4/build'\n",
      "make[2]: Entering directory '/proj/berzelius-2022-9-video/users/x_huzha/hw4/build'\n",
      "make[3]: Entering directory '/proj/berzelius-2022-9-video/users/x_huzha/hw4/build'\n",
      "\u001b[35m\u001b[1mConsolidate compiler generated dependencies of target ndarray_backend_cpu\u001b[0m\n",
      "make[3]: Leaving directory '/proj/berzelius-2022-9-video/users/x_huzha/hw4/build'\n",
      "make[3]: Entering directory '/proj/berzelius-2022-9-video/users/x_huzha/hw4/build'\n",
      "[ 25%] \u001b[32mBuilding CXX object CMakeFiles/ndarray_backend_cpu.dir/src/ndarray_backend_cpu.cc.o\u001b[0m\n",
      "[ 50%] \u001b[32m\u001b[1mLinking CXX shared module ../python/needle/backend_ndarray/ndarray_backend_cpu.cpython-38-x86_64-linux-gnu.so\u001b[0m\n",
      "make[3]: Leaving directory '/proj/berzelius-2022-9-video/users/x_huzha/hw4/build'\n",
      "[ 50%] Built target ndarray_backend_cpu\n",
      "make[3]: Entering directory '/proj/berzelius-2022-9-video/users/x_huzha/hw4/build'\n",
      "[ 75%] \u001b[34m\u001b[1mBuilding NVCC (Device) object CMakeFiles/ndarray_backend_cuda.dir/src/ndarray_backend_cuda_generated_ndarray_backend_cuda.cu.o\u001b[0m\n",
      "make[3]: Leaving directory '/proj/berzelius-2022-9-video/users/x_huzha/hw4/build'\n",
      "make[3]: Entering directory '/proj/berzelius-2022-9-video/users/x_huzha/hw4/build'\n",
      "[100%] \u001b[32m\u001b[1mLinking CXX shared module ../python/needle/backend_ndarray/ndarray_backend_cuda.cpython-38-x86_64-linux-gnu.so\u001b[0m\n",
      "make[3]: Leaving directory '/proj/berzelius-2022-9-video/users/x_huzha/hw4/build'\n",
      "[100%] Built target ndarray_backend_cuda\n",
      "make[2]: Leaving directory '/proj/berzelius-2022-9-video/users/x_huzha/hw4/build'\n",
      "make[1]: Leaving directory '/proj/berzelius-2022-9-video/users/x_huzha/hw4/build'\n"
     ]
    }
   ],
   "source": [
    "!make"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Distributed training with mpi4py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we introduce the usage and implementation details of distributed trainig based on mpi4py framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we demostrate how to use distributed training.\n",
    "\n",
    "In this project, we tried to create the process similar to what horovod provides. The training process will take place in different process at the same time, and each process would communicate with each other through Message Passing Interface (MPI) protocol.\n",
    "\n",
    "Let's see how it works.\n",
    "\n",
    "In the file `train_resnet.py`, we use distributed training to train a ResNet9 model. Let's walk through the code in `train_resnet.py` briefly to show how to use distributed training.\n",
    "\n",
    "Firstly, import the packages we need.\n",
    "```\n",
    "import sys\n",
    "import numpy as np\n",
    "sys.path.append('./python')\n",
    "sys.path.append('./apps')\n",
    "import needle as ndl\n",
    "from simple_training import train_cifar10, evaluate_cifar10\n",
    "from models import ResNet9\n",
    "```\n",
    "\n",
    "After importing what we need from the basic needle framework, we now can import the ddp (distributed data parallel) from apps:\n",
    "```\n",
    "import apps.ddp as ddp\n",
    "```\n",
    "\n",
    "Here, we are going to initialize everything we need:\n",
    "\n",
    "```\n",
    "# this function initialize the ddp functionality\n",
    "# and return a desired cuda device\n",
    "rank, device = ddp.init()\n",
    "\n",
    "dataset = ndl.data.CIFAR10Dataset(\"data/cifar-10-batches-py\", train=True)\n",
    "\n",
    "#  this function do the partition for dataset and\n",
    "#  returns a dataloader and batch_size for the current process\n",
    "train_dataloader, bsz = ddp.partition_dataset(\n",
    "    dataset=dataset, batch_size=128, device=device, dtype='float32')\n",
    "\n",
    "model = ResNet9(device=device, dtype=\"float32\")\n",
    "#  Before training, we must broadcast the parameters to different process\n",
    "ddp.broadcast_parameters(model)\n",
    "\n",
    "model.train()\n",
    "opt = ndl.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.001)\n",
    "#  After defining the optimizer, we need to call this to\n",
    "#  make the optimizer work for distributed class\n",
    "opt  = ddp.DistributedOptimizer(opt)\n",
    "\n",
    "loss_fn = ndl.nn.SoftmaxLoss()\n",
    "```\n",
    "\n",
    "After the initialization, the training step is very simple. Here we can see that the training process is similar to what we normally do in needle framework:\n",
    "```\n",
    "n_epochs = 1\n",
    "for i in range(n_epochs):\n",
    "    if rank == 0:\n",
    "        print(f'epoch: {i+1}/{n_epochs}')\n",
    "    for batch in train_dataloader:\n",
    "        opt.reset_grad()\n",
    "        X, y = batch\n",
    "        out = model(X)\n",
    "        correct = np.sum(np.argmax(out.numpy(), axis=1) == y.numpy())\n",
    "        loss = loss_fn(out, y)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "```\n",
    "\n",
    "Using pytorch to find how many gpu available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/x_huzha/.local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "num_of_gpus = torch.cuda.device_count()\n",
    "print(num_of_gpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 3 GPUs here. Now, let's train the ResNet model using distributed training with the 3 GPUs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use cuda: 1\n",
      "Use cuda: 0\n",
      "Use cuda: 2\n",
      "partitioned dataset length: 16666\n",
      "partitioned dataset length: 16666\n",
      "partitioned dataset length: 16666\n",
      "0  correct: 0.34255370214808595  loss: [1.850796]\n",
      "0  correct: 0.3422536901476059  loss: [1.8461978]\n",
      "0  correct: 0.35077403096123844  loss: [1.8468144]\n",
      "Time: 30.70683526992798\n",
      "0.21972878915156607 [4.4106684]\n",
      "0.21408856354254172 [4.6044602]\n",
      "0.22146885875435018 [4.3124213]\n"
     ]
    }
   ],
   "source": [
    "!mpiexec -np 3 python apps/distribute_training.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we show the implementation details of distributed training in our project. Most of the code relevant to distributed training are located in the file `apps/ddp.py`.\n",
    "\n",
    "\n",
    "The `DataPartitioner` in the file divides a dataset into multiple partitions with the size specified by users. The code are shown as below:\n",
    "```\n",
    "class DataPartitioner(object):\n",
    "    \"\"\" Partitions a dataset into different chuncks. \"\"\"\n",
    "\n",
    "    def __init__(self, data, sizes=[0.7, 0.2, 0.1], seed=1234):\n",
    "        self.data = data\n",
    "        self.partitions = []\n",
    "        rng = Random()\n",
    "        rng.seed(seed)\n",
    "        data_len = len(data)\n",
    "        indexes = [x for x in range(0, data_len)]\n",
    "        rng.shuffle(indexes)\n",
    "\n",
    "        for frac in sizes:\n",
    "            part_len = int(frac * data_len)\n",
    "            self.partitions.append(indexes[0:part_len])\n",
    "            indexes = indexes[part_len:]\n",
    "\n",
    "    def use(self, partition):\n",
    "        return Partition(self.data, self.partitions[partition])\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "The `broadcast_parameters` function broadcasts the model parameters to all the nodes. The following shows the code of `broadcast_parameters` function:\n",
    "```\n",
    "def broadcast_parameters(model, root_rank=0):\n",
    "    if is_nccl:\n",
    "        // use nccl as backend\n",
    "        ...\n",
    "    else:\n",
    "       // use mpi4py as backend\n",
    "        for p in model.parameters():\n",
    "            p_data = p.numpy()\n",
    "            p_data = comm.bcast(p_data, root=0)\n",
    "            p.data = ndl.Tensor(p_data, device=device, dtype=p.dtype)\n",
    "```\n",
    "\n",
    "The `DistributedOptimizer` class uses the all-reduce functionality to aggrate the gradients calculated by each nodes and calculate the mean of these gradients. The model of each node update the model parameters based on the mean gradients. The code of `DistributedOptimizer` are shown below.\n",
    "```\n",
    "class DistributedOptimizer(ndl.optim.Optimizer):\n",
    "    def __init__(self, opt):\n",
    "        super().__init__(opt.params)\n",
    "        self.opt = opt\n",
    "\n",
    "    def step(self):\n",
    "        self.average_gradients()\n",
    "        self.opt.step()\n",
    "\n",
    "    def average_gradients(self):\n",
    "        for p in self.params:\n",
    "            if p.grad is None:\n",
    "                continue\n",
    "            sendbuf = np.ascontiguousarray(p.grad.numpy())\n",
    "            recvbuf = np.empty_like(sendbuf, dtype=p.dtype)\n",
    "            comm.Allreduce(sendbuf, recvbuf, op=MPI.SUM)\n",
    "            recvbuf = recvbuf / world_size\n",
    "            p.grad.data = ndl.Tensor(recvbuf, device=device, dtype=p.grad.dtype)\n",
    "```\n",
    "\n",
    "\n",
    "In order to select the GPU we want to run the training workloads, we add a function named `SetDevice(int32_t device_id)` in `src/ndarray_backend_cuda.cu`. Users need to specify the device_id when invoking `needle.cuda(device_id)`. For example,\n",
    "`needle.cuda(1)` return a device which represents GPU 1. The code of `SetDevice(int32_t device_id)` are shown below.\n",
    "```\n",
    "void SetDevice(int id)\n",
    "{\n",
    "    mess.localRank=id;\n",
    "    cudaSetDevice(id);\n",
    "}\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Distributed training with NCCL\n",
    "In section two, we implemented distributed trainging using MPI communication API. This type of communication is very inconvenient, we need to turn the data into numpy, and use CPU to communicate. It doesn't take advantage of multiple GPUs. Therefore it is essential to use the NVIDIA Collective Communication Library(NCCL), which is developed by NVIDIA official.\n",
    "\n",
    "To enable direct communication between GPUs in NCCL, we should crreate a communicator first. In terms of concrete implementation, first we need to call the `ncclGetUniqueId()` function, it will return an ID, which will be used by all processees and threads to synchronize and understand they are part of the same communicator. Then we can use `ncclCommInitRank()` to create the communicator objects. The key issue is that we need to broadcast ID to all participating threads and processes using any CPU communication system. In the original MPI with CUDA program, we can call the CUDA-based MPI API to finish the broadcast. But in our project, we call CUDA program via Python, MPI is also based on Python. As a result, we can't use the CUDA-based MPI API but we can use the Python-based. \n",
    "\n",
    "Our solutions are as follows:\n",
    "\n",
    "1. Python program calls CUDA API, CUDA program gets the ID and returns it to Python.\n",
    "2. Python program calls Python-based MPI API to broadcast the ID.\n",
    "3. All processees and threads get the same ID, calls CUDA API to establish a connection.\n",
    "\n",
    "\n",
    "The relevant codes arre as follows:\n",
    "\n",
    "Python code:\n",
    "```\n",
    "def init():\n",
    "    comm = MPI.COMM_WORLD\n",
    "    size = comm.Get_size()\n",
    "    rank = comm.Get_rank() # call MPI API to get world_size and rank\n",
    "    device = ndl.cuda(rank) # choose different GPUs\n",
    "    print(f'Use cuda: {rank}')\n",
    "\n",
    "    if rank==0:\n",
    "        vec = device.get_id() # get ID\n",
    "    else:\n",
    "        vec = None\n",
    "    vec = comm.bcast(vec, root=0) # broadcast ID\n",
    "\n",
    "    device.init_nccl(vec,rank,size) # establish a connection\n",
    "    return rank, size, device\n",
    "```\n",
    "\n",
    "CUDA code:\n",
    "```\n",
    "struct CudaCommAndStream{\n",
    "    int nRanks,localRank,myRank;\n",
    "    ncclUniqueId id;\n",
    "    ncclComm_t comm;\n",
    "    cudaStream_t s;\n",
    "}mess;\n",
    "void SetDevice(int id) # set different device\n",
    "{\n",
    "    mess.localRank=id;\n",
    "    cudaSetDevice(id);\n",
    "}\n",
    "std::vector<uint8_t> GetId()\n",
    "{\n",
    "    ncclGetUniqueId(&mess.id); # get id \n",
    "    auto vec = std::vector<uint8_t>(reinterpret_cast<uint8_t*>(&mess.id),reinterpret_cast<uint8_t*>(&mess.id) + NCCL_UNIQUE_ID_BYTES); # put id into vector\n",
    "    return vec;\n",
    "}\n",
    "\n",
    "void InitNccl(std::vector<uint8_t> vec,int rank,int size) \n",
    "{\n",
    "    mess.nRanks = size;\n",
    "    mess.myRank = rank;\n",
    "    std::memcpy(&mess.id, vec.data(), vec.size()); # change vector to id\n",
    "    ncclCommInitRank(&mess.comm, mess.nRanks, mess.id, mess.myRank); # establish a connection\n",
    "    cudaStreamCreate(&mess.s);\n",
    "}\n",
    "PYBIND11_MODULE(ndarray_backend_cuda, m) {\n",
    "    ...\n",
    "    m.def(\"set_device\", SetDevice);\n",
    "    m.def(\"get_id\", GetId);\n",
    "    m.def(\"init_nccl\", InitNccl);\n",
    "}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage\n",
    "Before running, we should install NCCL and modify CMakeLists.txt to find library path of NCCL in order to compile successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!make"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using nvidia-smi to find how many gpu available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jan  9 07:09:13 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.39       Driver Version: 460.39       CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P100-PCIE...  Off  | 00000000:1A:00.0 Off |                  Off |\n",
      "| N/A   27C    P0    24W / 250W |      2MiB / 16280MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla P100-PCIE...  Off  | 00000000:1B:00.0 Off |                  Off |\n",
      "| N/A   25C    P0    25W / 250W |      2MiB / 16280MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla P100-PCIE...  Off  | 00000000:1D:00.0 Off |                    0 |\n",
      "| N/A   29C    P0    25W / 250W |      2MiB / 16280MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla P100-PCIE...  Off  | 00000000:1E:00.0 Off |                  Off |\n",
      "| N/A   29C    P0    26W / 250W |      2MiB / 16280MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  Tesla P100-PCIE...  Off  | 00000000:3D:00.0 Off |                  Off |\n",
      "| N/A   27C    P0    25W / 250W |      2MiB / 16280MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  Tesla P100-PCIE...  Off  | 00000000:3E:00.0 Off |                  Off |\n",
      "| N/A   25C    P0    26W / 250W |      2MiB / 16280MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  Tesla P100-PCIE...  Off  | 00000000:41:00.0 Off |                  Off |\n",
      "| N/A   50C    P0    40W / 250W |   1165MiB / 16280MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  Tesla P100-PCIE...  Off  | 00000000:42:00.0 Off |                  Off |\n",
      "| N/A   26C    P0    26W / 250W |      2MiB / 16280MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using mpiexec to run the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use cuda: 2\n",
      "Use cuda: 3\n",
      "Use cuda: 7\n",
      "Use cuda: 4\n",
      "Use cuda: 5\n",
      "Use cuda: 1\n",
      "Use cuda: 0\n",
      "Use cuda: 6\n",
      "partitioned dataset length: 6250\n",
      "partitioned dataset length: 6250\n",
      "partitioned dataset length: 6250\n",
      "partitioned dataset length: 6250\n",
      "partitioned dataset length: 6250\n",
      "partitioned dataset length: 6250\n",
      "partitioned dataset length: 6250\n",
      "partitioned dataset length: 6250\n",
      "0  correct: 0.33664  loss: 0  correct: [1.8683679]\n",
      "0  correct: 0  correct: 0.33888  loss: 0.33664  loss: 0  correct: 0.34624  loss: 0.3392  loss: 0  correct: [1.8647475]\n",
      "[1.8663205]\n",
      "0.33904  loss: [1.8584825]\n",
      "[1.8618884]\n",
      "0  correct: [1.8806661]\n",
      "0.3328  loss: Time: 43.870232820510864\n",
      "[1.8832192]\n",
      "0  correct: 0.33728  loss: [1.8480227]\n",
      "0.24752 [5.1031213]\n",
      "0.25008 [5.1413326]\n",
      "0.25008 0.25232 [5.1249537]\n",
      "[5.0528364]\n",
      "0.24896 [5.10206]\n",
      "0.2496 [4.9337025]\n",
      "0.24448 [5.0527062]\n",
      "0.25712 [5.2262936]\n"
     ]
    }
   ],
   "source": [
    "!mpiexec -np {num_of_gpus} python apps/distribute_training.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare efficiency with simple training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use cuda: 0\n",
      "partitioned dataset length: 50000\n",
      "0  correct: 0.34488  loss: [1.8502939]\n",
      "Time: 49.71820831298828\n",
      "0.2096 [4.8264384]\n"
     ]
    }
   ],
   "source": [
    "!mpiexec -np 1 python apps/distribute_training.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "601b499b2199c856df16ec98313964189b457c31a1c25fec9d7f71bee01fcfcc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
