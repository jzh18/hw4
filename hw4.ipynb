{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10-714 Homework 4\n",
    "\n",
    "In this homework, you will leverage all of the components built in the last three homeworks to solve some modern problems with high performing network structures. We will start by adding a few new ops leveraging our new CPU/CUDA backends. Then, you will implement convolution, and a convolutional neural network to train a classifier on the CIFAR-10 image classification dataset. Then, you will implement recurrent and long-short term memory (LSTM) neural networks, and do word-level prediction language modeling on the Penn Treebank dataset.\n",
    "\n",
    "As always, we will start by copying this notebook and getting the starting code.\n",
    "Reminder: __you must save a copy in drive__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to set up the assignment\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "%cd /content/drive/MyDrive/\n",
    "!mkdir -p 10714\n",
    "%cd /content/drive/MyDrive/10714\n",
    "!git clone https://github.com/dlsys10714/hw4.git\n",
    "%cd /content/drive/MyDrive/10714/hw4\n",
    "\n",
    "!pip3 install --upgrade --no-deps git+https://github.com/dlsys10714/mugrade.git\n",
    "!pip3 install pybind11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Found pybind11: /home/ubuntu/anaconda3/envs/mlsys_hw/lib/python3.8/site-packages/pybind11/include (found version \"2.10.1\")\n",
      "-- Found cuda, building cuda backend\n",
      "Tue Nov 29 09:07:59 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.142.00   Driver Version: 450.142.00   CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            On   | 00000000:00:1E.0 Off |                    0 |\n",
      "| N/A   26C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n",
      "-- Autodetected CUDA architecture(s):  7.5\n",
      "-- Configuring done\n",
      "-- Generating done\n",
      "-- Build files have been written to: /home/ubuntu/repos/mlsys/hw4/build\n",
      "make[1]: Entering directory '/home/ubuntu/repos/mlsys/hw4/build'\n",
      "make[2]: Entering directory '/home/ubuntu/repos/mlsys/hw4/build'\n",
      "make[3]: Entering directory '/home/ubuntu/repos/mlsys/hw4/build'\n",
      "\u001b[35m\u001b[1mConsolidate compiler generated dependencies of target ndarray_backend_cpu\u001b[0m\n",
      "make[3]: Leaving directory '/home/ubuntu/repos/mlsys/hw4/build'\n",
      "make[3]: Entering directory '/home/ubuntu/repos/mlsys/hw4/build'\n",
      "[ 25%] \u001b[32mBuilding CXX object CMakeFiles/ndarray_backend_cpu.dir/src/ndarray_backend_cpu.cc.o\u001b[0m\n",
      "[ 50%] \u001b[32m\u001b[1mLinking CXX shared module ../python/needle/backend_ndarray/ndarray_backend_cpu.cpython-38-x86_64-linux-gnu.so\u001b[0m\n",
      "make[3]: Leaving directory '/home/ubuntu/repos/mlsys/hw4/build'\n",
      "[ 50%] Built target ndarray_backend_cpu\n",
      "make[3]: Entering directory '/home/ubuntu/repos/mlsys/hw4/build'\n",
      "[ 75%] \u001b[34m\u001b[1mBuilding NVCC (Device) object CMakeFiles/ndarray_backend_cuda.dir/src/ndarray_backend_cuda_generated_ndarray_backend_cuda.cu.o\u001b[0m\n",
      "make[3]: Leaving directory '/home/ubuntu/repos/mlsys/hw4/build'\n",
      "make[3]: Entering directory '/home/ubuntu/repos/mlsys/hw4/build'\n",
      "[100%] \u001b[32m\u001b[1mLinking CXX shared module ../python/needle/backend_ndarray/ndarray_backend_cuda.cpython-38-x86_64-linux-gnu.so\u001b[0m\n",
      "make[3]: Leaving directory '/home/ubuntu/repos/mlsys/hw4/build'\n",
      "[100%] Built target ndarray_backend_cuda\n",
      "make[2]: Leaving directory '/home/ubuntu/repos/mlsys/hw4/build'\n",
      "make[1]: Leaving directory '/home/ubuntu/repos/mlsys/hw4/build'\n"
     ]
    }
   ],
   "source": [
    "!make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the datasets you will be using for this assignment\n",
    "\n",
    "import urllib.request\n",
    "import os\n",
    "\n",
    "!mkdir -p './data/ptb'\n",
    "# Download Penn Treebank dataset\n",
    "ptb_data = \"https://raw.githubusercontent.com/wojzaremba/lstm/master/data/ptb.\"\n",
    "for f in ['train.txt', 'test.txt', 'valid.txt']:\n",
    "    if not os.path.exists(os.path.join('./data/ptb', f)):\n",
    "        urllib.request.urlretrieve(ptb_data + f, os.path.join('./data/ptb', f))\n",
    "\n",
    "# Download CIFAR-10 dataset\n",
    "if not os.path.isdir(\"./data/cifar-10-batches-py\"):\n",
    "    urllib.request.urlretrieve(\"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\", \"./data/cifar-10-python.tar.gz\")\n",
    "    !tar -xvzf './data/cifar-10-python.tar.gz' -C './data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To finish setting up the assignment, go ahead and fill in all the code in `python/needle/autograd.py` using your solution code from the previous homework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: ND Backend [10 pts]\n",
    "\n",
    "Fill in the following classes in `python/needle/ops.py`:\n",
    "\n",
    "- `PowerScalar`\n",
    "- `EWiseDiv`\n",
    "- `DivScalar`\n",
    "- `Transpose`\n",
    "- `Reshape`\n",
    "- `BroadcastTo`\n",
    "- `Summation`\n",
    "- `MatMul`\n",
    "- `Negate`\n",
    "- `Log`\n",
    "- `Exp`\n",
    "- `ReLU`\n",
    "- `LogSumExp`\n",
    "- `Tanh` (new)\n",
    "- `Stack` (new)\n",
    "- `Split` (new)\n",
    "\n",
    "Note that for most of these, you already wrote the solutions in the previous homework and you should not need to change your previous solution, however `TanhOp`, `Stack`, and `Split` are newly added. `Stack` concatenates same-sized tensors along a new axis, and `Split` undoes this operation. The gradients of the two operations can be written in terms of each other. We do not directly test `Split`, and only test the backward pass of `Stack` (for which we assume you used `Split`).\n",
    "\n",
    "**Note:** You may want to make your Summation op support sums over multiple axes; you will likely need it for the backward pass of the BroadcastTo op if yours supports broadcasting over multiple axes at a time. However, this is more about ease of use than necessity, and we leave this decision up to you (there are no corresponding tests).\n",
    "\n",
    "**Note:** Depending on your implementations, you may want to ensure that you call `.compact()` before reshaping arrays. (If this is necessary, you will run into corresponding error messages later in the assignment.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.8.13, pytest-7.2.0, pluggy-1.0.0 -- /home/ubuntu/anaconda3/envs/mlsys_hw/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/ubuntu/repos/mlsys/hw4\n",
      "plugins: typeguard-2.13.3, anyio-3.6.2\n",
      "collected 1803 items / 1685 deselected / 118 selected                          \u001b[0m\u001b[1m\n",
      "\n",
      "tests/test_nd_backend.py::test_ewise_fn[cpu-shape0-divide] \u001b[32mPASSED\u001b[0m\u001b[32m        [  0%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_ewise_fn[cpu-shape0-subtract] \u001b[32mPASSED\u001b[0m\u001b[32m      [  1%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_ewise_fn[cpu-shape1-divide] \u001b[32mPASSED\u001b[0m\u001b[32m        [  2%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_ewise_fn[cpu-shape1-subtract] \u001b[32mPASSED\u001b[0m\u001b[32m      [  3%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_ewise_fn[cuda-shape0-divide] \u001b[32mPASSED\u001b[0m\u001b[32m       [  4%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_ewise_fn[cuda-shape0-subtract] \u001b[32mPASSED\u001b[0m\u001b[32m     [  5%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_ewise_fn[cuda-shape1-divide] \u001b[32mPASSED\u001b[0m\u001b[32m       [  5%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_ewise_fn[cuda-shape1-subtract] \u001b[32mPASSED\u001b[0m\u001b[32m     [  6%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_scalar_fn[cpu-shape0-divide] \u001b[32mPASSED\u001b[0m\u001b[32m       [  7%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_scalar_fn[cpu-shape0-subtract] \u001b[32mPASSED\u001b[0m\u001b[32m     [  8%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_scalar_fn[cpu-shape1-divide] \u001b[32mPASSED\u001b[0m\u001b[32m       [  9%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_scalar_fn[cpu-shape1-subtract] \u001b[32mPASSED\u001b[0m\u001b[32m     [ 10%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_scalar_fn[cuda-shape0-divide] \u001b[32mPASSED\u001b[0m\u001b[32m      [ 11%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_scalar_fn[cuda-shape0-subtract] \u001b[32mPASSED\u001b[0m\u001b[32m    [ 11%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_scalar_fn[cuda-shape1-divide] \u001b[32mPASSED\u001b[0m\u001b[32m      [ 12%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_scalar_fn[cuda-shape1-subtract] \u001b[32mPASSED\u001b[0m\u001b[32m    [ 13%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cpu-16-16-16] \u001b[32mPASSED\u001b[0m\u001b[32m               [ 14%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cpu-8-8-8] \u001b[32mPASSED\u001b[0m\u001b[32m                  [ 15%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cpu-1-2-3] \u001b[32mPASSED\u001b[0m\u001b[32m                  [ 16%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cpu-3-4-5] \u001b[32mPASSED\u001b[0m\u001b[32m                  [ 16%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cpu-5-4-3] \u001b[32mPASSED\u001b[0m\u001b[32m                  [ 17%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cpu-16-16-32] \u001b[32mPASSED\u001b[0m\u001b[32m               [ 18%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cpu-64-64-64] \u001b[32mPASSED\u001b[0m\u001b[32m               [ 19%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cpu-72-72-72] \u001b[32mPASSED\u001b[0m\u001b[32m               [ 20%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cpu-72-73-74] \u001b[32mPASSED\u001b[0m\u001b[32m               [ 21%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cpu-74-73-72] \u001b[32mPASSED\u001b[0m\u001b[32m               [ 22%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cpu-128-128-128] \u001b[32mPASSED\u001b[0m\u001b[32m            [ 22%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cuda-16-16-16] \u001b[32mPASSED\u001b[0m\u001b[32m              [ 23%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cuda-8-8-8] \u001b[32mPASSED\u001b[0m\u001b[32m                 [ 24%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cuda-1-2-3] \u001b[32mPASSED\u001b[0m\u001b[32m                 [ 25%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cuda-3-4-5] \u001b[32mPASSED\u001b[0m\u001b[32m                 [ 26%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cuda-5-4-3] \u001b[32mPASSED\u001b[0m\u001b[32m                 [ 27%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cuda-16-16-32] \u001b[32mPASSED\u001b[0m\u001b[32m              [ 27%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cuda-64-64-64] \u001b[32mPASSED\u001b[0m\u001b[32m              [ 28%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cuda-72-72-72] \u001b[32mPASSED\u001b[0m\u001b[32m              [ 29%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cuda-72-73-74] \u001b[32mPASSED\u001b[0m\u001b[32m              [ 30%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cuda-74-73-72] \u001b[32mPASSED\u001b[0m\u001b[32m              [ 31%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cuda-128-128-128] \u001b[32mPASSED\u001b[0m\u001b[32m           [ 32%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_power[cpu-shape0] \u001b[32mPASSED\u001b[0m\u001b[32m                  [ 33%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_power[cpu-shape1] \u001b[32mPASSED\u001b[0m\u001b[32m                  [ 33%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_power[cuda-shape0] \u001b[32mPASSED\u001b[0m\u001b[32m                 [ 34%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_power[cuda-shape1] \u001b[32mPASSED\u001b[0m\u001b[32m                 [ 35%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_log[cpu-shape0] \u001b[32mPASSED\u001b[0m\u001b[32m                    [ 36%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_log[cpu-shape1] \u001b[32mPASSED\u001b[0m\u001b[32m                    [ 37%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_log[cuda-shape0] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 38%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_log[cuda-shape1] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 38%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_exp[cpu-shape0] \u001b[32mPASSED\u001b[0m\u001b[32m                    [ 39%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_exp[cpu-shape1] \u001b[32mPASSED\u001b[0m\u001b[32m                    [ 40%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_exp[cuda-shape0] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 41%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_exp[cuda-shape1] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 42%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_relu[cpu-shape0] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 43%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_relu[cpu-shape1] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 44%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_relu[cuda-shape0] \u001b[32mPASSED\u001b[0m\u001b[32m                  [ 44%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_relu[cuda-shape1] \u001b[32mPASSED\u001b[0m\u001b[32m                  [ 45%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_tanh[cpu-shape0] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 46%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_tanh[cpu-shape1] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 47%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_tanh[cuda-shape0] \u001b[32mPASSED\u001b[0m\u001b[32m                  [ 48%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_tanh[cuda-shape1] \u001b[32mPASSED\u001b[0m\u001b[32m                  [ 49%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_tanh_backward[cpu-shape0] \u001b[32mPASSED\u001b[0m\u001b[32m          [ 50%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_tanh_backward[cpu-shape1] \u001b[32mPASSED\u001b[0m\u001b[32m          [ 50%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_tanh_backward[cuda-shape0] \u001b[32mPASSED\u001b[0m\u001b[32m         [ 51%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_tanh_backward[cuda-shape1] \u001b[32mPASSED\u001b[0m\u001b[32m         [ 52%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_stack[cpu-shape0-0-1] \u001b[32mPASSED\u001b[0m\u001b[32m              [ 53%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_stack[cpu-shape1-0-2] \u001b[32mPASSED\u001b[0m\u001b[32m              [ 54%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_stack[cpu-shape2-2-5] \u001b[32mPASSED\u001b[0m\u001b[32m              [ 55%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_stack[cuda-shape0-0-1] \u001b[32mPASSED\u001b[0m\u001b[32m             [ 55%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_stack[cuda-shape1-0-2] \u001b[32mPASSED\u001b[0m\u001b[32m             [ 56%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_stack[cuda-shape2-2-5] \u001b[32mPASSED\u001b[0m\u001b[32m             [ 57%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_stack_backward[cpu-shape0-0-1] \u001b[32mPASSED\u001b[0m\u001b[32m     [ 58%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_stack_backward[cpu-shape1-0-2] \u001b[32mPASSED\u001b[0m\u001b[32m     [ 59%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_stack_backward[cpu-shape2-2-5] \u001b[32mPASSED\u001b[0m\u001b[32m     [ 60%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_stack_backward[cuda-shape0-0-1] \u001b[32mPASSED\u001b[0m\u001b[32m    [ 61%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_stack_backward[cuda-shape1-0-2] \u001b[32mPASSED\u001b[0m\u001b[32m    [ 61%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_stack_backward[cuda-shape2-2-5] \u001b[32mPASSED\u001b[0m\u001b[32m    [ 62%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_summation[cpu-shape0-None] \u001b[32mPASSED\u001b[0m\u001b[32m         [ 63%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_summation[cpu-shape1-0] \u001b[32mPASSED\u001b[0m\u001b[32m            [ 64%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_summation[cpu-shape2-1] \u001b[32mPASSED\u001b[0m\u001b[32m            [ 65%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_summation[cpu-shape3-2] \u001b[32mPASSED\u001b[0m\u001b[32m            [ 66%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_summation[cuda-shape0-None] \u001b[32mPASSED\u001b[0m\u001b[32m        [ 66%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_summation[cuda-shape1-0] \u001b[32mPASSED\u001b[0m\u001b[32m           [ 67%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_summation[cuda-shape2-1] \u001b[32mPASSED\u001b[0m\u001b[32m           [ 68%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_summation[cuda-shape3-2] \u001b[32mPASSED\u001b[0m\u001b[32m           [ 69%]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tests/test_nd_backend.py::test_summation_backward[cpu-shape0-None] \u001b[32mPASSED\u001b[0m\u001b[32m [ 70%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_summation_backward[cpu-shape1-0] \u001b[32mPASSED\u001b[0m\u001b[32m   [ 71%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_summation_backward[cpu-shape2-1] \u001b[32mPASSED\u001b[0m\u001b[32m   [ 72%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_summation_backward[cpu-shape3-2] \u001b[32mPASSED\u001b[0m\u001b[32m   [ 72%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_summation_backward[cuda-shape0-None] \u001b[32mPASSED\u001b[0m\u001b[32m [ 73%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_summation_backward[cuda-shape1-0] \u001b[32mPASSED\u001b[0m\u001b[32m  [ 74%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_summation_backward[cuda-shape2-1] \u001b[32mPASSED\u001b[0m\u001b[32m  [ 75%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_summation_backward[cuda-shape3-2] \u001b[32mPASSED\u001b[0m\u001b[32m  [ 76%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_broadcast_to[cpu-shape0-shape_to0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 77%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_broadcast_to[cpu-shape1-shape_to1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 77%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_broadcast_to[cuda-shape0-shape_to0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 78%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_broadcast_to[cuda-shape1-shape_to1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 79%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_reshape[cpu-shape0-shape_to0] \u001b[32mPASSED\u001b[0m\u001b[32m      [ 80%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_reshape[cpu-shape1-shape_to1] \u001b[32mPASSED\u001b[0m\u001b[32m      [ 81%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_reshape[cuda-shape0-shape_to0] \u001b[32mPASSED\u001b[0m\u001b[32m     [ 82%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_reshape[cuda-shape1-shape_to1] \u001b[32mPASSED\u001b[0m\u001b[32m     [ 83%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_transpose[cpu-axes0-shape0] \u001b[32mPASSED\u001b[0m\u001b[32m        [ 83%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_transpose[cpu-axes0-shape1] \u001b[32mPASSED\u001b[0m\u001b[32m        [ 84%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_transpose[cpu-axes1-shape0] \u001b[32mPASSED\u001b[0m\u001b[32m        [ 85%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_transpose[cpu-axes1-shape1] \u001b[32mPASSED\u001b[0m\u001b[32m        [ 86%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_transpose[cpu-None-shape0] \u001b[32mPASSED\u001b[0m\u001b[32m         [ 87%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_transpose[cpu-None-shape1] \u001b[32mPASSED\u001b[0m\u001b[32m         [ 88%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_transpose[cuda-axes0-shape0] \u001b[32mPASSED\u001b[0m\u001b[32m       [ 88%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_transpose[cuda-axes0-shape1] \u001b[32mPASSED\u001b[0m\u001b[32m       [ 89%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_transpose[cuda-axes1-shape0] \u001b[32mPASSED\u001b[0m\u001b[32m       [ 90%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_transpose[cuda-axes1-shape1] \u001b[32mPASSED\u001b[0m\u001b[32m       [ 91%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_transpose[cuda-None-shape0] \u001b[32mPASSED\u001b[0m\u001b[32m        [ 92%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_transpose[cuda-None-shape1] \u001b[32mPASSED\u001b[0m\u001b[32m        [ 93%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_logsumexp[cpu-shape0-None] \u001b[32mPASSED\u001b[0m\u001b[32m         [ 94%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_logsumexp[cpu-shape1-0] \u001b[32mPASSED\u001b[0m\u001b[32m            [ 94%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_logsumexp[cpu-shape2-1] \u001b[32mPASSED\u001b[0m\u001b[32m            [ 95%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_logsumexp[cpu-shape3-2] \u001b[32mPASSED\u001b[0m\u001b[32m            [ 96%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_logsumexp[cuda-shape0-None] \u001b[32mPASSED\u001b[0m\u001b[32m        [ 97%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_logsumexp[cuda-shape1-0] \u001b[32mPASSED\u001b[0m\u001b[32m           [ 98%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_logsumexp[cuda-shape2-1] \u001b[32mPASSED\u001b[0m\u001b[32m           [ 99%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_logsumexp[cuda-shape3-2] \u001b[32mPASSED\u001b[0m\u001b[32m           [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m===================== \u001b[32m\u001b[1m118 passed\u001b[0m, \u001b[33m1685 deselected\u001b[0m\u001b[32m in 3.48s\u001b[0m\u001b[32m =====================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pytest -l -v -k \"nd_backend\"\n",
    "#!python3 -m pytest -l -v -k \"test_logsumexp[cpu-shape1-0]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submit\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.8.13, pytest-7.2.0, pluggy-1.0.0\n",
      "rootdir: /home/ubuntu/repos/mlsys/hw4\n",
      "plugins: typeguard-2.13.3, anyio-3.6.2\n",
      "collected 10 items / 9 deselected / 1 selected                                 \u001b[0m\u001b[1m\n",
      "\n",
      "tests/test_nd_backend.py \n",
      "Submitting new_nd_backend...\n",
      "Grader test 1 passed\n",
      "Grader test 2 passed\n",
      "Grader test 3 passed\n",
      "Grader test 4 passed\n",
      "Grader test 5 passed\n",
      "Grader test 6 passed\n",
      "Grader test 7 passed\n",
      "Grader test 8 passed\n",
      "Grader test 9 passed\n",
      "matmul tiled...\n",
      "Grader test 10 passed\n",
      "Grader test 11 passed\n",
      "Grader test 12 passed\n",
      "Grader test 13 passed\n",
      "Grader test 14 passed\n",
      "Grader test 15 passed\n",
      "Grader test 16 passed\n",
      "Grader test 17 passed\n",
      "Grader test 18 passed\n",
      "Grader test 19 passed\n",
      "Grader test 20 passed\n",
      "Grader test 21 passed\n",
      "Grader test 22 passed\n",
      "Grader test 23 passed\n",
      "Grader test 24 passed\n",
      "Grader test 25 passed\n",
      "Grader test 26 passed\n",
      "Grader test 27 passed\n",
      "Grader test 28 passed\n",
      "Grader test 29 passed\n",
      "Grader test 30 passed\n",
      "Grader test 31 passed\n",
      "Grader test 32 passed\n",
      "Grader test 33 passed\n",
      "Grader test 34 passed\n",
      "Grader test 35 passed\n",
      "Grader test 36 passed\n",
      "Grader test 37 passed\n",
      "Grader test 38 passed\n",
      "Grader test 39 passed\n",
      "Grader test 40 passed\n",
      "Grader test 41 passed\n",
      "Grader test 42 passed\n",
      "Grader test 43 passed\n",
      "Grader test 44 passed\n",
      "Grader test 45 passed\n",
      "Grader test 46 passed\n",
      "Grader test 47 passed\n",
      "Grader test 48 passed\n",
      "Grader test 49 passed\n",
      "Grader test 50 passed\n",
      "Grader test 51 passed\n",
      "Grader test 52 passed\n",
      "Grader test 53 passed\n",
      "Grader test 54 passed\n",
      "\u001b[32m.\u001b[0m\n",
      "\n",
      "\u001b[32m======================= \u001b[32m\u001b[1m1 passed\u001b[0m, \u001b[33m9 deselected\u001b[0m\u001b[32m in 35.74s\u001b[0m\u001b[32m =======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m mugrade submit \"_Oo1E5jrsROcSyO0QctSs\" -k \"new_nd_backend\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: CIFAR-10 dataset [10 points]\n",
    "\n",
    "Next, you will write support for the [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) image classification dataset, which consists of 60,000 32x32 color images in 10 classes, with 6,000 images per class. There are 50k training images and 10k test images. \n",
    "\n",
    "Start by implementing the `__init__` function in the `CIFAR10Dataset` class. You can read in the link above how to properly read the CIFAR-10 dataset files you downloaded at the beginning of the homework. Also fill in `__getitem__` and `__len__`. Note that the return shape of the data from `__getitem__` should be in order (3, 32, 32).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.8.13, pytest-7.2.0, pluggy-1.0.0 -- /home/ubuntu/anaconda3/envs/mlsys_hw/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/ubuntu/repos/mlsys/hw4\n",
      "plugins: typeguard-2.13.3, anyio-3.6.2\n",
      "collected 1803 items / 1791 deselected / 12 selected                           \u001b[0m\u001b[1m\n",
      "\n",
      "tests/test_cifar_ptb_data.py::test_cifar10_dataset[True] \u001b[32mPASSED\u001b[0m\u001b[32m          [  8%]\u001b[0m\n",
      "tests/test_cifar_ptb_data.py::test_cifar10_dataset[False] \u001b[32mPASSED\u001b[0m\u001b[32m         [ 16%]\u001b[0m\n",
      "tests/test_cifar_ptb_data.py::test_cifar10_loader[cpu-True-1] \u001b[32mPASSED\u001b[0m\u001b[32m     [ 25%]\u001b[0m\n",
      "tests/test_cifar_ptb_data.py::test_cifar10_loader[cpu-True-15] \u001b[32mPASSED\u001b[0m\u001b[32m    [ 33%]\u001b[0m\n",
      "tests/test_cifar_ptb_data.py::test_cifar10_loader[cpu-False-1] \u001b[32mPASSED\u001b[0m\u001b[32m    [ 41%]\u001b[0m\n",
      "tests/test_cifar_ptb_data.py::test_cifar10_loader[cpu-False-15] \u001b[32mPASSED\u001b[0m\u001b[32m   [ 50%]\u001b[0m\n",
      "tests/test_cifar_ptb_data.py::test_cifar10_loader[cuda-True-1] \u001b[32mPASSED\u001b[0m\u001b[32m    [ 58%]\u001b[0m\n",
      "tests/test_cifar_ptb_data.py::test_cifar10_loader[cuda-True-15] \u001b[32mPASSED\u001b[0m\u001b[32m   [ 66%]\u001b[0m\n",
      "tests/test_cifar_ptb_data.py::test_cifar10_loader[cuda-False-1] \u001b[32mPASSED\u001b[0m\u001b[32m   [ 75%]\u001b[0m\n",
      "tests/test_cifar_ptb_data.py::test_cifar10_loader[cuda-False-15] \u001b[32mPASSED\u001b[0m\u001b[32m  [ 83%]\u001b[0m\n",
      "tests/test_conv.py::test_train_cifar10[needle.backend_ndarray.ndarray_backend_cpu] \u001b[31mFAILED\u001b[0m\u001b[31m [ 91%]\u001b[0m\n",
      "tests/test_conv.py::test_train_cifar10[needle.backend_ndarray.ndarray_backend_cuda] \u001b[31mFAILED\u001b[0m\u001b[31m [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m________ test_train_cifar10[needle.backend_ndarray.ndarray_backend_cpu] ________\u001b[0m\n",
      "\n",
      "device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_train_cifar10\u001b[39;49;00m(device):\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\n",
      "        dataset = ndl.data.CIFAR10Dataset(\u001b[33m\"\u001b[39;49;00m\u001b[33m./data/cifar-10-batches-py\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, train=\u001b[94mTrue\u001b[39;49;00m)\n",
      "        dataloader = ndl.data.DataLoader(\\\n",
      "                 dataset=dataset,\n",
      "                 batch_size=\u001b[94m128\u001b[39;49;00m,\n",
      "                 shuffle=\u001b[94mFalse\u001b[39;49;00m\n",
      "                 \u001b[90m# collate_fn=ndl.data.collate_ndarray,\u001b[39;49;00m\n",
      "                 \u001b[90m# drop_last=False,\u001b[39;49;00m\n",
      "                 \u001b[90m# device=device,\u001b[39;49;00m\n",
      "                 \u001b[90m# dtype=\"float32\"\u001b[39;49;00m\n",
      "                 )\n",
      "        \u001b[94mfrom\u001b[39;49;00m \u001b[04m\u001b[96mapps\u001b[39;49;00m\u001b[04m\u001b[96m.\u001b[39;49;00m\u001b[04m\u001b[96mmodels\u001b[39;49;00m \u001b[94mimport\u001b[39;49;00m ResNet9\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\n",
      ">       model = ResNet9(device=device, dtype=\u001b[33m\"\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "ResNet9    = <class 'apps.models.ResNet9'>\n",
      "dataloader = <needle.data.DataLoader object at 0x7fda7c3514c0>\n",
      "dataset    = <needle.data.CIFAR10Dataset object at 0x7fda7c351550>\n",
      "device     = cpu()\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_conv.py\u001b[0m:466: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <apps.models.ResNet9 object at 0x7fda7c3514f0>, device = cpu()\n",
      "dtype = 'float32'\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__init__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, dtype=\u001b[33m\"\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m):\n",
      "        \u001b[96msuper\u001b[39;49;00m().\u001b[92m__init__\u001b[39;49;00m()\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION ###\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m() \u001b[90m###\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "__class__  = <class 'apps.models.ResNet9'>\n",
      "device     = cpu()\n",
      "dtype      = 'float32'\n",
      "self       = <apps.models.ResNet9 object at 0x7fda7c3514f0>\n",
      "\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:14: NotImplementedError\n",
      "\u001b[31m\u001b[1m_______ test_train_cifar10[needle.backend_ndarray.ndarray_backend_cuda] ________\u001b[0m\n",
      "\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_train_cifar10\u001b[39;49;00m(device):\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\n",
      "        dataset = ndl.data.CIFAR10Dataset(\u001b[33m\"\u001b[39;49;00m\u001b[33m./data/cifar-10-batches-py\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, train=\u001b[94mTrue\u001b[39;49;00m)\n",
      "        dataloader = ndl.data.DataLoader(\\\n",
      "                 dataset=dataset,\n",
      "                 batch_size=\u001b[94m128\u001b[39;49;00m,\n",
      "                 shuffle=\u001b[94mFalse\u001b[39;49;00m\n",
      "                 \u001b[90m# collate_fn=ndl.data.collate_ndarray,\u001b[39;49;00m\n",
      "                 \u001b[90m# drop_last=False,\u001b[39;49;00m\n",
      "                 \u001b[90m# device=device,\u001b[39;49;00m\n",
      "                 \u001b[90m# dtype=\"float32\"\u001b[39;49;00m\n",
      "                 )\n",
      "        \u001b[94mfrom\u001b[39;49;00m \u001b[04m\u001b[96mapps\u001b[39;49;00m\u001b[04m\u001b[96m.\u001b[39;49;00m\u001b[04m\u001b[96mmodels\u001b[39;49;00m \u001b[94mimport\u001b[39;49;00m ResNet9\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\n",
      ">       model = ResNet9(device=device, dtype=\u001b[33m\"\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "ResNet9    = <class 'apps.models.ResNet9'>\n",
      "dataloader = <needle.data.DataLoader object at 0x7fda7c2f0df0>\n",
      "dataset    = <needle.data.CIFAR10Dataset object at 0x7fda7c2f0dc0>\n",
      "device     = cuda()\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_conv.py\u001b[0m:466: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <apps.models.ResNet9 object at 0x7fda7c2f0f10>, device = cuda()\n",
      "dtype = 'float32'\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__init__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, dtype=\u001b[33m\"\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m):\n",
      "        \u001b[96msuper\u001b[39;49;00m().\u001b[92m__init__\u001b[39;49;00m()\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION ###\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m() \u001b[90m###\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "__class__  = <class 'apps.models.ResNet9'>\n",
      "device     = cuda()\n",
      "dtype      = 'float32'\n",
      "self       = <apps.models.ResNet9 object at 0x7fda7c2f0f10>\n",
      "\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:14: NotImplementedError\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m tests/test_conv.py::\u001b[1mtest_train_cifar10[needle.backend_ndarray.ndarray_backend_cpu]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/test_conv.py::\u001b[1mtest_train_cifar10[needle.backend_ndarray.ndarray_backend_cuda]\u001b[0m - NotImplementedError\n",
      "\u001b[31m================ \u001b[31m\u001b[1m2 failed\u001b[0m, \u001b[32m10 passed\u001b[0m, \u001b[33m1791 deselected\u001b[0m\u001b[31m in 9.83s\u001b[0m\u001b[31m =================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pytest -l -v -k \"cifar10\"\n",
    "#!python3 -m pytest -l -v -k \"test_train_cifar10[needle.backend_ndarray.ndarray_backend_cpu]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submit\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.8.13, pytest-7.2.0, pluggy-1.0.0\n",
      "rootdir: /home/ubuntu/repos/mlsys/hw4\n",
      "plugins: typeguard-2.13.3, anyio-3.6.2\n",
      "collected 10 items / 9 deselected / 1 selected                                 \u001b[0m\u001b[1m\n",
      "\n",
      "tests/test_cifar_ptb_data.py \n",
      "Submitting cifar10...\n",
      "Grader test 1 passed\n",
      "Grader test 2 passed\n",
      "Grader test 3 failed: Failed: incorrect output\n",
      "Grader test 4 passed\n",
      "Grader test 5 failed: Failed: incorrect output\n",
      "Grader test 6 passed\n",
      "Grader test 7 failed: Failed: incorrect output\n",
      "Grader test 8 passed\n",
      "Grader test 9 failed: Failed: incorrect output\n",
      "Grader test 10 passed\n",
      "Grader test 11 passed\n",
      "Grader test 12 failed: Failed: incorrect output\n",
      "Grader test 13 passed\n",
      "Grader test 14 failed: Failed: incorrect output\n",
      "Grader test 15 passed\n",
      "Grader test 16 failed: Failed: incorrect output\n",
      "Grader test 17 passed\n",
      "Grader test 18 failed: Failed: incorrect output\n",
      "\u001b[31mF\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m________________________________ submit_cifar10 ________________________________\u001b[0m\n",
      "\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m tests/test_cifar_ptb_data.py::\u001b[1msubmit_cifar10\u001b[0m\n",
      "\u001b[31m======================= \u001b[31m\u001b[1m1 failed\u001b[0m, \u001b[33m9 deselected\u001b[0m\u001b[31m in 13.55s\u001b[0m\u001b[31m =======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m mugrade submit \"_Oo1E5jrsROcSyO0QctSs\" -k \"cifar10\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Part 3: Convolutional neural network [40 points]\n",
    "\n",
    "Here's an outline of what you will do in this task.\n",
    "\n",
    "In `python/needle/backend_ndarray/ndarray.py`, implement:\n",
    "- `flip`\n",
    "- `pad`\n",
    "\n",
    "In `python/needle/ops.py`, implement (forward and backward):\n",
    "- `Flip`\n",
    "- `Dilate`\n",
    "- `UnDilate`\n",
    "- `Conv`\n",
    "\n",
    "In `python/needle/nn.py`, implement:\n",
    "- `Flatten`\n",
    "- `Conv`\n",
    "\n",
    "In `python/apps/models.py`, fill in the `ResNet9` class.  \n",
    "\n",
    "In `apps/simple_training.py`, fill in:\n",
    "- `epoch_general_cifar10`,\n",
    "- `train_cifar10`\n",
    "- `evaluate_cifar10`\n",
    "\n",
    "We have provided a `BatchNorm2d` implementation for you as a wrapper around your previous `BatchNorm1d` implementation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding ndarrays\n",
    "\n",
    "Convolution as typically implemented in deep learning libraries cuts down the size of inputs;\n",
    "e.g., a (1, 32, 32, 3) image convolved with a 3x3 filter would give a (1, 30, 30, c) output.\n",
    "A way around this is to pad the input ndarray before performing convolution, e.g., pad with zeros to get a (1, 34, 34, 3) ndarray so that the result is (1, 32, 32, 3). \n",
    "\n",
    "Padding is also required for the backward pass of convolution.\n",
    "\n",
    "You should implement `pad` in `ndarray.py` to closely reflect the behavior of `np.pad`.\n",
    "That is, `pad` should take a tuple of 2-tuples with length equal to the number of dimensions of the array,\n",
    "where each element in the 2-tuple corresponds to \"left padding\" and \"right padding\", respectively.\n",
    "\n",
    "For example, if `A` is a (10, 32, 32, 8) ndarray (think NHWC), then `A.pad( (0, 0), (2, 2), (2, 2), (0, 0) )` would be a (10, 36, 36, 8) ndarray where the \"spatial\" dimension has been padded by two zeros on all sides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.8.13, pytest-7.2.0, pluggy-1.0.0 -- /home/ubuntu/anaconda3/envs/mlsys_hw/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/ubuntu/repos/mlsys/hw4\n",
      "plugins: typeguard-2.13.3, anyio-3.6.2\n",
      "collected 1803 items / 1801 deselected / 2 selected                            \u001b[0m\u001b[1m\n",
      "\n",
      "tests/test_conv.py::test_pad_forward[params0-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 50%]\u001b[0m\n",
      "tests/test_conv.py::test_pad_forward[params1-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m====================== \u001b[32m\u001b[1m2 passed\u001b[0m, \u001b[33m1801 deselected\u001b[0m\u001b[32m in 1.17s\u001b[0m\u001b[32m ======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pytest -l -v -k \"pad_forward\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flipping ndarrays & FlipOp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import ctypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some utility code for a demonstration below which you can probably ignore. It might be instructive to check out the `offset` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reads off the underlying data array in order (i.e., offset 0, offset 1, ..., offset n)\n",
    "# i.e., ignoring strides\n",
    "def raw_data(X):\n",
    "    X = np.array(X) # copy, thus compact X\n",
    "    return np.frombuffer(ctypes.string_at(X.ctypes.data, X.nbytes), dtype=X.dtype, count=X.size)\n",
    "\n",
    "# Xold and Xnew should reference the same underlying data\n",
    "def offset(Xold, Xnew):\n",
    "    assert Xold.itemsize == Xnew.itemsize\n",
    "    # compare addresses to the beginning of the arrays\n",
    "    return (Xnew.ctypes.data - Xold.ctypes.data)//Xnew.itemsize\n",
    "\n",
    "def strides(X):\n",
    "    return ', '.join([str(x//X.itemsize) for x in X.strides])\n",
    "\n",
    "def format_array(X, shape):\n",
    "    assert len(shape) == 3, \"I only made this formatting work for ndims = 3\"\n",
    "    def chunks(l, n):\n",
    "        n = max(1, n)\n",
    "        return (l[i:i+n] for i in range(0, len(l), n))\n",
    "    a = [str(x) if x >= 10 else ' ' + str(x) for x in X]\n",
    "    a = ['(' + ' '.join(y) + ')' for y in [x for x in chunks(a, shape[-1])]]\n",
    "    a = ['|' + ' '.join(y) + '|' for y in [x for x in chunks(a, shape[-2])]]\n",
    "    return '  '.join(a)\n",
    "\n",
    "def inspect_array(X, *, is_a_copy_of):\n",
    "    # compacts X, then reads it off in order\n",
    "    print('Data: %s' % format_array(raw_data(X), X.shape))\n",
    "    # compares address of X to copy_of, thus finding X's offset\n",
    "    print('Offset: %s' % offset(is_a_copy_of, X))\n",
    "    print('Strides: %s' % strides(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "In order to implement the backwards pass of 2D convolution, we will (probably) need a function which _flips_\n",
    "axes of ndarrays. We say \"probably\" because you could probably cleverly implement your convolution forward\n",
    "function to avoid this. However, we think it is easiest to think about this if you have the ability to \"flip\" the kernel along its vertical and horizontal dimensions.\n",
    "\n",
    "We will try to build up your intuition for the \"flip\" operation below in order to help you figure out how to implement it in `ndarray.py`. To do that, we explore numpy's `np.flip` function below. One thing to note is that\n",
    "`flip` is typically implemented by using negative strides and changing the _offset_ of the underlying array.\n",
    "\n",
    "For example, flipping an array on _all_ of its axes is equivalent to reversing the array. In this case, you can imagine that we would want all the strides to be negative, and the offset to be the length of the array (to start at the end of the array and \"stride\" backwards).\n",
    "\n",
    "Since we did not explicitly support negative strides in our implementation for the last homework, we will merely call `NDArray.make` with them to make our \"flipped\" array and then immediately call `.compact()`. Other than changing unsigned ints to signed ints in a few places, we suspect your existing `compact` function should not have to change at all to accomodate negative strides. In the .cc and .cu files we distributed, we have already changed the function signatures to reflect this.\n",
    "\n",
    "Alternatively, you could simply implement `flip` in the CPU backend by copying memory, which you _may_ find more intuitive. We suggest following our mini tutorial below to keep your implementation Python-focused, since we believe it is involves approximately the same amount of effort to implement it slightly more naively in C."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this array as reference for the other examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: |( 1  2  3  4) ( 5  6  7  8)|  |( 9 10 11 12) (13 14 15 16)|  |(17 18 19 20) (21 22 23 24)|\n",
      "Offset: 0\n",
      "Strides: 8, 4, 1\n"
     ]
    }
   ],
   "source": [
    "A = np.arange(1, 25).reshape(3, 2, 4)\n",
    "inspect_array(A, is_a_copy_of=A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have put brackets around each axis of the array. Notice that for this array, the offset is 0 and the strides are all positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See what happens when you flip the array along the last axis below. \n",
    "Note that the `inspect_array` function compacts the array after flipping it so you can see the\n",
    "\"logical\" order of the data, and the offset is calculated by comparing the address of the **non**-compacted\n",
    "flipped array with that of `is_copy_of`, i.e., the array `A` we looked at above.\n",
    "\n",
    "That is, we are looking at how numpy calculates the strides and offset for flipped arrays in order\n",
    "to copy this behavior in our own implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: |( 4  3  2  1) ( 8  7  6  5)|  |(12 11 10  9) (16 15 14 13)|  |(20 19 18 17) (24 23 22 21)|\n",
      "Offset: 3\n",
      "Strides: 8, 4, -1\n"
     ]
    }
   ],
   "source": [
    "inspect_array(np.flip(A, (2,)), is_a_copy_of=A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So flipping the last axis reverses the order of the elements within each 4-dimensional \"cell\", as you can see above. The stride corresponding to the axis we flipped has been negated. And the offset is 3 -- this makes sense, e.g., because we want the new \"first\" element of the array to be 4, which was at index 3 in `A`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: |( 5  6  7  8) ( 1  2  3  4)|  |(13 14 15 16) ( 9 10 11 12)|  |(21 22 23 24) (17 18 19 20)|\n",
      "Offset: 4\n",
      "Strides: 8, -4, 1\n"
     ]
    }
   ],
   "source": [
    "inspect_array(np.flip(A, (1,)), is_a_copy_of=A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again for the middle axis: we negate the middle stride, and the offset is 4, which seems reasonable since we now want the first element to be 5, which was at index 4 in the original array `A`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: |(17 18 19 20) (21 22 23 24)|  |( 9 10 11 12) (13 14 15 16)|  |( 1  2  3  4) ( 5  6  7  8)|\n",
      "Offset: 16\n",
      "Strides: -8, 4, 1\n"
     ]
    }
   ],
   "source": [
    "inspect_array(np.flip(A, (0,)), is_a_copy_of=A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to infer the more general algorithm for computing the offset given the axis to flip."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe what happens when we flip _all_ axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: |(24 23 22 21) (20 19 18 17)|  |(16 15 14 13) (12 11 10  9)|  |( 8  7  6  5) ( 4  3  2  1)|\n",
      "Offset: 23\n",
      "Strides: -8, -4, -1\n"
     ]
    }
   ],
   "source": [
    "inspect_array(np.flip(A, (0,1,2)), is_a_copy_of=A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned earlier, the offset is then sufficient to point to the last element of the array, and this is just the \"reverse order\" version of `A`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we flip just axes 1 and 0..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: |(21 22 23 24) (17 18 19 20)|  |(13 14 15 16) ( 9 10 11 12)|  |( 5  6  7  8) ( 1  2  3  4)|\n",
      "Offset: 20\n",
      "Strides: -8, -4, 1\n"
     ]
    }
   ],
   "source": [
    "inspect_array(np.flip(A, (0,1)), is_a_copy_of=A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The offset is 20. Looking back on our previous offset computations, do you notice something?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------\n",
    "\n",
    "With this exploration of numpy's ndarray flipping functionality, which uses negative strides and a custom offset,\n",
    "try to implement `flip` in `ndarray.py`. You also must implement \"flip\" forward and backward functions in `ops.py`; note that these should be extremely short.\n",
    "\n",
    "**Important:** You should call NDArray.make with the new strides and offset, and then immediately `.compact()` this array. The resulting array is then copied and has positive strides. We want this (less-than-optimal) behavior because we did not account for negative strides in our previous implementation. _Aside:_ If you want, consider where/if negative strides break your implementation. `__getitem__` definitely doesn't work due to how we processed slices; is there anything else? (_Note_: this isn't graded.)\n",
    "\n",
    "Also, if you want to instead add a `flip` operator on the CPU/CUDA backends, that's also okay.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.8.13, pytest-7.2.0, pluggy-1.0.0 -- /home/ubuntu/anaconda3/envs/mlsys_hw/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/ubuntu/repos/mlsys/hw4\n",
      "plugins: typeguard-2.13.3, anyio-3.6.2\n",
      "collected 1803 items / 1763 deselected / 40 selected                           \u001b[0m\u001b[1m\n",
      "\n",
      "tests/test_conv.py::test_flip_forward[params0-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [  2%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_forward[params0-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [  5%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_forward[params1-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [  7%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_forward[params1-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 10%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_forward[params2-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 12%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_forward[params2-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 15%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_forward[params3-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 17%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_forward[params3-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 20%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_forward[params4-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 22%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_forward[params4-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 25%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_forward[params5-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 27%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_forward[params5-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 30%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_forward[params6-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 32%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_forward[params6-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 35%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_forward[params7-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 37%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_forward[params7-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 40%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_forward[params8-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 42%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_forward[params8-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 45%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_forward[params9-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 47%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_forward[params9-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 50%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_backward[params0-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 52%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_backward[params0-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 55%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_backward[params1-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 57%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_backward[params1-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 60%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_backward[params2-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 62%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_backward[params2-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 65%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_backward[params3-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 67%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_backward[params3-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 70%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_backward[params4-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 72%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_backward[params4-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 75%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_backward[params5-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 77%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_backward[params5-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 80%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_backward[params6-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 82%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_backward[params6-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 85%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_backward[params7-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 87%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_backward[params7-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 90%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_backward[params8-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 92%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_backward[params8-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 95%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_backward[params9-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 97%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_backward[params9-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m===================== \u001b[32m\u001b[1m40 passed\u001b[0m, \u001b[33m1763 deselected\u001b[0m\u001b[32m in 2.04s\u001b[0m\u001b[32m ======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!python3 -m pytest -l -v -k \"flip\"\n",
    "#!python3 -m pytest -l -v -k \"test_flip_backward[params0-needle.backend_ndarray.ndarray_backend_cpu]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dilation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dilation operator puts zeros between elements of an ndarray. We will need it for computing the backward pass of convolution when the stride of the convolution is greater than 1. As an example, dilation should do the following to a 2x2 matrix when dilated by 1 on both axes:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 2 \\\\\n",
    "3 & 4\n",
    "\\end{bmatrix}\n",
    "\\Longrightarrow\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 2 & 0 \\\\\n",
    "0 & 0 & 0 & 0 \\\\\n",
    "3 & 0 & 4 & 0 \\\\\n",
    "0 & 0 & 0 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "To get some intuition for why we need dilation for the backward pass of strided convolution, consider a  `stride=2`, `padding=\"same\"`, `input_channels=output_channels=8` convolution applied to an input of size (10, 32, 32, 8). The resulting output will be of size (10, 16, 16, 8) due to the stride, and thus `out_grad` will have shape (10, 16, 16, 8). Yet, the gradient of the input needs to, of course, have shape (10, 32, 32, 8) -- so we must need to increase the size of `out_grad` in some way. Consider also that you could implement strided convolution as `Conv(x)[:, ::2, ::2, :]`, i.e., only keeping every other pixel in the spatial dimension.\n",
    "\n",
    "\n",
    "Implement `Dilate` in `ops.py`. This function takes two additional parameters (in attrs): the `dilation` amount and the `axes` to dilate. You must also implement the corresponding op `UnDilate`, whose forward pass will be used to implement the gradient of `Dilate`. (This is so we do not have to implement `GetItem` and `SetItem` ops, which can be highly inefficient to backprop through without additional optimizations.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.8.13, pytest-7.2.0, pluggy-1.0.0 -- /home/ubuntu/anaconda3/envs/mlsys_hw/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/ubuntu/repos/mlsys/hw4\n",
      "plugins: typeguard-2.13.3, anyio-3.6.2\n",
      "collected 1803 items / 1777 deselected / 26 selected                           \u001b[0m\u001b[1m\n",
      "\n",
      "tests/test_conv.py::test_dilate_forward[needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [  3%]\u001b[0m\n",
      "tests/test_conv.py::test_dilate_forward[needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [  7%]\u001b[0m\n",
      "tests/test_conv.py::test_dilate_backward[params0-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 11%]\u001b[0m\n",
      "tests/test_conv.py::test_dilate_backward[params0-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 15%]\u001b[0m\n",
      "tests/test_conv.py::test_dilate_backward[params1-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 19%]\u001b[0m\n",
      "tests/test_conv.py::test_dilate_backward[params1-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 23%]\u001b[0m\n",
      "tests/test_conv.py::test_dilate_backward[params2-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 26%]\u001b[0m\n",
      "tests/test_conv.py::test_dilate_backward[params2-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 30%]\u001b[0m\n",
      "tests/test_conv.py::test_dilate_backward[params3-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 34%]\u001b[0m\n",
      "tests/test_conv.py::test_dilate_backward[params3-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 38%]\u001b[0m\n",
      "tests/test_conv.py::test_dilate_backward[params4-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 42%]\u001b[0m\n",
      "tests/test_conv.py::test_dilate_backward[params4-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 46%]\u001b[0m\n",
      "tests/test_conv.py::test_dilate_backward[params5-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 50%]\u001b[0m\n",
      "tests/test_conv.py::test_dilate_backward[params5-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 53%]\u001b[0m\n",
      "tests/test_conv.py::test_dilate_backward[params6-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 57%]\u001b[0m\n",
      "tests/test_conv.py::test_dilate_backward[params6-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 61%]\u001b[0m\n",
      "tests/test_conv.py::test_dilate_backward[params7-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 65%]\u001b[0m\n",
      "tests/test_conv.py::test_dilate_backward[params7-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 69%]\u001b[0m\n",
      "tests/test_conv.py::test_dilate_backward[params8-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 73%]\u001b[0m\n",
      "tests/test_conv.py::test_dilate_backward[params8-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 76%]\u001b[0m\n",
      "tests/test_conv.py::test_dilate_backward[params9-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 80%]\u001b[0m\n",
      "tests/test_conv.py::test_dilate_backward[params9-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 84%]\u001b[0m\n",
      "tests/test_conv.py::test_dilate_backward[params10-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 88%]\u001b[0m\n",
      "tests/test_conv.py::test_dilate_backward[params10-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 92%]\u001b[0m\n",
      "tests/test_conv.py::test_dilate_backward[params11-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 96%]\u001b[0m\n",
      "tests/test_conv.py::test_dilate_backward[params11-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m===================== \u001b[32m\u001b[1m26 passed\u001b[0m, \u001b[33m1777 deselected\u001b[0m\u001b[32m in 1.95s\u001b[0m\u001b[32m ======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pytest -l -v -k \"dilate\"\n",
    "#!python3 -m pytest -l -v -k \"test_dilate_backward[params0-needle.backend_ndarray.ndarray_backend_cpu]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit new ops (flip/dilation) to mugrade [10 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submit\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.8.13, pytest-7.2.0, pluggy-1.0.0\n",
      "rootdir: /home/ubuntu/repos/mlsys/hw4\n",
      "plugins: typeguard-2.13.3, anyio-3.6.2\n",
      "collected 10 items / 9 deselected / 1 selected                                 \u001b[0m\u001b[1m\n",
      "\n",
      "tests/test_conv.py \n",
      "Submitting new_ops...\n",
      "Grader test 1 passed\n",
      "Grader test 2 passed\n",
      "Grader test 3 passed\n",
      "Grader test 4 passed\n",
      "Grader test 5 passed\n",
      "Grader test 6 passed\n",
      "Grader test 7 passed\n",
      "Grader test 8 passed\n",
      "Grader test 9 passed\n",
      "Grader test 10 passed\n",
      "Grader test 11 passed\n",
      "\u001b[32m.\u001b[0m\n",
      "\n",
      "\u001b[32m======================= \u001b[32m\u001b[1m1 passed\u001b[0m, \u001b[33m9 deselected\u001b[0m\u001b[32m in 7.13s\u001b[0m\u001b[32m ========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m mugrade submit \"_Oo1E5jrsROcSyO0QctSs\" -k \"new_ops\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution forward\n",
    "\n",
    "Implement the forward pass of 2D multi-channel convolution in `ops.py`. You should probably refer to [this notebook](https://github.com/dlsyscourse/public_notebooks/blob/main/convolution_implementation.ipynb) from lecture, which implements 2D multi-channel convolution using im2col in numpy.\n",
    "\n",
    "**Note:** Your convolution op should accept tensors in the NHWC format, as in the example above, and weights in the format (kernel_size, kernel_size, input_channels, output_channels).\n",
    "\n",
    "However, you will need to add two additional features. Your convolution function should accept arguments for `padding` (default 0) and `stride` (default 1). For `padding`, you should simply apply your padding function to the spatial dimensions (i.e., axes 1 and 2). \n",
    "\n",
    "Implementing strided convolution should consist of a relatively small set of changes to your plain convolution implementation.\n",
    "\n",
    "We recommend implementing convolution without stride first, ensuring you pass some of the tests below, and then adding in stride."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.8.13, pytest-7.2.0, pluggy-1.0.0 -- /home/ubuntu/anaconda3/envs/mlsys_hw/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/ubuntu/repos/mlsys/hw4\n",
      "plugins: typeguard-2.13.3, anyio-3.6.2\n",
      "collected 1803 items / 1769 deselected / 34 selected                           \u001b[0m\u001b[1m\n",
      "\n",
      "tests/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape0-W_shape0-1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [  2%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape1-W_shape1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [  5%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape2-W_shape2-1-2] \u001b[32mPASSED\u001b[0m\u001b[32m [  8%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape3-W_shape3-1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 11%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape4-W_shape4-1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 14%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape5-W_shape5-2-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 17%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape6-W_shape6-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 20%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape7-W_shape7-2-2] \u001b[32mPASSED\u001b[0m\u001b[32m [ 23%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape8-W_shape8-2-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 26%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape9-W_shape9-2-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 29%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape10-W_shape10-1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 32%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape11-W_shape11-1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 35%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape12-W_shape12-1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 38%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape13-W_shape13-1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 41%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape14-W_shape14-1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 44%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape15-W_shape15-1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 47%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape16-W_shape16-1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 50%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape0-W_shape0-1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 52%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape1-W_shape1-1-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 55%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape2-W_shape2-1-2] \u001b[32mPASSED\u001b[0m\u001b[32m [ 58%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape3-W_shape3-1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 61%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape4-W_shape4-1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 64%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape5-W_shape5-2-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 67%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape6-W_shape6-2-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 70%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape7-W_shape7-2-2] \u001b[32mPASSED\u001b[0m\u001b[32m [ 73%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape8-W_shape8-2-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 76%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape9-W_shape9-2-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 79%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape10-W_shape10-1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 82%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape11-W_shape11-1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 85%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape12-W_shape12-1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 88%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape13-W_shape13-1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 91%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape14-W_shape14-1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 94%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape15-W_shape15-1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 97%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape16-W_shape16-1-0] \u001b[32mPASSED\u001b[0m\u001b[32m [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m===================== \u001b[32m\u001b[1m34 passed\u001b[0m, \u001b[33m1769 deselected\u001b[0m\u001b[32m in 1.48s\u001b[0m\u001b[32m ======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pytest -l -v -k \"op_conv and forward\"\n",
    "# !python3 -m pytest -l -v -k \"test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape0-W_shape0-1-0]\"\n",
    "# !python3 -m pytest -l -v -k \"test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape1-W_shape1-1-1]\"\n",
    "# !python3 -m pytest -l -v -k \"test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape5-W_shape5-2-0]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the gradients of 2D multi-channel convolution can be technically quite challenging (especially \"rigorously\"). We will try to provide some useful hints here. Basically, we encourage you to make use of the surprising fact that _whatever makes the dimensions work out is typically right_.\n",
    "\n",
    "Ultimately, the backward pass of convolution can be done in terms of the convolution operator itself, with some clever manipulations using `flip`, `dilate`, and multiple applications of `transpose` to both the arguments and the results.\n",
    "\n",
    "In the last section, we essentially implemented convolution as a matrix product: ignoring the various restride and reshape operations, we basically have something like `X @ W`, where `X` is the input and `W` is the weight. We also have `out_grad`, which is the same shape as `X @ W`. Now, you have already implemented the backward pass of matrix multiplication in a previous assignment, and we can use this knowledge to get some insight into the backward pass of convolution. In particular, referencing your matmul backward implementation, you may notice (heuristically speaking here):\n",
    "\n",
    "`X.grad = out_grad @ W.transpose` \\\n",
    "`W.grad = X.transpose @ out_grad`\n",
    "\n",
    "Surprisingly enough, things work out if we just assume that these are also convolutions (and now assuming that `out_grad`, `W`, and `X` are tensors amenable to 2D multi-channel convolution instead of matrices):\n",
    "\n",
    "`X.grad = ≈conv(≈out_grad, ≈W)` \\\n",
    "`W.grad = ≈conv(≈X, ≈out_grad)`\n",
    "\n",
    "In which the \"≈\" indicates that you need to apply some additional operators to these terms in order to get the dimensions to work out, such as permuting/transposing axes, dilating, changing the `padding=` argument to the convolution function, or permuting/transposing axes of the resulting convolution.\n",
    "\n",
    "As we saw on the [last few slides here](https://dlsyscourse.org/slides/conv_nets.pdf) in class, the transpose of a convolution can be found by simply flipping the kernel. Since we're working in 2D instead of 1D, this means flipping the kernel both vertically and horizontally (thus why we implemented `flip`).\n",
    "\n",
    "Summarizing some hints for both `X.grad` and `W.grad`:\n",
    "\n",
    "`X.grad`\n",
    "- The convolution of `out_grad` and `W`, with some operations applied to those\n",
    "- `W` should be flipped over both the kernel dimensions\n",
    "- If the convolution is strided, increase the size of `out_grad` with a corresponding dilation\n",
    "- Do an example to analyze dimensions: note the shape you want for `X.grad`, and think about how you must permute/transpose the arguments and add padding to the convolution to achieve this shape \n",
    "    - This padding depends on both the kernel size and the `padding` argument to the convolution\n",
    "\n",
    "`W.grad`\n",
    "- The convolution of `X` and `out_grad`, with some operations applied to those\n",
    "- The gradients of `W` must be accumulated over the batches; how can you make the conv operator itself do this accumulation?\n",
    "    - Consider turning batches into channels via transpose/permute\n",
    "- Analyze dimensions: how can you modify `X` and `out_grad` so that the shape of their convolution matches the shape of `W`? You may need to transpose/permute the result.\n",
    "    - Remember to account for the `padding` argument passed to convolution\n",
    "\n",
    "General tips\n",
    "- Deal with strided convolutions last (you should be able to just drop in `dilate` when you've passed most of the tests)\n",
    "- Start with the case where `padding=0`, then consider changing `padding` arguments\n",
    "- You can \"permute\" axes with multiple calls to `transpose`\n",
    "\n",
    "It might also be useful to skip ahead to nn.Conv, pass the forward tests, and then use both the tests below and the nn.Conv backward tests to debug your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.8.13, pytest-7.2.0, pluggy-1.0.0 -- /home/ubuntu/anaconda3/envs/mlsys_hw/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/ubuntu/repos/mlsys/hw4\n",
      "plugins: typeguard-2.13.3, anyio-3.6.2\n",
      "collected 1803 items / 1769 deselected / 34 selected                           \u001b[0m\u001b[1m\n",
      "\n",
      "tests/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape0-W_shape0-1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [  2%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape1-W_shape1-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [  5%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape2-W_shape2-1-2] \u001b[31mFAILED\u001b[0m\u001b[31m [  8%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape3-W_shape3-1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 11%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape4-W_shape4-1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 14%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape5-W_shape5-2-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 17%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape6-W_shape6-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 20%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape7-W_shape7-2-2] \u001b[31mFAILED\u001b[0m\u001b[31m [ 23%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape8-W_shape8-2-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 26%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape9-W_shape9-2-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 29%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape10-W_shape10-1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 32%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape11-W_shape11-1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 35%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape12-W_shape12-1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 38%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape13-W_shape13-1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 41%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape14-W_shape14-1-0] \u001b[32mPASSED\u001b[0m\u001b[31m [ 44%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape15-W_shape15-1-0] \u001b[32mPASSED\u001b[0m\u001b[31m [ 47%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape16-W_shape16-1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 50%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape0-W_shape0-1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 52%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape1-W_shape1-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 55%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape2-W_shape2-1-2] \u001b[31mFAILED\u001b[0m\u001b[31m [ 58%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape3-W_shape3-1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 61%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape4-W_shape4-1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 64%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape5-W_shape5-2-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 67%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape6-W_shape6-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 70%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape7-W_shape7-2-2] \u001b[31mFAILED\u001b[0m\u001b[31m [ 73%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape8-W_shape8-2-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 76%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape9-W_shape9-2-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 79%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape10-W_shape10-1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 82%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape11-W_shape11-1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 85%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape12-W_shape12-1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 88%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape13-W_shape13-1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 91%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape14-W_shape14-1-0] \u001b[32mPASSED\u001b[0m\u001b[31m [ 94%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape15-W_shape15-1-0] \u001b[32mPASSED\u001b[0m\u001b[31m [ 97%]\u001b[0m\n",
      "tests/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape16-W_shape16-1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m_ test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape0-W_shape0-1-0] _\u001b[0m\n",
      "\n",
      "Z_shape = (3, 14, 14, 8), W_shape = (3, 3, 8, 16), stride = 1, padding = 0\n",
      "backward = True, device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\n",
      "        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\n",
      "        _Z = _Z.astype(np.float32)\n",
      "        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\n",
      "        _W = _W.astype(np.float32)\n",
      "        Z = ndl.Tensor(_Z, device=device)\n",
      "        W = ndl.Tensor(_W, device=device)\n",
      "        y = ndl.conv(Z, W, padding=padding, stride=stride)\n",
      "        y2 = y.sum()\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\n",
      "            y2.backward()\n",
      "        Ztch = torch.Tensor(_Z).float()\n",
      "        Ztch.requires_grad=\u001b[94mTrue\u001b[39;49;00m\n",
      "        Wtch = torch.Tensor(_W).float()\n",
      "        Wtch.requires_grad=\u001b[94mTrue\u001b[39;49;00m\n",
      "        out = torch.nn.functional.conv2d(Ztch.permute(\u001b[94m0\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m), Wtch.permute(\u001b[94m3\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m), padding=padding, stride=stride)\n",
      "        out2 = out.sum()\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\n",
      "            out2.backward()\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\n",
      "            err1 = np.linalg.norm(Ztch.grad.numpy() - Z.grad.numpy())\n",
      "            err2 = np.linalg.norm(Wtch.grad.numpy() - W.grad.numpy())\n",
      "        err3 = np.linalg.norm(out2.detach().numpy() - y2.numpy())\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\n",
      ">           \u001b[94massert\u001b[39;49;00m err1 < \u001b[94m1e-2\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33minput grads match\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           AssertionError: input grads match\u001b[0m\n",
      "\u001b[1m\u001b[31mE           assert nan < 0.01\u001b[0m\n",
      "\n",
      "W          = needle.Tensor([[[[  4.282669     8.133604    -4.6223483  ...   2.0798197\n",
      "      2.1704152   -0.38803983]\n",
      "   [  2.245844...2.618842     0.71487164]\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n",
      "      0.03223436   2.0512383 ]]]])\n",
      "W_shape    = (3, 3, 8, 16)\n",
      "Wtch       = tensor([[[[  4.2827,   8.1336,  -4.6223,  ...,   2.0798,   2.1704,  -0.3880],\n",
      "          [  2.2458,  -0.6565,  -3.7547,...   0.7149],\n",
      "          [ -4.0937,   3.7292, -10.5518,  ...,   0.3174,   0.0322,   2.0512]]]],\n",
      "       requires_grad=True)\n",
      "Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e...3e+00]\n",
      "   [ 1.09905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n",
      "    -8.48226726e-01 -2.29984760e+00]]]])\n",
      "Z_shape    = (3, 14, 14, 8)\n",
      "Ztch       = tensor([[[[ 8.8203e+00,  2.0008e+00,  4.8937e+00,  ..., -4.8864e+00,\n",
      "            4.7504e+00, -7.5679e-01],\n",
      "          [...[ 1.0991e+01,  2.2317e+00,  4.6206e+00,  ..., -6.2309e-01,\n",
      "           -8.4823e-01, -2.2998e+00]]]], requires_grad=True)\n",
      "_W         = array([[[[  4.282669  ,   8.133604  ,  -4.6223483 , ...,   2.0798197 ,\n",
      "            2.1704152 ,  -0.38803983],\n",
      "        ... [ -4.09374   ,   3.7291534 , -10.551835  , ...,   0.31741843,\n",
      "            0.03223436,   2.0512383 ]]]], dtype=float32)\n",
      "_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\n",
      "          -4.88638926e+00,  4.75044203e+00, -7.56786...166728e+00,  4.62055349e+00, ...,\n",
      "          -6.23090088e-01, -8.48226726e-01, -2.29984760e+00]]]],\n",
      "      dtype=float32)\n",
      "backward   = True\n",
      "device     = cpu()\n",
      "err1       = nan\n",
      "err2       = 0.0016771061\n",
      "err3       = 0.0034179688\n",
      "out        = tensor([[[[-2.1851e+01, -1.8266e+02, -2.9496e+02,  ...,  9.3036e+01,\n",
      "           -1.0480e+02,  1.1030e+02],\n",
      "          [...,  5.7277e+02, -9.0837e+01,  ..., -1.8747e+02,\n",
      "           -1.1281e+02,  2.0660e+02]]]], grad_fn=<ConvolutionBackward0>)\n",
      "out2       = tensor(-6614.2402, grad_fn=<SumBackward0>)\n",
      "padding    = 0\n",
      "stride     = 1\n",
      "torch      = <module 'torch' from '/home/ubuntu/anaconda3/envs/mlsys_hw/lib/python3.8/site-packages/torch/__init__.py'>\n",
      "y          = needle.Tensor([[[[-2.18507862e+01  7.10567322e+01 -2.19253067e+02 ...  1.22643860e+02\n",
      "     1.09581779e+02  5.29599533e...5e+02]\n",
      "   [ 1.20957283e+02 -4.53491516e+02  7.81254578e+01 ...  1.71220749e+02\n",
      "    -1.71809418e+02  2.06600098e+02]]]])\n",
      "y2         = needle.Tensor([-6614.237])\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_conv.py\u001b[0m:446: AssertionError\n",
      "----------------------------- Captured stdout call -----------------------------\n",
      "matmul tiled...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "out_grad shape: (3, 12, 12, 16)\n",
      "stride: 1\n",
      "_out_grad shape: (3, 12, 12, 16)\n",
      "W shape: (3, 3, 8, 16)\n",
      "_W shape: (3, 3, 16, 8)\n",
      "X_grad shape: (3, 14, 14, 8)\n",
      "X_shape: (3, 14, 14, 8)\n",
      "_X shape: (8, 14, 14, 3)\n",
      "_out_grad shape: (12, 12, 3, 16)\n",
      "matmul tiled...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "W_grad shape: (8, 3, 3, 16)\n",
      "W_grad shape: (3, 3, 8, 16)\n",
      "\u001b[31m\u001b[1m_ test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape1-W_shape1-1-1] _\u001b[0m\n",
      "\n",
      "Z_shape = (3, 14, 14, 8), W_shape = (3, 3, 8, 16), stride = 1, padding = 1\n",
      "backward = True, device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\n",
      "        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\n",
      "        _Z = _Z.astype(np.float32)\n",
      "        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\n",
      "        _W = _W.astype(np.float32)\n",
      "        Z = ndl.Tensor(_Z, device=device)\n",
      "        W = ndl.Tensor(_W, device=device)\n",
      "        y = ndl.conv(Z, W, padding=padding, stride=stride)\n",
      "        y2 = y.sum()\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\n",
      "            y2.backward()\n",
      "        Ztch = torch.Tensor(_Z).float()\n",
      "        Ztch.requires_grad=\u001b[94mTrue\u001b[39;49;00m\n",
      "        Wtch = torch.Tensor(_W).float()\n",
      "        Wtch.requires_grad=\u001b[94mTrue\u001b[39;49;00m\n",
      "        out = torch.nn.functional.conv2d(Ztch.permute(\u001b[94m0\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m), Wtch.permute(\u001b[94m3\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m), padding=padding, stride=stride)\n",
      "        out2 = out.sum()\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\n",
      "            out2.backward()\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\n",
      "            err1 = np.linalg.norm(Ztch.grad.numpy() - Z.grad.numpy())\n",
      "            err2 = np.linalg.norm(Wtch.grad.numpy() - W.grad.numpy())\n",
      "        err3 = np.linalg.norm(out2.detach().numpy() - y2.numpy())\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\n",
      ">           \u001b[94massert\u001b[39;49;00m err1 < \u001b[94m1e-2\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33minput grads match\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           AssertionError: input grads match\u001b[0m\n",
      "\u001b[1m\u001b[31mE           assert nan < 0.01\u001b[0m\n",
      "\n",
      "W          = needle.Tensor([[[[  4.282669     8.133604    -4.6223483  ...   2.0798197\n",
      "      2.1704152   -0.38803983]\n",
      "   [  2.245844...2.618842     0.71487164]\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n",
      "      0.03223436   2.0512383 ]]]])\n",
      "W_shape    = (3, 3, 8, 16)\n",
      "Wtch       = tensor([[[[  4.2827,   8.1336,  -4.6223,  ...,   2.0798,   2.1704,  -0.3880],\n",
      "          [  2.2458,  -0.6565,  -3.7547,...   0.7149],\n",
      "          [ -4.0937,   3.7292, -10.5518,  ...,   0.3174,   0.0322,   2.0512]]]],\n",
      "       requires_grad=True)\n",
      "Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e...3e+00]\n",
      "   [ 1.09905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n",
      "    -8.48226726e-01 -2.29984760e+00]]]])\n",
      "Z_shape    = (3, 14, 14, 8)\n",
      "Ztch       = tensor([[[[ 8.8203e+00,  2.0008e+00,  4.8937e+00,  ..., -4.8864e+00,\n",
      "            4.7504e+00, -7.5679e-01],\n",
      "          [...[ 1.0991e+01,  2.2317e+00,  4.6206e+00,  ..., -6.2309e-01,\n",
      "           -8.4823e-01, -2.2998e+00]]]], requires_grad=True)\n",
      "_W         = array([[[[  4.282669  ,   8.133604  ,  -4.6223483 , ...,   2.0798197 ,\n",
      "            2.1704152 ,  -0.38803983],\n",
      "        ... [ -4.09374   ,   3.7291534 , -10.551835  , ...,   0.31741843,\n",
      "            0.03223436,   2.0512383 ]]]], dtype=float32)\n",
      "_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\n",
      "          -4.88638926e+00,  4.75044203e+00, -7.56786...166728e+00,  4.62055349e+00, ...,\n",
      "          -6.23090088e-01, -8.48226726e-01, -2.29984760e+00]]]],\n",
      "      dtype=float32)\n",
      "backward   = True\n",
      "device     = cpu()\n",
      "err1       = nan\n",
      "err2       = 0.0020093916\n",
      "err3       = 0.009765625\n",
      "out        = tensor([[[[ 107.2545, -109.1249,   -9.3744,  ...,  118.7832,  127.2525,\n",
      "           -146.6358],\n",
      "          [-296.2552,  ...[-107.9026,   -8.9582,  137.1271,  ..., -355.2759,  128.8402,\n",
      "             28.4111]]]], grad_fn=<ConvolutionBackward0>)\n",
      "out2       = tensor(-12155.8203, grad_fn=<SumBackward0>)\n",
      "padding    = 1\n",
      "stride     = 1\n",
      "torch      = <module 'torch' from '/home/ubuntu/anaconda3/envs/mlsys_hw/lib/python3.8/site-packages/torch/__init__.py'>\n",
      "y          = needle.Tensor([[[[ 107.25448      31.246029   -213.92354    ...  -29.267391\n",
      "      90.55236      20.23182   ]\n",
      "   [-109....98     128.84021   ]\n",
      "   [ 243.11641     201.83783      41.88371    ...  121.594315\n",
      "     127.64035      28.411123  ]]]])\n",
      "y2         = needle.Tensor([-12155.83])\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_conv.py\u001b[0m:446: AssertionError\n",
      "----------------------------- Captured stdout call -----------------------------\n",
      "out_grad shape: (3, 14, 14, 16)\n",
      "stride: 1\n",
      "_out_grad shape: (3, 14, 14, 16)\n",
      "W shape: (3, 3, 8, 16)\n",
      "_W shape: (3, 3, 16, 8)\n",
      "X_grad shape: (3, 14, 14, 8)\n",
      "X_shape: (3, 14, 14, 8)\n",
      "_X shape: (8, 14, 14, 3)\n",
      "_out_grad shape: (14, 14, 3, 16)\n",
      "W_grad shape: (8, 3, 3, 16)\n",
      "W_grad shape: (3, 3, 8, 16)\n",
      "\u001b[31m\u001b[1m_ test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape2-W_shape2-1-2] _\u001b[0m\n",
      "\n",
      "Z_shape = (3, 16, 16, 8), W_shape = (3, 3, 8, 16), stride = 1, padding = 2\n",
      "backward = True, device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\n",
      "        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\n",
      "        _Z = _Z.astype(np.float32)\n",
      "        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\n",
      "        _W = _W.astype(np.float32)\n",
      "        Z = ndl.Tensor(_Z, device=device)\n",
      "        W = ndl.Tensor(_W, device=device)\n",
      "        y = ndl.conv(Z, W, padding=padding, stride=stride)\n",
      "        y2 = y.sum()\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\n",
      "            y2.backward()\n",
      "        Ztch = torch.Tensor(_Z).float()\n",
      "        Ztch.requires_grad=\u001b[94mTrue\u001b[39;49;00m\n",
      "        Wtch = torch.Tensor(_W).float()\n",
      "        Wtch.requires_grad=\u001b[94mTrue\u001b[39;49;00m\n",
      "        out = torch.nn.functional.conv2d(Ztch.permute(\u001b[94m0\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m), Wtch.permute(\u001b[94m3\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m), padding=padding, stride=stride)\n",
      "        out2 = out.sum()\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\n",
      "            out2.backward()\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\n",
      "            err1 = np.linalg.norm(Ztch.grad.numpy() - Z.grad.numpy())\n",
      "            err2 = np.linalg.norm(Wtch.grad.numpy() - W.grad.numpy())\n",
      "        err3 = np.linalg.norm(out2.detach().numpy() - y2.numpy())\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\n",
      ">           \u001b[94massert\u001b[39;49;00m err1 < \u001b[94m1e-2\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33minput grads match\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           AssertionError: input grads match\u001b[0m\n",
      "\u001b[1m\u001b[31mE           assert nan < 0.01\u001b[0m\n",
      "\n",
      "W          = needle.Tensor([[[[  0.40910086  -0.64982927  11.320294   ...  -0.12077556\n",
      "      1.428692    -5.5845156 ]\n",
      "   [ -6.89178...-1.5437248  -10.136281  ]\n",
      "   [  1.1511405    0.7441038   16.463472   ...  -6.8931007\n",
      "     -0.5833115   -2.504636  ]]]])\n",
      "W_shape    = (3, 3, 8, 16)\n",
      "Wtch       = tensor([[[[  0.4091,  -0.6498,  11.3203,  ...,  -0.1208,   1.4287,  -5.5845],\n",
      "          [ -6.8918,  -9.3836,  10.6335,... -10.1363],\n",
      "          [  1.1511,   0.7441,  16.4635,  ...,  -6.8931,  -0.5833,  -2.5046]]]],\n",
      "       requires_grad=True)\n",
      "Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n",
      "     4.75044203e+00 -7.56786048e...5e+00]\n",
      "   [ 6.10367346e+00 -2.05434513e+00 -4.41384506e+00 ... -6.15245628e+00\n",
      "     3.33514667e+00 -1.35630560e+00]]]])\n",
      "Z_shape    = (3, 16, 16, 8)\n",
      "Ztch       = tensor([[[[ 8.8203e+00,  2.0008e+00,  4.8937e+00,  ..., -4.8864e+00,\n",
      "            4.7504e+00, -7.5679e-01],\n",
      "          [...[ 6.1037e+00, -2.0543e+00, -4.4138e+00,  ..., -6.1525e+00,\n",
      "            3.3351e+00, -1.3563e+00]]]], requires_grad=True)\n",
      "_W         = array([[[[  0.40910086,  -0.64982927,  11.320294  , ...,  -0.12077556,\n",
      "            1.428692  ,  -5.5845156 ],\n",
      "        ... [  1.1511405 ,   0.7441038 ,  16.463472  , ...,  -6.8931007 ,\n",
      "           -0.5833115 ,  -2.504636  ]]]], dtype=float32)\n",
      "_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\n",
      "          -4.88638926e+00,  4.75044203e+00, -7.56786...434513e+00, -4.41384506e+00, ...,\n",
      "          -6.15245628e+00,  3.33514667e+00, -1.35630560e+00]]]],\n",
      "      dtype=float32)\n",
      "backward   = True\n",
      "device     = cpu()\n",
      "err1       = nan\n",
      "err2       = 0.003127568\n",
      "err3       = 0.056640625\n",
      "out        = tensor([[[[-6.3835e+01, -7.3393e+01,  7.0442e+01,  ..., -8.6601e+01,\n",
      "            2.9473e+01,  3.8723e+01],\n",
      "          [...,  8.9989e+01,  2.4763e+01,  ..., -2.3400e+02,\n",
      "            1.3548e+02, -2.8245e+01]]]], grad_fn=<ConvolutionBackward0>)\n",
      "out2       = tensor(23346.6914, grad_fn=<SumBackward0>)\n",
      "padding    = 2\n",
      "stride     = 1\n",
      "torch      = <module 'torch' from '/home/ubuntu/anaconda3/envs/mlsys_hw/lib/python3.8/site-packages/torch/__init__.py'>\n",
      "y          = needle.Tensor([[[[-6.38349190e+01  1.26404709e+02 -6.92413330e+01 ...  7.84971542e+01\n",
      "     5.02614059e+01 -1.00136520e...1e+02]\n",
      "   [-7.55530777e+01 -3.63743286e+01  5.69509811e+01 ...  6.93700714e+01\n",
      "    -3.12737598e+01 -2.82451649e+01]]]])\n",
      "y2         = needle.Tensor([23346.748])\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_conv.py\u001b[0m:446: AssertionError\n",
      "----------------------------- Captured stdout call -----------------------------\n",
      "out_grad shape: (3, 18, 18, 16)\n",
      "stride: 1\n",
      "_out_grad shape: (3, 18, 18, 16)\n",
      "W shape: (3, 3, 8, 16)\n",
      "_W shape: (3, 3, 16, 8)\n",
      "matmul tiled...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "X_grad shape: (3, 16, 16, 8)\n",
      "X_shape: (3, 16, 16, 8)\n",
      "_X shape: (8, 16, 16, 3)\n",
      "_out_grad shape: (18, 18, 3, 16)\n",
      "W_grad shape: (8, 3, 3, 16)\n",
      "W_grad shape: (3, 3, 8, 16)\n",
      "\u001b[31m\u001b[1m_ test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape3-W_shape3-1-0] _\u001b[0m\n",
      "\n",
      "Z_shape = (3, 16, 16, 8), W_shape = (3, 3, 8, 14), stride = 1, padding = 0\n",
      "backward = True, device = cpu()\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\r\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\r\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\r\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\r\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\r\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\r\n",
      "        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\r\n",
      "        _Z = _Z.astype(np.float32)\r\n",
      "        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\r\n",
      "        _W = _W.astype(np.float32)\r\n",
      "        Z = ndl.Tensor(_Z, device=device)\r\n",
      "        W = ndl.Tensor(_W, device=device)\r\n",
      "        y = ndl.conv(Z, W, padding=padding, stride=stride)\r\n",
      "        y2 = y.sum()\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      "            y2.backward()\r\n",
      "        Ztch = torch.Tensor(_Z).float()\r\n",
      "        Ztch.requires_grad=\u001b[94mTrue\u001b[39;49;00m\r\n",
      "        Wtch = torch.Tensor(_W).float()\r\n",
      "        Wtch.requires_grad=\u001b[94mTrue\u001b[39;49;00m\r\n",
      "        out = torch.nn.functional.conv2d(Ztch.permute(\u001b[94m0\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m), Wtch.permute(\u001b[94m3\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m), padding=padding, stride=stride)\r\n",
      "        out2 = out.sum()\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      "            out2.backward()\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      "            err1 = np.linalg.norm(Ztch.grad.numpy() - Z.grad.numpy())\r\n",
      "            err2 = np.linalg.norm(Wtch.grad.numpy() - W.grad.numpy())\r\n",
      "        err3 = np.linalg.norm(out2.detach().numpy() - y2.numpy())\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      ">           \u001b[94massert\u001b[39;49;00m err1 < \u001b[94m1e-2\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33minput grads match\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "\u001b[1m\u001b[31mE           AssertionError: input grads match\u001b[0m\r\n",
      "\u001b[1m\u001b[31mE           assert nan < 0.01\u001b[0m\r\n",
      "\r\n",
      "W          = needle.Tensor([[[[ 4.09100860e-01 -6.49829268e-01  1.13202944e+01 ... -5.56089449e+00\r\n",
      "    -4.84385538e+00 -1.20775558e...5e+00]\r\n",
      "   [-1.10714078e+00  2.54448557e+00 -7.73867989e+00 ... -1.13336074e+00\r\n",
      "    -1.21658230e+00 -4.79330921e+00]]]])\r\n",
      "W_shape    = (3, 3, 8, 14)\r\n",
      "Wtch       = tensor([[[[ 4.0910e-01, -6.4983e-01,  1.1320e+01,  ..., -5.5609e+00,\r\n",
      "           -4.8439e+00, -1.2078e-01],\r\n",
      "          [...[-1.1071e+00,  2.5445e+00, -7.7387e+00,  ..., -1.1334e+00,\r\n",
      "           -1.2166e+00, -4.7933e+00]]]], requires_grad=True)\r\n",
      "Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\r\n",
      "     4.75044203e+00 -7.56786048e...5e+00]\r\n",
      "   [ 6.10367346e+00 -2.05434513e+00 -4.41384506e+00 ... -6.15245628e+00\r\n",
      "     3.33514667e+00 -1.35630560e+00]]]])\r\n",
      "Z_shape    = (3, 16, 16, 8)\r\n",
      "Ztch       = tensor([[[[ 8.8203e+00,  2.0008e+00,  4.8937e+00,  ..., -4.8864e+00,\r\n",
      "            4.7504e+00, -7.5679e-01],\r\n",
      "          [...[ 6.1037e+00, -2.0543e+00, -4.4138e+00,  ..., -6.1525e+00,\r\n",
      "            3.3351e+00, -1.3563e+00]]]], requires_grad=True)\r\n",
      "_W         = array([[[[ 4.09100860e-01, -6.49829268e-01,  1.13202944e+01, ...,\r\n",
      "          -5.56089449e+00, -4.84385538e+00, -1.20775...448557e+00, -7.73867989e+00, ...,\r\n",
      "          -1.13336074e+00, -1.21658230e+00, -4.79330921e+00]]]],\r\n",
      "      dtype=float32)\r\n",
      "_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\r\n",
      "          -4.88638926e+00,  4.75044203e+00, -7.56786...434513e+00, -4.41384506e+00, ...,\r\n",
      "          -6.15245628e+00,  3.33514667e+00, -1.35630560e+00]]]],\r\n",
      "      dtype=float32)\r\n",
      "backward   = True\r\n",
      "device     = cpu()\r\n",
      "err1       = nan\r\n",
      "err2       = 0.0018837381\r\n",
      "err3       = 0.049804688\r\n",
      "out        = tensor([[[[ 5.7346e+00, -2.1858e+02, -5.5911e+01,  ..., -8.9695e+01,\r\n",
      "            2.2620e+02,  3.4342e+02],\r\n",
      "          [...,  2.3405e+01, -2.1477e+01,  ...,  1.2947e+02,\r\n",
      "            1.2442e+02, -4.6003e+02]]]], grad_fn=<ConvolutionBackward0>)\r\n",
      "out2       = tensor(16161.7031, grad_fn=<SumBackward0>)\r\n",
      "padding    = 0\r\n",
      "stride     = 1\r\n",
      "torch      = <module 'torch' from '/home/ubuntu/anaconda3/envs/mlsys_hw/lib/python3.8/site-packages/torch/__init__.py'>\r\n",
      "y          = needle.Tensor([[[[ 5.73456383e+00 -2.22225971e+01  2.64719360e+02 ... -6.97373962e+01\r\n",
      "    -3.79622650e+02  9.98186493e...8e+02]\r\n",
      "   [ 3.72021103e+01  8.33753281e+01 -4.17866564e+00 ... -1.67722427e+02\r\n",
      "    -1.28576309e+02 -4.60030090e+02]]]])\r\n",
      "y2         = needle.Tensor([16161.753])\r\n",
      "\r\n",
      "\u001b[1m\u001b[31mtests/test_conv.py\u001b[0m:446: AssertionError\r\n",
      "----------------------------- Captured stdout call -----------------------------\r\n",
      "out_grad shape: (3, 14, 14, 14)\r\n",
      "stride: 1\r\n",
      "_out_grad shape: (3, 14, 14, 14)\r\n",
      "W shape: (3, 3, 8, 14)\r\n",
      "_W shape: (3, 3, 14, 8)\r\n",
      "X_grad shape: (3, 16, 16, 8)\r\n",
      "X_shape: (3, 16, 16, 8)\r\n",
      "_X shape: (8, 16, 16, 3)\r\n",
      "_out_grad shape: (14, 14, 3, 14)\r\n",
      "W_grad shape: (8, 3, 3, 14)\r\n",
      "W_grad shape: (3, 3, 8, 14)\r\n",
      "\u001b[31m\u001b[1m_ test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape4-W_shape4-1-0] _\u001b[0m\r\n",
      "\r\n",
      "Z_shape = (3, 16, 16, 2), W_shape = (3, 3, 2, 14), stride = 1, padding = 0\r\n",
      "backward = True, device = cpu()\r\n",
      "\r\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\r\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\r\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\r\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\r\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\r\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\r\n",
      "        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\r\n",
      "        _Z = _Z.astype(np.float32)\r\n",
      "        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\r\n",
      "        _W = _W.astype(np.float32)\r\n",
      "        Z = ndl.Tensor(_Z, device=device)\r\n",
      "        W = ndl.Tensor(_W, device=device)\r\n",
      "        y = ndl.conv(Z, W, padding=padding, stride=stride)\r\n",
      "        y2 = y.sum()\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      "            y2.backward()\r\n",
      "        Ztch = torch.Tensor(_Z).float()\r\n",
      "        Ztch.requires_grad=\u001b[94mTrue\u001b[39;49;00m\r\n",
      "        Wtch = torch.Tensor(_W).float()\r\n",
      "        Wtch.requires_grad=\u001b[94mTrue\u001b[39;49;00m\r\n",
      "        out = torch.nn.functional.conv2d(Ztch.permute(\u001b[94m0\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m), Wtch.permute(\u001b[94m3\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m), padding=padding, stride=stride)\r\n",
      "        out2 = out.sum()\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      "            out2.backward()\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      "            err1 = np.linalg.norm(Ztch.grad.numpy() - Z.grad.numpy())\r\n",
      "            err2 = np.linalg.norm(Wtch.grad.numpy() - W.grad.numpy())\r\n",
      "        err3 = np.linalg.norm(out2.detach().numpy() - y2.numpy())\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      ">           \u001b[94massert\u001b[39;49;00m err1 < \u001b[94m1e-2\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33minput grads match\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "\u001b[1m\u001b[31mE           AssertionError: input grads match\u001b[0m\r\n",
      "\u001b[1m\u001b[31mE           assert nan < 0.01\u001b[0m\r\n",
      "\r\n",
      "W          = needle.Tensor([[[[ -8.0968      -2.5552022    8.703147    -1.4674252    4.5861077\r\n",
      "     -0.28521433   4.383634    -9.13...727333   0.22582927   9.296732    -8.13161     -0.67411226\r\n",
      "     -2.9204676    1.675528   -12.187821     5.5746226 ]]]])\r\n",
      "W_shape    = (3, 3, 2, 14)\r\n",
      "Wtch       = tensor([[[[ -8.0968,  -2.5552,   8.7031,  -1.4674,   4.5861,  -0.2852,   4.3836,\r\n",
      "            -9.1346,  -2.0159,   4.74...0.2258,\r\n",
      "             9.2967,  -8.1316,  -0.6741,  -2.9205,   1.6755, -12.1878,   5.5746]]]],\r\n",
      "       requires_grad=True)\r\n",
      "Z          = needle.Tensor([[[[  8.820262     2.000786  ]\r\n",
      "   [  4.89369     11.204466  ]\r\n",
      "   [  9.33779     -4.8863893 ]\r\n",
      "   ...\r\n",
      "   [...   -5.112822  ]\r\n",
      "   ...\r\n",
      "   [ -7.922368     4.2222714 ]\r\n",
      "   [ -6.064339     1.4188478 ]\r\n",
      "   [ -1.4109794   -5.791016  ]]]])\r\n",
      "Z_shape    = (3, 16, 16, 2)\r\n",
      "Ztch       = tensor([[[[  8.8203,   2.0008],\r\n",
      "          [  4.8937,  11.2045],\r\n",
      "          [  9.3378,  -4.8864],\r\n",
      "          ...,\r\n",
      "       ...\r\n",
      "          [ -7.9224,   4.2223],\r\n",
      "          [ -6.0643,   1.4188],\r\n",
      "          [ -1.4110,  -5.7910]]]], requires_grad=True)\r\n",
      "_W         = array([[[[ -8.0968    ,  -2.5552022 ,   8.703147  ,  -1.4674252 ,\r\n",
      "            4.5861077 ,  -0.28521433,   4.383634  , ...        -8.13161   ,  -0.67411226,  -2.9204676 ,   1.675528  ,\r\n",
      "          -12.187821  ,   5.5746226 ]]]], dtype=float32)\r\n",
      "_Z         = array([[[[  8.820262  ,   2.000786  ],\r\n",
      "         [  4.89369   ,  11.204466  ],\r\n",
      "         [  9.33779   ,  -4.8863893 ],\r\n",
      " ...22368  ,   4.2222714 ],\r\n",
      "         [ -6.064339  ,   1.4188478 ],\r\n",
      "         [ -1.4109794 ,  -5.791016  ]]]], dtype=float32)\r\n",
      "backward   = True\r\n",
      "device     = cpu()\r\n",
      "err1       = nan\r\n",
      "err2       = 0.0006921925\r\n",
      "err3       = 0.0014648438\r\n",
      "out        = tensor([[[[-346.0716,  -28.1759,   63.5063,  ..., -158.1952,  -57.8746,\r\n",
      "             63.6776],\r\n",
      "          [  73.3426,  ...[  58.9731,    3.5575, -155.4996,  ...,  123.7366,   54.5836,\r\n",
      "             92.4735]]]], grad_fn=<ConvolutionBackward0>)\r\n",
      "out2       = tensor(-6321.1787, grad_fn=<SumBackward0>)\r\n",
      "padding    = 0\r\n",
      "stride     = 1\r\n",
      "torch      = <module 'torch' from '/home/ubuntu/anaconda3/envs/mlsys_hw/lib/python3.8/site-packages/torch/__init__.py'>\r\n",
      "y          = needle.Tensor([[[[-346.07162     -1.1081867  420.15115   ...  -58.583153\r\n",
      "     -66.19938     33.158627 ]\r\n",
      "   [ -28.17595... -8.201235    54.58357  ]\r\n",
      "   [ -17.069721  -119.112015    24.423635  ...   47.367493\r\n",
      "     -38.16554     92.47348  ]]]])\r\n",
      "y2         = needle.Tensor([-6321.18])\r\n",
      "\r\n",
      "\u001b[1m\u001b[31mtests/test_conv.py\u001b[0m:446: AssertionError\r\n",
      "----------------------------- Captured stdout call -----------------------------\r\n",
      "out_grad shape: (3, 14, 14, 14)\r\n",
      "stride: 1\r\n",
      "_out_grad shape: (3, 14, 14, 14)\r\n",
      "W shape: (3, 3, 2, 14)\r\n",
      "_W shape: (3, 3, 14, 2)\r\n",
      "X_grad shape: (3, 16, 16, 2)\r\n",
      "X_shape: (3, 16, 16, 2)\r\n",
      "_X shape: (2, 16, 16, 3)\r\n",
      "_out_grad shape: (14, 14, 3, 14)\r\n",
      "W_grad shape: (2, 3, 3, 14)\r\n",
      "W_grad shape: (3, 3, 2, 14)\r\n",
      "\u001b[31m\u001b[1m_ test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape5-W_shape5-2-0] _\u001b[0m\r\n",
      "\r\n",
      "Z_shape = (3, 14, 14, 8), W_shape = (3, 3, 8, 16), stride = 2, padding = 0\r\n",
      "backward = True, device = cpu()\r\n",
      "\r\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\r\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\r\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\r\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\r\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\r\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\r\n",
      "        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\r\n",
      "        _Z = _Z.astype(np.float32)\r\n",
      "        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\r\n",
      "        _W = _W.astype(np.float32)\r\n",
      "        Z = ndl.Tensor(_Z, device=device)\r\n",
      "        W = ndl.Tensor(_W, device=device)\r\n",
      "        y = ndl.conv(Z, W, padding=padding, stride=stride)\r\n",
      "        y2 = y.sum()\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      "            y2.backward()\r\n",
      "        Ztch = torch.Tensor(_Z).float()\r\n",
      "        Ztch.requires_grad=\u001b[94mTrue\u001b[39;49;00m\r\n",
      "        Wtch = torch.Tensor(_W).float()\r\n",
      "        Wtch.requires_grad=\u001b[94mTrue\u001b[39;49;00m\r\n",
      "        out = torch.nn.functional.conv2d(Ztch.permute(\u001b[94m0\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m), Wtch.permute(\u001b[94m3\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m), padding=padding, stride=stride)\r\n",
      "        out2 = out.sum()\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      "            out2.backward()\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      "            err1 = np.linalg.norm(Ztch.grad.numpy() - Z.grad.numpy())\r\n",
      "            err2 = np.linalg.norm(Wtch.grad.numpy() - W.grad.numpy())\r\n",
      "        err3 = np.linalg.norm(out2.detach().numpy() - y2.numpy())\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      ">           \u001b[94massert\u001b[39;49;00m err1 < \u001b[94m1e-2\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33minput grads match\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "\u001b[1m\u001b[31mE           AssertionError: input grads match\u001b[0m\r\n",
      "\u001b[1m\u001b[31mE           assert inf < 0.01\u001b[0m\r\n",
      "\r\n",
      "W          = needle.Tensor([[[[  4.282669     8.133604    -4.6223483  ...   2.0798197\r\n",
      "      2.1704152   -0.38803983]\r\n",
      "   [  2.245844...2.618842     0.71487164]\r\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\r\n",
      "      0.03223436   2.0512383 ]]]])\r\n",
      "W_shape    = (3, 3, 8, 16)\r\n",
      "Wtch       = tensor([[[[  4.2827,   8.1336,  -4.6223,  ...,   2.0798,   2.1704,  -0.3880],\r\n",
      "          [  2.2458,  -0.6565,  -3.7547,...   0.7149],\r\n",
      "          [ -4.0937,   3.7292, -10.5518,  ...,   0.3174,   0.0322,   2.0512]]]],\r\n",
      "       requires_grad=True)\r\n",
      "Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\r\n",
      "     4.75044203e+00 -7.56786048e...3e+00]\r\n",
      "   [ 1.09905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\r\n",
      "    -8.48226726e-01 -2.29984760e+00]]]])\r\n",
      "Z_shape    = (3, 14, 14, 8)\r\n",
      "Ztch       = tensor([[[[ 8.8203e+00,  2.0008e+00,  4.8937e+00,  ..., -4.8864e+00,\r\n",
      "            4.7504e+00, -7.5679e-01],\r\n",
      "          [...[ 1.0991e+01,  2.2317e+00,  4.6206e+00,  ..., -6.2309e-01,\r\n",
      "           -8.4823e-01, -2.2998e+00]]]], requires_grad=True)\r\n",
      "_W         = array([[[[  4.282669  ,   8.133604  ,  -4.6223483 , ...,   2.0798197 ,\r\n",
      "            2.1704152 ,  -0.38803983],\r\n",
      "        ... [ -4.09374   ,   3.7291534 , -10.551835  , ...,   0.31741843,\r\n",
      "            0.03223436,   2.0512383 ]]]], dtype=float32)\r\n",
      "_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\r\n",
      "          -4.88638926e+00,  4.75044203e+00, -7.56786...166728e+00,  4.62055349e+00, ...,\r\n",
      "          -6.23090088e-01, -8.48226726e-01, -2.29984760e+00]]]],\r\n",
      "      dtype=float32)\r\n",
      "backward   = True\r\n",
      "device     = cpu()\r\n",
      "err1       = inf\r\n",
      "err2       = 1798.237\r\n",
      "err3       = 0.0029296875\r\n",
      "out        = tensor([[[[ -21.8508, -294.9621, -318.0497,   -4.7934,   59.4825, -104.8043],\r\n",
      "          [  14.3621,  303.2643, -221.53...          [ 183.8510, -115.7199,  115.7395, -105.5507,   75.9456, -171.5427]]]],\r\n",
      "       grad_fn=<ConvolutionBackward0>)\r\n",
      "out2       = tensor(-4902.2178, grad_fn=<SumBackward0>)\r\n",
      "padding    = 0\r\n",
      "stride     = 2\r\n",
      "torch      = <module 'torch' from '/home/ubuntu/anaconda3/envs/mlsys_hw/lib/python3.8/site-packages/torch/__init__.py'>\r\n",
      "y          = needle.Tensor([[[[ -21.850786    71.05673   -219.25307   ...  122.64386\r\n",
      "     109.58178     52.959953 ]\r\n",
      "   [-294.9621  ...-120.10991     75.945595 ]\r\n",
      "   [ 372.3654    -347.1187     253.12747   ...  220.83806\r\n",
      "     113.929146  -171.54272  ]]]])\r\n",
      "y2         = needle.Tensor([-4902.2207])\r\n",
      "\r\n",
      "\u001b[1m\u001b[31mtests/test_conv.py\u001b[0m:446: AssertionError\r\n",
      "----------------------------- Captured stdout call -----------------------------\r\n",
      "out_grad shape: (3, 6, 6, 16)\r\n",
      "stride: 2\r\n",
      "_out_grad shape: (3, 12, 12, 16)\r\n",
      "W shape: (3, 3, 8, 16)\r\n",
      "_W shape: (3, 3, 16, 8)\r\n",
      "X_grad shape: (3, 14, 14, 8)\r\n",
      "X_shape: (3, 14, 14, 8)\r\n",
      "_X shape: (8, 14, 14, 3)\r\n",
      "_out_grad shape: (12, 12, 3, 16)\r\n",
      "matmul tiled...\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "W_grad shape: (8, 3, 3, 16)\r\n",
      "W_grad shape: (3, 3, 8, 16)\r\n",
      "\u001b[31m\u001b[1m_ test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape6-W_shape6-2-1] _\u001b[0m\r\n",
      "\r\n",
      "Z_shape = (3, 14, 14, 8), W_shape = (3, 3, 8, 16), stride = 2, padding = 1\r\n",
      "backward = True, device = cpu()\r\n",
      "\r\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\r\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\r\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\r\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\r\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\r\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\r\n",
      "        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\r\n",
      "        _Z = _Z.astype(np.float32)\r\n",
      "        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\r\n",
      "        _W = _W.astype(np.float32)\r\n",
      "        Z = ndl.Tensor(_Z, device=device)\r\n",
      "        W = ndl.Tensor(_W, device=device)\r\n",
      "        y = ndl.conv(Z, W, padding=padding, stride=stride)\r\n",
      "        y2 = y.sum()\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      "            y2.backward()\r\n",
      "        Ztch = torch.Tensor(_Z).float()\r\n",
      "        Ztch.requires_grad=\u001b[94mTrue\u001b[39;49;00m\r\n",
      "        Wtch = torch.Tensor(_W).float()\r\n",
      "        Wtch.requires_grad=\u001b[94mTrue\u001b[39;49;00m\r\n",
      "        out = torch.nn.functional.conv2d(Ztch.permute(\u001b[94m0\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m), Wtch.permute(\u001b[94m3\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m), padding=padding, stride=stride)\r\n",
      "        out2 = out.sum()\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      "            out2.backward()\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      "            err1 = np.linalg.norm(Ztch.grad.numpy() - Z.grad.numpy())\r\n",
      "            err2 = np.linalg.norm(Wtch.grad.numpy() - W.grad.numpy())\r\n",
      "        err3 = np.linalg.norm(out2.detach().numpy() - y2.numpy())\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      ">           \u001b[94massert\u001b[39;49;00m err1 < \u001b[94m1e-2\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33minput grads match\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "\u001b[1m\u001b[31mE           AssertionError: input grads match\u001b[0m\r\n",
      "\u001b[1m\u001b[31mE           assert inf < 0.01\u001b[0m\r\n",
      "\r\n",
      "W          = needle.Tensor([[[[  4.282669     8.133604    -4.6223483  ...   2.0798197\r\n",
      "      2.1704152   -0.38803983]\r\n",
      "   [  2.245844...2.618842     0.71487164]\r\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\r\n",
      "      0.03223436   2.0512383 ]]]])\r\n",
      "W_shape    = (3, 3, 8, 16)\r\n",
      "Wtch       = tensor([[[[  4.2827,   8.1336,  -4.6223,  ...,   2.0798,   2.1704,  -0.3880],\r\n",
      "          [  2.2458,  -0.6565,  -3.7547,...   0.7149],\r\n",
      "          [ -4.0937,   3.7292, -10.5518,  ...,   0.3174,   0.0322,   2.0512]]]],\r\n",
      "       requires_grad=True)\r\n",
      "Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\r\n",
      "     4.75044203e+00 -7.56786048e...3e+00]\r\n",
      "   [ 1.09905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\r\n",
      "    -8.48226726e-01 -2.29984760e+00]]]])\r\n",
      "Z_shape    = (3, 14, 14, 8)\r\n",
      "Ztch       = tensor([[[[ 8.8203e+00,  2.0008e+00,  4.8937e+00,  ..., -4.8864e+00,\r\n",
      "            4.7504e+00, -7.5679e-01],\r\n",
      "          [...[ 1.0991e+01,  2.2317e+00,  4.6206e+00,  ..., -6.2309e-01,\r\n",
      "           -8.4823e-01, -2.2998e+00]]]], requires_grad=True)\r\n",
      "_W         = array([[[[  4.282669  ,   8.133604  ,  -4.6223483 , ...,   2.0798197 ,\r\n",
      "            2.1704152 ,  -0.38803983],\r\n",
      "        ... [ -4.09374   ,   3.7291534 , -10.551835  , ...,   0.31741843,\r\n",
      "            0.03223436,   2.0512383 ]]]], dtype=float32)\r\n",
      "_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\r\n",
      "          -4.88638926e+00,  4.75044203e+00, -7.56786...166728e+00,  4.62055349e+00, ...,\r\n",
      "          -6.23090088e-01, -8.48226726e-01, -2.29984760e+00]]]],\r\n",
      "      dtype=float32)\r\n",
      "backward   = True\r\n",
      "device     = cpu()\r\n",
      "err1       = inf\r\n",
      "err2       = 2246.7993\r\n",
      "err3       = 0.0034179688\r\n",
      "out        = tensor([[[[ 1.0725e+02, -9.3744e+00,  2.7834e+02,  ..., -9.7155e+01,\r\n",
      "            1.3371e+02,  1.2725e+02],\r\n",
      "          [...,  5.7277e+02,  2.3476e+01,  ..., -2.1788e+02,\r\n",
      "           -1.8747e+02,  2.0660e+02]]]], grad_fn=<ConvolutionBackward0>)\r\n",
      "out2       = tensor(4313.9385, grad_fn=<SumBackward0>)\r\n",
      "padding    = 1\r\n",
      "stride     = 2\r\n",
      "torch      = <module 'torch' from '/home/ubuntu/anaconda3/envs/mlsys_hw/lib/python3.8/site-packages/torch/__init__.py'>\r\n",
      "y          = needle.Tensor([[[[ 1.07254478e+02  3.12460289e+01 -2.13923538e+02 ... -2.92673912e+01\r\n",
      "     9.05523605e+01  2.02318192e...9e+02]\r\n",
      "   [ 1.20957283e+02 -4.53491516e+02  7.81254578e+01 ...  1.71220749e+02\r\n",
      "    -1.71809418e+02  2.06600098e+02]]]])\r\n",
      "y2         = needle.Tensor([4313.935])\r\n",
      "\r\n",
      "\u001b[1m\u001b[31mtests/test_conv.py\u001b[0m:446: AssertionError\r\n",
      "----------------------------- Captured stdout call -----------------------------\r\n",
      "out_grad shape: (3, 7, 7, 16)\r\n",
      "stride: 2\r\n",
      "_out_grad shape: (3, 14, 14, 16)\r\n",
      "W shape: (3, 3, 8, 16)\r\n",
      "_W shape: (3, 3, 16, 8)\r\n",
      "X_grad shape: (3, 14, 14, 8)\r\n",
      "X_shape: (3, 14, 14, 8)\r\n",
      "_X shape: (8, 14, 14, 3)\r\n",
      "_out_grad shape: (14, 14, 3, 16)\r\n",
      "W_grad shape: (8, 3, 3, 16)\r\n",
      "W_grad shape: (3, 3, 8, 16)\r\n",
      "\u001b[31m\u001b[1m_ test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape7-W_shape7-2-2] _\u001b[0m\r\n",
      "\r\n",
      "Z_shape = (3, 16, 16, 8), W_shape = (3, 3, 8, 16), stride = 2, padding = 2\r\n",
      "backward = True, device = cpu()\r\n",
      "\r\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\r\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\r\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\r\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\r\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\r\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\r\n",
      "        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\r\n",
      "        _Z = _Z.astype(np.float32)\r\n",
      "        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\r\n",
      "        _W = _W.astype(np.float32)\r\n",
      "        Z = ndl.Tensor(_Z, device=device)\r\n",
      "        W = ndl.Tensor(_W, device=device)\r\n",
      "        y = ndl.conv(Z, W, padding=padding, stride=stride)\r\n",
      "        y2 = y.sum()\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      "            y2.backward()\r\n",
      "        Ztch = torch.Tensor(_Z).float()\r\n",
      "        Ztch.requires_grad=\u001b[94mTrue\u001b[39;49;00m\r\n",
      "        Wtch = torch.Tensor(_W).float()\r\n",
      "        Wtch.requires_grad=\u001b[94mTrue\u001b[39;49;00m\r\n",
      "        out = torch.nn.functional.conv2d(Ztch.permute(\u001b[94m0\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m), Wtch.permute(\u001b[94m3\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m), padding=padding, stride=stride)\r\n",
      "        out2 = out.sum()\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      "            out2.backward()\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      "            err1 = np.linalg.norm(Ztch.grad.numpy() - Z.grad.numpy())\r\n",
      "            err2 = np.linalg.norm(Wtch.grad.numpy() - W.grad.numpy())\r\n",
      "        err3 = np.linalg.norm(out2.detach().numpy() - y2.numpy())\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      ">           \u001b[94massert\u001b[39;49;00m err1 < \u001b[94m1e-2\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33minput grads match\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "\u001b[1m\u001b[31mE           AssertionError: input grads match\u001b[0m\r\n",
      "\u001b[1m\u001b[31mE           assert inf < 0.01\u001b[0m\r\n",
      "\r\n",
      "W          = needle.Tensor([[[[  0.40910086  -0.64982927  11.320294   ...  -0.12077556\r\n",
      "      1.428692    -5.5845156 ]\r\n",
      "   [ -6.89178...-1.5437248  -10.136281  ]\r\n",
      "   [  1.1511405    0.7441038   16.463472   ...  -6.8931007\r\n",
      "     -0.5833115   -2.504636  ]]]])\r\n",
      "W_shape    = (3, 3, 8, 16)\r\n",
      "Wtch       = tensor([[[[  0.4091,  -0.6498,  11.3203,  ...,  -0.1208,   1.4287,  -5.5845],\r\n",
      "          [ -6.8918,  -9.3836,  10.6335,... -10.1363],\r\n",
      "          [  1.1511,   0.7441,  16.4635,  ...,  -6.8931,  -0.5833,  -2.5046]]]],\r\n",
      "       requires_grad=True)\r\n",
      "Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\r\n",
      "     4.75044203e+00 -7.56786048e...5e+00]\r\n",
      "   [ 6.10367346e+00 -2.05434513e+00 -4.41384506e+00 ... -6.15245628e+00\r\n",
      "     3.33514667e+00 -1.35630560e+00]]]])\r\n",
      "Z_shape    = (3, 16, 16, 8)\r\n",
      "Ztch       = tensor([[[[ 8.8203e+00,  2.0008e+00,  4.8937e+00,  ..., -4.8864e+00,\r\n",
      "            4.7504e+00, -7.5679e-01],\r\n",
      "          [...[ 6.1037e+00, -2.0543e+00, -4.4138e+00,  ..., -6.1525e+00,\r\n",
      "            3.3351e+00, -1.3563e+00]]]], requires_grad=True)\r\n",
      "_W         = array([[[[  0.40910086,  -0.64982927,  11.320294  , ...,  -0.12077556,\r\n",
      "            1.428692  ,  -5.5845156 ],\r\n",
      "        ... [  1.1511405 ,   0.7441038 ,  16.463472  , ...,  -6.8931007 ,\r\n",
      "           -0.5833115 ,  -2.504636  ]]]], dtype=float32)\r\n",
      "_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\r\n",
      "          -4.88638926e+00,  4.75044203e+00, -7.56786...434513e+00, -4.41384506e+00, ...,\r\n",
      "          -6.15245628e+00,  3.33514667e+00, -1.35630560e+00]]]],\r\n",
      "      dtype=float32)\r\n",
      "backward   = True\r\n",
      "device     = cpu()\r\n",
      "err1       = inf\r\n",
      "err2       = 2713.6147\r\n",
      "err3       = 0.0078125\r\n",
      "out        = tensor([[[[-6.3835e+01,  7.0442e+01, -1.5787e+02,  ..., -1.1571e+02,\r\n",
      "            2.7167e+02,  2.9473e+01],\r\n",
      "          [...,  1.5725e+02, -4.1424e+01,  ...,  1.6422e+02,\r\n",
      "            1.0298e+02,  8.0763e+01]]]], grad_fn=<ConvolutionBackward0>)\r\n",
      "out2       = tensor(-8988.4932, grad_fn=<SumBackward0>)\r\n",
      "padding    = 2\r\n",
      "stride     = 2\r\n",
      "torch      = <module 'torch' from '/home/ubuntu/anaconda3/envs/mlsys_hw/lib/python3.8/site-packages/torch/__init__.py'>\r\n",
      "y          = needle.Tensor([[[[-6.38349190e+01  1.26404709e+02 -6.92413330e+01 ...  7.84971542e+01\r\n",
      "     5.02614059e+01 -1.00136520e...3e+02]\r\n",
      "   [-8.42394543e+00 -1.02299728e+02 -4.70671501e+01 ...  7.48113098e+01\r\n",
      "     7.02560806e+01  8.07628021e+01]]]])\r\n",
      "y2         = needle.Tensor([-8988.485])\r\n",
      "\r\n",
      "\u001b[1m\u001b[31mtests/test_conv.py\u001b[0m:446: AssertionError\r\n",
      "----------------------------- Captured stdout call -----------------------------\r\n",
      "out_grad shape: (3, 9, 9, 16)\r\n",
      "stride: 2\r\n",
      "_out_grad shape: (3, 18, 18, 16)\r\n",
      "W shape: (3, 3, 8, 16)\r\n",
      "_W shape: (3, 3, 16, 8)\r\n",
      "matmul tiled...\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "X_grad shape: (3, 16, 16, 8)\r\n",
      "X_shape: (3, 16, 16, 8)\r\n",
      "_X shape: (8, 16, 16, 3)\r\n",
      "_out_grad shape: (18, 18, 3, 16)\r\n",
      "W_grad shape: (8, 3, 3, 16)\r\n",
      "W_grad shape: (3, 3, 8, 16)\r\n",
      "\u001b[31m\u001b[1m_ test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape8-W_shape8-2-0] _\u001b[0m\r\n",
      "\r\n",
      "Z_shape = (3, 16, 16, 8), W_shape = (3, 3, 8, 14), stride = 2, padding = 0\r\n",
      "backward = True, device = cpu()\r\n",
      "\r\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\r\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\r\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\r\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\r\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\r\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\r\n",
      "        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\r\n",
      "        _Z = _Z.astype(np.float32)\r\n",
      "        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\r\n",
      "        _W = _W.astype(np.float32)\r\n",
      "        Z = ndl.Tensor(_Z, device=device)\r\n",
      "        W = ndl.Tensor(_W, device=device)\r\n",
      "        y = ndl.conv(Z, W, padding=padding, stride=stride)\r\n",
      "        y2 = y.sum()\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      "            y2.backward()\r\n",
      "        Ztch = torch.Tensor(_Z).float()\r\n",
      "        Ztch.requires_grad=\u001b[94mTrue\u001b[39;49;00m\r\n",
      "        Wtch = torch.Tensor(_W).float()\r\n",
      "        Wtch.requires_grad=\u001b[94mTrue\u001b[39;49;00m\r\n",
      "        out = torch.nn.functional.conv2d(Ztch.permute(\u001b[94m0\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m), Wtch.permute(\u001b[94m3\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m), padding=padding, stride=stride)\r\n",
      "        out2 = out.sum()\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      "            out2.backward()\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      "            err1 = np.linalg.norm(Ztch.grad.numpy() - Z.grad.numpy())\r\n",
      "            err2 = np.linalg.norm(Wtch.grad.numpy() - W.grad.numpy())\r\n",
      "        err3 = np.linalg.norm(out2.detach().numpy() - y2.numpy())\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      ">           \u001b[94massert\u001b[39;49;00m err1 < \u001b[94m1e-2\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33minput grads match\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "\u001b[1m\u001b[31mE           AssertionError: input grads match\u001b[0m\r\n",
      "\u001b[1m\u001b[31mE           assert nan < 0.01\u001b[0m\r\n",
      "\r\n",
      "W          = needle.Tensor([[[[ 4.09100860e-01 -6.49829268e-01  1.13202944e+01 ... -5.56089449e+00\r\n",
      "    -4.84385538e+00 -1.20775558e...5e+00]\r\n",
      "   [-1.10714078e+00  2.54448557e+00 -7.73867989e+00 ... -1.13336074e+00\r\n",
      "    -1.21658230e+00 -4.79330921e+00]]]])\r\n",
      "W_shape    = (3, 3, 8, 14)\r\n",
      "Wtch       = tensor([[[[ 4.0910e-01, -6.4983e-01,  1.1320e+01,  ..., -5.5609e+00,\r\n",
      "           -4.8439e+00, -1.2078e-01],\r\n",
      "          [...[-1.1071e+00,  2.5445e+00, -7.7387e+00,  ..., -1.1334e+00,\r\n",
      "           -1.2166e+00, -4.7933e+00]]]], requires_grad=True)\r\n",
      "Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\r\n",
      "     4.75044203e+00 -7.56786048e...5e+00]\r\n",
      "   [ 6.10367346e+00 -2.05434513e+00 -4.41384506e+00 ... -6.15245628e+00\r\n",
      "     3.33514667e+00 -1.35630560e+00]]]])\r\n",
      "Z_shape    = (3, 16, 16, 8)\r\n",
      "Ztch       = tensor([[[[ 8.8203e+00,  2.0008e+00,  4.8937e+00,  ..., -4.8864e+00,\r\n",
      "            4.7504e+00, -7.5679e-01],\r\n",
      "          [...[ 6.1037e+00, -2.0543e+00, -4.4138e+00,  ..., -6.1525e+00,\r\n",
      "            3.3351e+00, -1.3563e+00]]]], requires_grad=True)\r\n",
      "_W         = array([[[[ 4.09100860e-01, -6.49829268e-01,  1.13202944e+01, ...,\r\n",
      "          -5.56089449e+00, -4.84385538e+00, -1.20775...448557e+00, -7.73867989e+00, ...,\r\n",
      "          -1.13336074e+00, -1.21658230e+00, -4.79330921e+00]]]],\r\n",
      "      dtype=float32)\r\n",
      "_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\r\n",
      "          -4.88638926e+00,  4.75044203e+00, -7.56786...434513e+00, -4.41384506e+00, ...,\r\n",
      "          -6.15245628e+00,  3.33514667e+00, -1.35630560e+00]]]],\r\n",
      "      dtype=float32)\r\n",
      "backward   = True\r\n",
      "device     = cpu()\r\n",
      "err1       = nan\r\n",
      "err2       = 2255.245\r\n",
      "err3       = 0.00390625\r\n",
      "out        = tensor([[[[   5.7346,  -55.9106,  320.3619,  ..., -472.6719,  -63.6798,\r\n",
      "            226.2005],\r\n",
      "          [ -39.7949, -...[ -34.6254, -262.0989, -192.8079,  ..., -480.3307,  172.4242,\r\n",
      "            163.7107]]]], grad_fn=<ConvolutionBackward0>)\r\n",
      "out2       = tensor(10302.4697, grad_fn=<SumBackward0>)\r\n",
      "padding    = 0\r\n",
      "stride     = 2\r\n",
      "torch      = <module 'torch' from '/home/ubuntu/anaconda3/envs/mlsys_hw/lib/python3.8/site-packages/torch/__init__.py'>\r\n",
      "y          = needle.Tensor([[[[   5.734564   -22.222597   264.71936   ...  -69.7374\r\n",
      "    -379.62265     99.81865  ]\r\n",
      "   [ -55.910625 ... 143.49785    172.42424  ]\r\n",
      "   [ 140.06458    -33.92796    128.48518   ...  228.07355\r\n",
      "     153.4408     163.71072  ]]]])\r\n",
      "y2         = needle.Tensor([10302.474])\r\n",
      "\r\n",
      "\u001b[1m\u001b[31mtests/test_conv.py\u001b[0m:446: AssertionError\r\n",
      "----------------------------- Captured stdout call -----------------------------\r\n",
      "out_grad shape: (3, 7, 7, 14)\r\n",
      "stride: 2\r\n",
      "_out_grad shape: (3, 14, 14, 14)\r\n",
      "W shape: (3, 3, 8, 14)\r\n",
      "_W shape: (3, 3, 14, 8)\r\n",
      "X_grad shape: (3, 16, 16, 8)\r\n",
      "X_shape: (3, 16, 16, 8)\r\n",
      "_X shape: (8, 16, 16, 3)\r\n",
      "_out_grad shape: (14, 14, 3, 14)\r\n",
      "W_grad shape: (8, 3, 3, 14)\r\n",
      "W_grad shape: (3, 3, 8, 14)\r\n",
      "\u001b[31m\u001b[1m_ test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape9-W_shape9-2-0] _\u001b[0m\r\n",
      "\r\n",
      "Z_shape = (3, 16, 16, 2), W_shape = (3, 3, 2, 14), stride = 2, padding = 0\r\n",
      "backward = True, device = cpu()\r\n",
      "\r\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\r\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\r\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\r\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\r\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\r\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\r\n",
      "        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\r\n",
      "        _Z = _Z.astype(np.float32)\r\n",
      "        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\r\n",
      "        _W = _W.astype(np.float32)\r\n",
      "        Z = ndl.Tensor(_Z, device=device)\r\n",
      "        W = ndl.Tensor(_W, device=device)\r\n",
      "        y = ndl.conv(Z, W, padding=padding, stride=stride)\r\n",
      "        y2 = y.sum()\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      "            y2.backward()\r\n",
      "        Ztch = torch.Tensor(_Z).float()\r\n",
      "        Ztch.requires_grad=\u001b[94mTrue\u001b[39;49;00m\r\n",
      "        Wtch = torch.Tensor(_W).float()\r\n",
      "        Wtch.requires_grad=\u001b[94mTrue\u001b[39;49;00m\r\n",
      "        out = torch.nn.functional.conv2d(Ztch.permute(\u001b[94m0\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m), Wtch.permute(\u001b[94m3\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m), padding=padding, stride=stride)\r\n",
      "        out2 = out.sum()\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      "            out2.backward()\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      "            err1 = np.linalg.norm(Ztch.grad.numpy() - Z.grad.numpy())\r\n",
      "            err2 = np.linalg.norm(Wtch.grad.numpy() - W.grad.numpy())\r\n",
      "        err3 = np.linalg.norm(out2.detach().numpy() - y2.numpy())\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      ">           \u001b[94massert\u001b[39;49;00m err1 < \u001b[94m1e-2\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33minput grads match\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "\u001b[1m\u001b[31mE           AssertionError: input grads match\u001b[0m\r\n",
      "\u001b[1m\u001b[31mE           assert nan < 0.01\u001b[0m\r\n",
      "\r\n",
      "W          = needle.Tensor([[[[ -8.0968      -2.5552022    8.703147    -1.4674252    4.5861077\r\n",
      "     -0.28521433   4.383634    -9.13...727333   0.22582927   9.296732    -8.13161     -0.67411226\r\n",
      "     -2.9204676    1.675528   -12.187821     5.5746226 ]]]])\r\n",
      "W_shape    = (3, 3, 2, 14)\r\n",
      "Wtch       = tensor([[[[ -8.0968,  -2.5552,   8.7031,  -1.4674,   4.5861,  -0.2852,   4.3836,\r\n",
      "            -9.1346,  -2.0159,   4.74...0.2258,\r\n",
      "             9.2967,  -8.1316,  -0.6741,  -2.9205,   1.6755, -12.1878,   5.5746]]]],\r\n",
      "       requires_grad=True)\r\n",
      "Z          = needle.Tensor([[[[  8.820262     2.000786  ]\r\n",
      "   [  4.89369     11.204466  ]\r\n",
      "   [  9.33779     -4.8863893 ]\r\n",
      "   ...\r\n",
      "   [...   -5.112822  ]\r\n",
      "   ...\r\n",
      "   [ -7.922368     4.2222714 ]\r\n",
      "   [ -6.064339     1.4188478 ]\r\n",
      "   [ -1.4109794   -5.791016  ]]]])\r\n",
      "Z_shape    = (3, 16, 16, 2)\r\n",
      "Ztch       = tensor([[[[  8.8203,   2.0008],\r\n",
      "          [  4.8937,  11.2045],\r\n",
      "          [  9.3378,  -4.8864],\r\n",
      "          ...,\r\n",
      "       ...\r\n",
      "          [ -7.9224,   4.2223],\r\n",
      "          [ -6.0643,   1.4188],\r\n",
      "          [ -1.4110,  -5.7910]]]], requires_grad=True)\r\n",
      "_W         = array([[[[ -8.0968    ,  -2.5552022 ,   8.703147  ,  -1.4674252 ,\r\n",
      "            4.5861077 ,  -0.28521433,   4.383634  , ...        -8.13161   ,  -0.67411226,  -2.9204676 ,   1.675528  ,\r\n",
      "          -12.187821  ,   5.5746226 ]]]], dtype=float32)\r\n",
      "_Z         = array([[[[  8.820262  ,   2.000786  ],\r\n",
      "         [  4.89369   ,  11.204466  ],\r\n",
      "         [  9.33779   ,  -4.8863893 ],\r\n",
      " ...22368  ,   4.2222714 ],\r\n",
      "         [ -6.064339  ,   1.4188478 ],\r\n",
      "         [ -1.4109794 ,  -5.791016  ]]]], dtype=float32)\r\n",
      "backward   = True\r\n",
      "device     = cpu()\r\n",
      "err1       = nan\r\n",
      "err2       = 1072.2002\r\n",
      "err3       = 0.005126953\r\n",
      "out        = tensor([[[[-3.4607e+02,  6.3506e+01, -3.3188e+01,  ...,  1.9525e+02,\r\n",
      "            1.2715e+02, -5.7875e+01],\r\n",
      "          [..., -1.1399e+02, -6.9067e+01,  ..., -1.4515e+02,\r\n",
      "            5.3236e+01,  3.2069e+00]]]], grad_fn=<ConvolutionBackward0>)\r\n",
      "out2       = tensor(-2297.1741, grad_fn=<SumBackward0>)\r\n",
      "padding    = 0\r\n",
      "stride     = 2\r\n",
      "torch      = <module 'torch' from '/home/ubuntu/anaconda3/envs/mlsys_hw/lib/python3.8/site-packages/torch/__init__.py'>\r\n",
      "y          = needle.Tensor([[[[-3.46071625e+02 -1.10818672e+00  4.20151154e+02 ... -5.85831528e+01\r\n",
      "    -6.61993790e+01  3.31586266e...3e+01]\r\n",
      "   [-9.30875702e+01 -4.32016563e+01  1.87175808e+01 ... -3.38230171e+01\r\n",
      "    -5.38563805e+01  3.20688462e+00]]]])\r\n",
      "y2         = needle.Tensor([-2297.1792])\r\n",
      "\r\n",
      "\u001b[1m\u001b[31mtests/test_conv.py\u001b[0m:446: AssertionError\r\n",
      "----------------------------- Captured stdout call -----------------------------\r\n",
      "out_grad shape: (3, 7, 7, 14)\r\n",
      "stride: 2\r\n",
      "_out_grad shape: (3, 14, 14, 14)\r\n",
      "W shape: (3, 3, 2, 14)\r\n",
      "_W shape: (3, 3, 14, 2)\r\n",
      "X_grad shape: (3, 16, 16, 2)\r\n",
      "X_shape: (3, 16, 16, 2)\r\n",
      "_X shape: (2, 16, 16, 3)\r\n",
      "_out_grad shape: (14, 14, 3, 14)\r\n",
      "W_grad shape: (2, 3, 3, 14)\r\n",
      "W_grad shape: (3, 3, 2, 14)\r\n",
      "\u001b[31m\u001b[1m_ test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape10-W_shape10-1-0] _\u001b[0m\r\n",
      "\r\n",
      "Z_shape = (3, 16, 16, 24), W_shape = (3, 3, 24, 14), stride = 1, padding = 0\r\n",
      "backward = True, device = cpu()\r\n",
      "\r\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\r\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\r\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\r\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\r\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\r\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\r\n",
      "        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\r\n",
      "        _Z = _Z.astype(np.float32)\r\n",
      "        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\r\n",
      "        _W = _W.astype(np.float32)\r\n",
      "        Z = ndl.Tensor(_Z, device=device)\r\n",
      "        W = ndl.Tensor(_W, device=device)\r\n",
      "        y = ndl.conv(Z, W, padding=padding, stride=stride)\r\n",
      "        y2 = y.sum()\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      "            y2.backward()\r\n",
      "        Ztch = torch.Tensor(_Z).float()\r\n",
      "        Ztch.requires_grad=\u001b[94mTrue\u001b[39;49;00m\r\n",
      "        Wtch = torch.Tensor(_W).float()\r\n",
      "        Wtch.requires_grad=\u001b[94mTrue\u001b[39;49;00m\r\n",
      "        out = torch.nn.functional.conv2d(Ztch.permute(\u001b[94m0\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m), Wtch.permute(\u001b[94m3\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m), padding=padding, stride=stride)\r\n",
      "        out2 = out.sum()\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      "            out2.backward()\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      "            err1 = np.linalg.norm(Ztch.grad.numpy() - Z.grad.numpy())\r\n",
      "            err2 = np.linalg.norm(Wtch.grad.numpy() - W.grad.numpy())\r\n",
      "        err3 = np.linalg.norm(out2.detach().numpy() - y2.numpy())\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      ">           \u001b[94massert\u001b[39;49;00m err1 < \u001b[94m1e-2\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33minput grads match\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "\u001b[1m\u001b[31mE           AssertionError: input grads match\u001b[0m\r\n",
      "\u001b[1m\u001b[31mE           assert nan < 0.01\u001b[0m\r\n",
      "\r\n",
      "W          = needle.Tensor([[[[ -1.3245817    1.0111231   -1.9236585  ...  -2.5907016\r\n",
      "     -0.36175704   4.6234684 ]\r\n",
      "   [  5.279258... -4.82095      4.98517   ]\r\n",
      "   [  3.2300415   -2.4161792   -3.420825   ...  -9.444658\r\n",
      "      8.537833     2.3818388 ]]]])\r\n",
      "W_shape    = (3, 3, 24, 14)\r\n",
      "Wtch       = tensor([[[[ -1.3246,   1.0111,  -1.9237,  ...,  -2.5907,  -0.3618,   4.6235],\r\n",
      "          [  5.2793,  -1.3090,   0.0500,...   4.9852],\r\n",
      "          [  3.2300,  -2.4162,  -3.4208,  ...,  -9.4447,   8.5378,   2.3818]]]],\r\n",
      "       requires_grad=True)\r\n",
      "Z          = needle.Tensor([[[[  8.820262     2.000786     4.89369    ...   3.2680929\r\n",
      "      4.322181    -3.7108252 ]\r\n",
      "   [ 11.348773...4.207389     8.935435  ]\r\n",
      "   [  2.5084004    3.1399443   -0.12958507 ...   0.13740699\r\n",
      "     -1.400853     0.07475674]]]])\r\n",
      "Z_shape    = (3, 16, 16, 24)\r\n",
      "Ztch       = tensor([[[[  8.8203,   2.0008,   4.8937,  ...,   3.2681,   4.3222,  -3.7108],\r\n",
      "          [ 11.3488,  -7.2718,   0.2288,...   8.9354],\r\n",
      "          [  2.5084,   3.1399,  -0.1296,  ...,   0.1374,  -1.4009,   0.0748]]]],\r\n",
      "       requires_grad=True)\r\n",
      "_W         = array([[[[ -1.3245817 ,   1.0111231 ,  -1.9236585 , ...,  -2.5907016 ,\r\n",
      "           -0.36175704,   4.6234684 ],\r\n",
      "        ... [  3.2300415 ,  -2.4161792 ,  -3.420825  , ...,  -9.444658  ,\r\n",
      "            8.537833  ,   2.3818388 ]]]], dtype=float32)\r\n",
      "_Z         = array([[[[  8.820262  ,   2.000786  ,   4.89369   , ...,   3.2680929 ,\r\n",
      "            4.322181  ,  -3.7108252 ],\r\n",
      "        ... [  2.5084004 ,   3.1399443 ,  -0.12958507, ...,   0.13740699,\r\n",
      "           -1.400853  ,   0.07475674]]]], dtype=float32)\r\n",
      "backward   = True\r\n",
      "device     = cpu()\r\n",
      "err1       = nan\r\n",
      "err2       = 0.002582637\r\n",
      "err3       = 0.032226562\r\n",
      "out        = tensor([[[[ 5.9644e+02,  1.7622e+01, -3.8665e+02,  ..., -5.6437e+02,\r\n",
      "           -1.4383e+02, -1.2310e+02],\r\n",
      "          [..., -6.6977e+02,  7.6359e+00,  ...,  3.7344e+02,\r\n",
      "           -8.9822e+01,  1.5355e+02]]]], grad_fn=<ConvolutionBackward0>)\r\n",
      "out2       = tensor(-5941.3750, grad_fn=<SumBackward0>)\r\n",
      "padding    = 0\r\n",
      "stride     = 1\r\n",
      "torch      = <module 'torch' from '/home/ubuntu/anaconda3/envs/mlsys_hw/lib/python3.8/site-packages/torch/__init__.py'>\r\n",
      "y          = needle.Tensor([[[[ 5.96437805e+02 -2.17912796e+02 -2.09804783e+01 ... -4.44460907e+02\r\n",
      "    -2.74169159e+02 -6.12073181e...4e+01]\r\n",
      "   [-5.03057983e+02 -1.26949539e+02 -1.28574600e+01 ...  4.08393738e+02\r\n",
      "    -1.71167480e+02  1.53551315e+02]]]])\r\n",
      "y2         = needle.Tensor([-5941.407])\r\n",
      "\r\n",
      "\u001b[1m\u001b[31mtests/test_conv.py\u001b[0m:446: AssertionError\r\n",
      "----------------------------- Captured stdout call -----------------------------\r\n",
      "out_grad shape: (3, 14, 14, 14)\r\n",
      "stride: 1\r\n",
      "_out_grad shape: (3, 14, 14, 14)\r\n",
      "W shape: (3, 3, 24, 14)\r\n",
      "_W shape: (3, 3, 14, 24)\r\n",
      "X_grad shape: (3, 16, 16, 24)\r\n",
      "X_shape: (3, 16, 16, 24)\r\n",
      "_X shape: (24, 16, 16, 3)\r\n",
      "_out_grad shape: (14, 14, 3, 14)\r\n",
      "W_grad shape: (24, 3, 3, 14)\r\n",
      "W_grad shape: (3, 3, 24, 14)\r\n",
      "\u001b[31m\u001b[1m_ test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape11-W_shape11-1-0] _\u001b[0m\r\n",
      "\r\n",
      "Z_shape = (3, 14, 14, 8), W_shape = (5, 5, 8, 16), stride = 1, padding = 0\r\n",
      "backward = True, device = cpu()\r\n",
      "\r\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\r\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\r\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\r\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\r\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\r\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\r\n",
      "        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\r\n",
      "        _Z = _Z.astype(np.float32)\r\n",
      "        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\r\n",
      "        _W = _W.astype(np.float32)\r\n",
      "        Z = ndl.Tensor(_Z, device=device)\r\n",
      "        W = ndl.Tensor(_W, device=device)\r\n",
      "        y = ndl.conv(Z, W, padding=padding, stride=stride)\r\n",
      "        y2 = y.sum()\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      "            y2.backward()\r\n",
      "        Ztch = torch.Tensor(_Z).float()\r\n",
      "        Ztch.requires_grad=\u001b[94mTrue\u001b[39;49;00m\r\n",
      "        Wtch = torch.Tensor(_W).float()\r\n",
      "        Wtch.requires_grad=\u001b[94mTrue\u001b[39;49;00m\r\n",
      "        out = torch.nn.functional.conv2d(Ztch.permute(\u001b[94m0\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m), Wtch.permute(\u001b[94m3\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m), padding=padding, stride=stride)\r\n",
      "        out2 = out.sum()\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      "            out2.backward()\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      "            err1 = np.linalg.norm(Ztch.grad.numpy() - Z.grad.numpy())\r\n",
      "            err2 = np.linalg.norm(Wtch.grad.numpy() - W.grad.numpy())\r\n",
      "        err3 = np.linalg.norm(out2.detach().numpy() - y2.numpy())\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      ">           \u001b[94massert\u001b[39;49;00m err1 < \u001b[94m1e-2\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33minput grads match\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "\u001b[1m\u001b[31mE           AssertionError: input grads match\u001b[0m\r\n",
      "\u001b[1m\u001b[31mE           assert nan < 0.01\u001b[0m\r\n",
      "\r\n",
      "W          = needle.Tensor([[[[ 4.28266907e+00  8.13360405e+00 -4.62234831e+00 ...  2.07981968e+00\r\n",
      "     2.17041516e+00 -3.88039827e...7e+00]\r\n",
      "   [-7.86027253e-01  4.30947495e+00 -4.97804356e+00 ... -5.69430470e-01\r\n",
      "     4.27775383e-01 -3.84437203e+00]]]])\r\n",
      "W_shape    = (5, 5, 8, 16)\r\n",
      "Wtch       = tensor([[[[ 4.2827e+00,  8.1336e+00, -4.6223e+00,  ...,  2.0798e+00,\r\n",
      "            2.1704e+00, -3.8804e-01],\r\n",
      "          [...[-7.8603e-01,  4.3095e+00, -4.9780e+00,  ..., -5.6943e-01,\r\n",
      "            4.2778e-01, -3.8444e+00]]]], requires_grad=True)\r\n",
      "Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\r\n",
      "     4.75044203e+00 -7.56786048e...3e+00]\r\n",
      "   [ 1.09905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\r\n",
      "    -8.48226726e-01 -2.29984760e+00]]]])\r\n",
      "Z_shape    = (3, 14, 14, 8)\r\n",
      "Ztch       = tensor([[[[ 8.8203e+00,  2.0008e+00,  4.8937e+00,  ..., -4.8864e+00,\r\n",
      "            4.7504e+00, -7.5679e-01],\r\n",
      "          [...[ 1.0991e+01,  2.2317e+00,  4.6206e+00,  ..., -6.2309e-01,\r\n",
      "           -8.4823e-01, -2.2998e+00]]]], requires_grad=True)\r\n",
      "_W         = array([[[[ 4.28266907e+00,  8.13360405e+00, -4.62234831e+00, ...,\r\n",
      "           2.07981968e+00,  2.17041516e+00, -3.88039...947495e+00, -4.97804356e+00, ...,\r\n",
      "          -5.69430470e-01,  4.27775383e-01, -3.84437203e+00]]]],\r\n",
      "      dtype=float32)\r\n",
      "_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\r\n",
      "          -4.88638926e+00,  4.75044203e+00, -7.56786...166728e+00,  4.62055349e+00, ...,\r\n",
      "          -6.23090088e-01, -8.48226726e-01, -2.29984760e+00]]]],\r\n",
      "      dtype=float32)\r\n",
      "backward   = True\r\n",
      "device     = cpu()\r\n",
      "err1       = nan\r\n",
      "err2       = 0.0019506718\r\n",
      "err3       = 0.0068359375\r\n",
      "out        = tensor([[[[ 1.7557e+02,  1.3950e+02,  4.5144e+02,  ...,  1.9506e+01,\r\n",
      "           -5.7378e+01, -2.2967e+02],\r\n",
      "          [...,  3.1106e+02,  4.1425e+02,  ...,  3.3462e+02,\r\n",
      "           -1.0345e+02,  2.4403e+02]]]], grad_fn=<ConvolutionBackward0>)\r\n",
      "out2       = tensor(-10705.7383, grad_fn=<SumBackward0>)\r\n",
      "padding    = 0\r\n",
      "stride     = 1\r\n",
      "torch      = <module 'torch' from '/home/ubuntu/anaconda3/envs/mlsys_hw/lib/python3.8/site-packages/torch/__init__.py'>\r\n",
      "y          = needle.Tensor([[[[ 1.75570267e+02  2.39688141e+02  5.25702209e+02 ...  4.85040955e+02\r\n",
      "    -2.37886475e+02 -2.36008549e...4e+02]\r\n",
      "   [-4.85305603e+02 -1.46666489e+02 -3.62995392e+02 ...  1.50065781e+02\r\n",
      "    -5.45707825e+02  2.44026398e+02]]]])\r\n",
      "y2         = needle.Tensor([-10705.731])\r\n",
      "\r\n",
      "\u001b[1m\u001b[31mtests/test_conv.py\u001b[0m:446: AssertionError\r\n",
      "----------------------------- Captured stdout call -----------------------------\r\n",
      "out_grad shape: (3, 10, 10, 16)\r\n",
      "stride: 1\r\n",
      "_out_grad shape: (3, 10, 10, 16)\r\n",
      "W shape: (5, 5, 8, 16)\r\n",
      "_W shape: (5, 5, 16, 8)\r\n",
      "X_grad shape: (3, 14, 14, 8)\r\n",
      "X_shape: (3, 14, 14, 8)\r\n",
      "_X shape: (8, 14, 14, 3)\r\n",
      "_out_grad shape: (10, 10, 3, 16)\r\n",
      "W_grad shape: (8, 5, 5, 16)\r\n",
      "W_grad shape: (5, 5, 8, 16)\r\n",
      "\u001b[31m\u001b[1m_ test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape12-W_shape12-1-0] _\u001b[0m\r\n",
      "\r\n",
      "Z_shape = (3, 17, 17, 8), W_shape = (5, 5, 8, 16), stride = 1, padding = 0\r\n",
      "backward = True, device = cpu()\r\n",
      "\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\r\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\r\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\r\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\r\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\r\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\r\n",
      "        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\r\n",
      "        _Z = _Z.astype(np.float32)\r\n",
      "        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\r\n",
      "        _W = _W.astype(np.float32)\r\n",
      "        Z = ndl.Tensor(_Z, device=device)\r\n",
      "        W = ndl.Tensor(_W, device=device)\r\n",
      "        y = ndl.conv(Z, W, padding=padding, stride=stride)\r\n",
      "        y2 = y.sum()\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      "            y2.backward()\r\n",
      "        Ztch = torch.Tensor(_Z).float()\r\n",
      "        Ztch.requires_grad=\u001b[94mTrue\u001b[39;49;00m\r\n",
      "        Wtch = torch.Tensor(_W).float()\r\n",
      "        Wtch.requires_grad=\u001b[94mTrue\u001b[39;49;00m\r\n",
      "        out = torch.nn.functional.conv2d(Ztch.permute(\u001b[94m0\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m), Wtch.permute(\u001b[94m3\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m), padding=padding, stride=stride)\r\n",
      "        out2 = out.sum()\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      "            out2.backward()\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      "            err1 = np.linalg.norm(Ztch.grad.numpy() - Z.grad.numpy())\r\n",
      "            err2 = np.linalg.norm(Wtch.grad.numpy() - W.grad.numpy())\r\n",
      "        err3 = np.linalg.norm(out2.detach().numpy() - y2.numpy())\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      ">           \u001b[94massert\u001b[39;49;00m err1 < \u001b[94m1e-2\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33minput grads match\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "\u001b[1m\u001b[31mE           AssertionError: input grads match\u001b[0m\r\n",
      "\u001b[1m\u001b[31mE           assert nan < 0.01\u001b[0m\r\n",
      "\r\n",
      "W          = needle.Tensor([[[[ 5.93117094e+00 -4.61600447e+00 -7.43411541e+00 ...  9.49053860e+00\r\n",
      "     6.47611380e+00 -3.38525438e...7e+00]\r\n",
      "   [-3.93575430e+00 -6.98609781e+00  1.63659811e+00 ...  8.22474575e+00\r\n",
      "     2.56555057e+00  1.63004756e+00]]]])\r\n",
      "W_shape    = (5, 5, 8, 16)\r\n",
      "Wtch       = tensor([[[[ 5.9312e+00, -4.6160e+00, -7.4341e+00,  ...,  9.4905e+00,\r\n",
      "            6.4761e+00, -3.3853e+00],\r\n",
      "          [...[-3.9358e+00, -6.9861e+00,  1.6366e+00,  ...,  8.2247e+00,\r\n",
      "            2.5656e+00,  1.6300e+00]]]], requires_grad=True)\r\n",
      "Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\r\n",
      "     4.75044203e+00 -7.56786048e...8e+00]\r\n",
      "   [ 2.80155873e+00  3.11298299e+00 -3.24756050e+00 ... -7.96233535e-01\r\n",
      "     4.72636795e+00 -5.75466204e+00]]]])\r\n",
      "Z_shape    = (3, 17, 17, 8)\r\n",
      "Ztch       = tensor([[[[ 8.8203e+00,  2.0008e+00,  4.8937e+00,  ..., -4.8864e+00,\r\n",
      "            4.7504e+00, -7.5679e-01],\r\n",
      "          [...[ 2.8016e+00,  3.1130e+00, -3.2476e+00,  ..., -7.9623e-01,\r\n",
      "            4.7264e+00, -5.7547e+00]]]], requires_grad=True)\r\n",
      "_W         = array([[[[ 5.93117094e+00, -4.61600447e+00, -7.43411541e+00, ...,\r\n",
      "           9.49053860e+00,  6.47611380e+00, -3.38525...609781e+00,  1.63659811e+00, ...,\r\n",
      "           8.22474575e+00,  2.56555057e+00,  1.63004756e+00]]]],\r\n",
      "      dtype=float32)\r\n",
      "_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\r\n",
      "          -4.88638926e+00,  4.75044203e+00, -7.56786...298299e+00, -3.24756050e+00, ...,\r\n",
      "          -7.96233535e-01,  4.72636795e+00, -5.75466204e+00]]]],\r\n",
      "      dtype=float32)\r\n",
      "backward   = True\r\n",
      "device     = cpu()\r\n",
      "err1       = nan\r\n",
      "err2       = 0.0029745598\r\n",
      "err3       = 0.0703125\r\n",
      "out        = tensor([[[[ 5.8605e+02,  6.5851e+02,  3.1471e+01,  ..., -2.0572e+02,\r\n",
      "           -5.6982e+02, -7.7784e+02],\r\n",
      "          [...,  1.3106e+02, -3.2224e+02,  ...,  7.1755e+01,\r\n",
      "           -5.2006e+02, -6.5310e+01]]]], grad_fn=<ConvolutionBackward0>)\r\n",
      "out2       = tensor(50567.7344, grad_fn=<SumBackward0>)\r\n",
      "padding    = 0\r\n",
      "stride     = 1\r\n",
      "torch      = <module 'torch' from '/home/ubuntu/anaconda3/envs/mlsys_hw/lib/python3.8/site-packages/torch/__init__.py'>\r\n",
      "y          = needle.Tensor([[[[ 5.86050903e+02 -5.11887695e+02  5.65467911e+01 ...  7.48446426e+01\r\n",
      "    -3.56891510e+02 -3.76215637e...2e+02]\r\n",
      "   [-6.95190125e+02 -8.10500183e+01  2.38304077e+02 ... -1.27415886e+02\r\n",
      "     8.60565063e+02 -6.53100510e+01]]]])\r\n",
      "y2         = needle.Tensor([50567.664])\r\n",
      "\r\n",
      "\u001b[1m\u001b[31mtests/test_conv.py\u001b[0m:446: AssertionError\r\n",
      "----------------------------- Captured stdout call -----------------------------\r\n",
      "out_grad shape: (3, 13, 13, 16)\r\n",
      "stride: 1\r\n",
      "_out_grad shape: (3, 13, 13, 16)\r\n",
      "W shape: (5, 5, 8, 16)\r\n",
      "_W shape: (5, 5, 16, 8)\r\n",
      "X_grad shape: (3, 17, 17, 8)\r\n",
      "X_shape: (3, 17, 17, 8)\r\n",
      "_X shape: (8, 17, 17, 3)\r\n",
      "_out_grad shape: (13, 13, 3, 16)\r\n",
      "W_grad shape: (8, 5, 5, 16)\r\n",
      "W_grad shape: (5, 5, 8, 16)\r\n",
      "\u001b[31m\u001b[1m_ test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape13-W_shape13-1-0] _\u001b[0m\r\n",
      "\r\n",
      "Z_shape = (3, 17, 17, 1), W_shape = (5, 5, 1, 16), stride = 1, padding = 0\r\n",
      "backward = True, device = cpu()\r\n",
      "\r\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\r\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\r\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\r\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\r\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\r\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\r\n",
      "        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\r\n",
      "        _Z = _Z.astype(np.float32)\r\n",
      "        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\r\n",
      "        _W = _W.astype(np.float32)\r\n",
      "        Z = ndl.Tensor(_Z, device=device)\r\n",
      "        W = ndl.Tensor(_W, device=device)\r\n",
      "        y = ndl.conv(Z, W, padding=padding, stride=stride)\r\n",
      "        y2 = y.sum()\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      "            y2.backward()\r\n",
      "        Ztch = torch.Tensor(_Z).float()\r\n",
      "        Ztch.requires_grad=\u001b[94mTrue\u001b[39;49;00m\r\n",
      "        Wtch = torch.Tensor(_W).float()\r\n",
      "        Wtch.requires_grad=\u001b[94mTrue\u001b[39;49;00m\r\n",
      "        out = torch.nn.functional.conv2d(Ztch.permute(\u001b[94m0\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m), Wtch.permute(\u001b[94m3\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m), padding=padding, stride=stride)\r\n",
      "        out2 = out.sum()\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      "            out2.backward()\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      "            err1 = np.linalg.norm(Ztch.grad.numpy() - Z.grad.numpy())\r\n",
      "            err2 = np.linalg.norm(Wtch.grad.numpy() - W.grad.numpy())\r\n",
      "        err3 = np.linalg.norm(out2.detach().numpy() - y2.numpy())\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      ">           \u001b[94massert\u001b[39;49;00m err1 < \u001b[94m1e-2\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33minput grads match\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "\u001b[1m\u001b[31mE           AssertionError: input grads match\u001b[0m\r\n",
      "\u001b[1m\u001b[31mE           assert inf < 0.01\u001b[0m\r\n",
      "\r\n",
      "W          = needle.Tensor([[[[  1.5437562   -6.8538       4.3282647    5.4068804   -3.15688\r\n",
      "     -1.206689    -4.3909516    3.4969...448   -2.242325     0.6578698\r\n",
      "     -7.0278      -1.7489109   10.11736      2.5269346    1.7962458\r\n",
      "     -7.9124722 ]]]])\r\n",
      "W_shape    = (5, 5, 1, 16)\r\n",
      "Wtch       = tensor([[[[  1.5438,  -6.8538,   4.3283,   5.4069,  -3.1569,  -1.2067,  -4.3910,\r\n",
      "             3.4969,  -5.3061,  -1.11...461,  -2.2423,   0.6579,  -7.0278,  -1.7489,  10.1174,   2.5269,\r\n",
      "             1.7962,  -7.9125]]]], requires_grad=True)\r\n",
      "Z          = needle.Tensor([[[[ 8.82026196e+00]\r\n",
      "   [ 2.00078607e+00]\r\n",
      "   [ 4.89369011e+00]\r\n",
      "   [ 1.12044659e+01]\r\n",
      "   [ 9.33778954e+00]...22066e-01]\r\n",
      "   [ 1.05310105e-01]\r\n",
      "   [ 4.97272283e-01]\r\n",
      "   [ 1.13696384e+00]\r\n",
      "   [-5.08369303e+00]\r\n",
      "   [-5.73876619e-01]]]])\r\n",
      "Z_shape    = (3, 17, 17, 1)\r\n",
      "Ztch       = tensor([[[[ 8.8203e+00],\r\n",
      "          [ 2.0008e+00],\r\n",
      "          [ 4.8937e+00],\r\n",
      "          [ 1.1204e+01],\r\n",
      "          [ 9.3378...       [ 4.9727e-01],\r\n",
      "          [ 1.1370e+00],\r\n",
      "          [-5.0837e+00],\r\n",
      "          [-5.7388e-01]]]], requires_grad=True)\r\n",
      "_W         = array([[[[  1.5437562 ,  -6.8538    ,   4.3282647 ,   5.4068804 ,\r\n",
      "           -3.15688   ,  -1.206689  ,  -4.3909516 , ...  -7.0278    ,  -1.7489109 ,\r\n",
      "           10.11736   ,   2.5269346 ,   1.7962458 ,  -7.9124722 ]]]],\r\n",
      "      dtype=float32)\r\n",
      "_Z         = array([[[[ 8.82026196e+00],\r\n",
      "         [ 2.00078607e+00],\r\n",
      "         [ 4.89369011e+00],\r\n",
      "         [ 1.12044659e+01],\r\n",
      "      ... 4.97272283e-01],\r\n",
      "         [ 1.13696384e+00],\r\n",
      "         [-5.08369303e+00],\r\n",
      "         [-5.73876619e-01]]]], dtype=float32)\r\n",
      "backward   = True\r\n",
      "device     = cpu()\r\n",
      "err1       = inf\r\n",
      "err2       = 0.0012842789\r\n",
      "err3       = 0.029296875\r\n",
      "out        = tensor([[[[ 4.1444e+01,  7.2539e+01, -1.2076e+02,  ...,  5.7938e+01,\r\n",
      "           -1.5660e+01, -6.3388e+00],\r\n",
      "          [...,  1.4962e+02,  3.9613e+01,  ...,  6.5264e+01,\r\n",
      "            1.5128e+01, -1.2588e+02]]]], grad_fn=<ConvolutionBackward0>)\r\n",
      "out2       = tensor(-27459.7266, grad_fn=<SumBackward0>)\r\n",
      "padding    = 0\r\n",
      "stride     = 1\r\n",
      "torch      = <module 'torch' from '/home/ubuntu/anaconda3/envs/mlsys_hw/lib/python3.8/site-packages/torch/__init__.py'>\r\n",
      "y          = needle.Tensor([[[[ 4.14443283e+01  1.16950035e+01 -4.72139893e+01 ...  1.41469421e+02\r\n",
      "    -2.21567459e+01 -3.79012985e...8e+01]\r\n",
      "   [ 6.41885986e+01  7.46171875e+01  3.42209511e+01 ...  1.39405457e+02\r\n",
      "    -1.01194525e+01 -1.25883507e+02]]]])\r\n",
      "y2         = needle.Tensor([-27459.697])\r\n",
      "\r\n",
      "\u001b[1m\u001b[31mtests/test_conv.py\u001b[0m:446: AssertionError\r\n",
      "----------------------------- Captured stdout call -----------------------------\r\n",
      "out_grad shape: (3, 13, 13, 16)\r\n",
      "stride: 1\r\n",
      "_out_grad shape: (3, 13, 13, 16)\r\n",
      "W shape: (5, 5, 1, 16)\r\n",
      "_W shape: (5, 5, 16, 1)\r\n",
      "X_grad shape: (3, 17, 17, 1)\r\n",
      "X_shape: (3, 17, 17, 1)\r\n",
      "_X shape: (1, 17, 17, 3)\r\n",
      "_out_grad shape: (13, 13, 3, 16)\r\n",
      "W_grad shape: (1, 5, 5, 16)\r\n",
      "W_grad shape: (5, 5, 1, 16)\r\n",
      "\u001b[31m\u001b[1m_ test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape16-W_shape16-1-0] _\u001b[0m\r\n",
      "\r\n",
      "Z_shape = (1, 14, 14, 2), W_shape = (3, 3, 2, 2), stride = 1, padding = 0\r\n",
      "backward = True, device = cpu()\r\n",
      "\r\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\r\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\r\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\r\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\r\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\r\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\r\n",
      "        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\r\n",
      "        _Z = _Z.astype(np.float32)\r\n",
      "        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\r\n",
      "        _W = _W.astype(np.float32)\r\n",
      "        Z = ndl.Tensor(_Z, device=device)\r\n",
      "        W = ndl.Tensor(_W, device=device)\r\n",
      "        y = ndl.conv(Z, W, padding=padding, stride=stride)\r\n",
      "        y2 = y.sum()\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      "            y2.backward()\r\n",
      "        Ztch = torch.Tensor(_Z).float()\r\n",
      "        Ztch.requires_grad=\u001b[94mTrue\u001b[39;49;00m\r\n",
      "        Wtch = torch.Tensor(_W).float()\r\n",
      "        Wtch.requires_grad=\u001b[94mTrue\u001b[39;49;00m\r\n",
      "        out = torch.nn.functional.conv2d(Ztch.permute(\u001b[94m0\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m), Wtch.permute(\u001b[94m3\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m), padding=padding, stride=stride)\r\n",
      "        out2 = out.sum()\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      "            out2.backward()\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      "            err1 = np.linalg.norm(Ztch.grad.numpy() - Z.grad.numpy())\r\n",
      "            err2 = np.linalg.norm(Wtch.grad.numpy() - W.grad.numpy())\r\n",
      "        err3 = np.linalg.norm(out2.detach().numpy() - y2.numpy())\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      ">           \u001b[94massert\u001b[39;49;00m err1 < \u001b[94m1e-2\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33minput grads match\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "\u001b[1m\u001b[31mE           AssertionError: input grads match\u001b[0m\r\n",
      "\u001b[1m\u001b[31mE           assert 30620902000000.0 < 0.01\u001b[0m\r\n",
      "\r\n",
      "W          = needle.Tensor([[[[ -1.7671587   -8.082371  ]\r\n",
      "   [ -1.4591868   -3.807461  ]]\r\n",
      "\r\n",
      "  [[  4.2896194    5.705509  ]\r\n",
      "   [  7.3...7526     2.911123  ]\r\n",
      "   [-10.473016     0.61860955]]\r\n",
      "\r\n",
      "  [[ -0.65053475   0.46976614]\r\n",
      "   [  4.7152305  -13.698386  ]]]])\r\n",
      "W_shape    = (3, 3, 2, 2)\r\n",
      "Wtch       = tensor([[[[ -1.7672,  -8.0824],\r\n",
      "          [ -1.4592,  -3.8075]],\r\n",
      "\r\n",
      "         [[  4.2896,   5.7055],\r\n",
      "          [  7.3329,...         [-10.4730,   0.6186]],\r\n",
      "\r\n",
      "         [[ -0.6505,   0.4698],\r\n",
      "          [  4.7152, -13.6984]]]], requires_grad=True)\r\n",
      "Z          = needle.Tensor([[[[  8.820262     2.000786  ]\r\n",
      "   [  4.89369     11.204466  ]\r\n",
      "   [  9.33779     -4.8863893 ]\r\n",
      "   [  4.750...19315276  -8.283575  ]\r\n",
      "   [ -4.9275537   -7.359175  ]\r\n",
      "   [  8.240675     0.8211388 ]\r\n",
      "   [  2.8364513   -1.1133755 ]]]])\r\n",
      "Z_shape    = (1, 14, 14, 2)\r\n",
      "Ztch       = tensor([[[[  8.8203,   2.0008],\r\n",
      "          [  4.8937,  11.2045],\r\n",
      "          [  9.3378,  -4.8864],\r\n",
      "          [  4.7504,  ...\r\n",
      "          [ -4.9276,  -7.3592],\r\n",
      "          [  8.2407,   0.8211],\r\n",
      "          [  2.8365,  -1.1134]]]], requires_grad=True)\r\n",
      "_W         = array([[[[ -1.7671587 ,  -8.082371  ],\r\n",
      "         [ -1.4591868 ,  -3.807461  ]],\r\n",
      "\r\n",
      "        [[  4.2896194 ,   5.705509  ],...016  ,   0.61860955]],\r\n",
      "\r\n",
      "        [[ -0.65053475,   0.46976614],\r\n",
      "         [  4.7152305 , -13.698386  ]]]], dtype=float32)\r\n",
      "_Z         = array([[[[  8.820262  ,   2.000786  ],\r\n",
      "         [  4.89369   ,  11.204466  ],\r\n",
      "         [  9.33779   ,  -4.8863893 ],\r\n",
      " ...275537 ,  -7.359175  ],\r\n",
      "         [  8.240675  ,   0.8211388 ],\r\n",
      "         [  2.8364513 ,  -1.1133755 ]]]], dtype=float32)\r\n",
      "backward   = True\r\n",
      "device     = cpu()\r\n",
      "err1       = 30620902000000.0\r\n",
      "err2       = 0.0\r\n",
      "err3       = 0.0017089844\r\n",
      "out        = tensor([[[[ -83.1438,   -6.3602,   93.8760,  -41.2035,  -96.8909,  152.8543,\r\n",
      "           -101.4654,  259.1811,    7.015...            -60.5771,   13.6480,  228.9975,    8.4413, -121.9792,   26.1601]]]],\r\n",
      "       grad_fn=<ConvolutionBackward0>)\r\n",
      "out2       = tensor(2243.9304, grad_fn=<SumBackward0>)\r\n",
      "padding    = 0\r\n",
      "stride     = 1\r\n",
      "torch      = <module 'torch' from '/home/ubuntu/anaconda3/envs/mlsys_hw/lib/python3.8/site-packages/torch/__init__.py'>\r\n",
      "y          = needle.Tensor([[[[ -83.14382     -46.58788   ]\r\n",
      "   [  -6.3601565    -5.1046524 ]\r\n",
      "   [  93.87601     -20.797268  ]\r\n",
      "   [ ...   228.99751   ]\r\n",
      "   [  41.437016      8.441346  ]\r\n",
      "   [ -26.093277   -121.97922   ]\r\n",
      "   [  22.205511     26.16009   ]]]])\r\n",
      "y2         = needle.Tensor([2243.9321])\r\n",
      "\r\n",
      "\u001b[1m\u001b[31mtests/test_conv.py\u001b[0m:446: AssertionError\r\n",
      "----------------------------- Captured stdout call -----------------------------\r\n",
      "out_grad shape: (1, 12, 12, 2)\r\n",
      "stride: 1\r\n",
      "_out_grad shape: (1, 12, 12, 2)\r\n",
      "W shape: (3, 3, 2, 2)\r\n",
      "_W shape: (3, 3, 2, 2)\r\n",
      "X_grad shape: (1, 14, 14, 2)\r\n",
      "X_shape: (1, 14, 14, 2)\r\n",
      "_X shape: (2, 14, 14, 1)\r\n",
      "_out_grad shape: (12, 12, 1, 2)\r\n",
      "W_grad shape: (2, 3, 3, 2)\r\n",
      "W_grad shape: (3, 3, 2, 2)\r\n",
      "\u001b[31m\u001b[1m_ test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape0-W_shape0-1-0] _\u001b[0m\r\n",
      "\r\n",
      "Z_shape = (3, 14, 14, 8), W_shape = (3, 3, 8, 16), stride = 1, padding = 0\r\n",
      "backward = True, device = cuda()\r\n",
      "\r\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\r\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\r\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\r\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\r\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\r\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\r\n",
      "        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\r\n",
      "        _Z = _Z.astype(np.float32)\r\n",
      "        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\r\n",
      "        _W = _W.astype(np.float32)\r\n",
      "        Z = ndl.Tensor(_Z, device=device)\r\n",
      "        W = ndl.Tensor(_W, device=device)\r\n",
      "        y = ndl.conv(Z, W, padding=padding, stride=stride)\r\n",
      "        y2 = y.sum()\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      "            y2.backward()\r\n",
      "        Ztch = torch.Tensor(_Z).float()\r\n",
      "        Ztch.requires_grad=\u001b[94mTrue\u001b[39;49;00m\r\n",
      "        Wtch = torch.Tensor(_W).float()\r\n",
      "        Wtch.requires_grad=\u001b[94mTrue\u001b[39;49;00m\r\n",
      "        out = torch.nn.functional.conv2d(Ztch.permute(\u001b[94m0\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m), Wtch.permute(\u001b[94m3\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m), padding=padding, stride=stride)\r\n",
      "        out2 = out.sum()\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      "            out2.backward()\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      "            err1 = np.linalg.norm(Ztch.grad.numpy() - Z.grad.numpy())\r\n",
      "            err2 = np.linalg.norm(Wtch.grad.numpy() - W.grad.numpy())\r\n",
      "        err3 = np.linalg.norm(out2.detach().numpy() - y2.numpy())\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      ">           \u001b[94massert\u001b[39;49;00m err1 < \u001b[94m1e-2\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33minput grads match\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "\u001b[1m\u001b[31mE           AssertionError: input grads match\u001b[0m\r\n",
      "\u001b[1m\u001b[31mE           assert 135523.02 < 0.01\u001b[0m\r\n",
      "\r\n",
      "W          = needle.Tensor([[[[  4.282669     8.133604    -4.6223483  ...   2.0798197\r\n",
      "      2.1704152   -0.38803983]\r\n",
      "   [  2.245844...2.618842     0.71487164]\r\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\r\n",
      "      0.03223436   2.0512383 ]]]])\r\n",
      "W_shape    = (3, 3, 8, 16)\r\n",
      "Wtch       = tensor([[[[  4.2827,   8.1336,  -4.6223,  ...,   2.0798,   2.1704,  -0.3880],\r\n",
      "          [  2.2458,  -0.6565,  -3.7547,...   0.7149],\r\n",
      "          [ -4.0937,   3.7292, -10.5518,  ...,   0.3174,   0.0322,   2.0512]]]],\r\n",
      "       requires_grad=True)\r\n",
      "Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\r\n",
      "     4.75044203e+00 -7.56786048e...3e+00]\r\n",
      "   [ 1.09905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\r\n",
      "    -8.48226726e-01 -2.29984760e+00]]]])\r\n",
      "Z_shape    = (3, 14, 14, 8)\r\n",
      "Ztch       = tensor([[[[ 8.8203e+00,  2.0008e+00,  4.8937e+00,  ..., -4.8864e+00,\r\n",
      "            4.7504e+00, -7.5679e-01],\r\n",
      "          [...[ 1.0991e+01,  2.2317e+00,  4.6206e+00,  ..., -6.2309e-01,\r\n",
      "           -8.4823e-01, -2.2998e+00]]]], requires_grad=True)\r\n",
      "_W         = array([[[[  4.282669  ,   8.133604  ,  -4.6223483 , ...,   2.0798197 ,\r\n",
      "            2.1704152 ,  -0.38803983],\r\n",
      "        ... [ -4.09374   ,   3.7291534 , -10.551835  , ...,   0.31741843,\r\n",
      "            0.03223436,   2.0512383 ]]]], dtype=float32)\r\n",
      "_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\r\n",
      "          -4.88638926e+00,  4.75044203e+00, -7.56786...166728e+00,  4.62055349e+00, ...,\r\n",
      "          -6.23090088e-01, -8.48226726e-01, -2.29984760e+00]]]],\r\n",
      "      dtype=float32)\r\n",
      "backward   = True\r\n",
      "device     = cuda()\r\n",
      "err1       = 135523.02\r\n",
      "err2       = 0.0016771061\r\n",
      "err3       = 0.0034179688\r\n",
      "out        = tensor([[[[-2.1851e+01, -1.8266e+02, -2.9496e+02,  ...,  9.3036e+01,\r\n",
      "           -1.0480e+02,  1.1030e+02],\r\n",
      "          [...,  5.7277e+02, -9.0837e+01,  ..., -1.8747e+02,\r\n",
      "           -1.1281e+02,  2.0660e+02]]]], grad_fn=<ConvolutionBackward0>)\r\n",
      "out2       = tensor(-6614.2402, grad_fn=<SumBackward0>)\r\n",
      "padding    = 0\r\n",
      "stride     = 1\r\n",
      "torch      = <module 'torch' from '/home/ubuntu/anaconda3/envs/mlsys_hw/lib/python3.8/site-packages/torch/__init__.py'>\r\n",
      "y          = needle.Tensor([[[[-2.18507862e+01  7.10567322e+01 -2.19253067e+02 ...  1.22643860e+02\r\n",
      "     1.09581779e+02  5.29599533e...5e+02]\r\n",
      "   [ 1.20957283e+02 -4.53491516e+02  7.81254578e+01 ...  1.71220749e+02\r\n",
      "    -1.71809418e+02  2.06600098e+02]]]])\r\n",
      "y2         = needle.Tensor([-6614.237])\r\n",
      "\r\n",
      "\u001b[1m\u001b[31mtests/test_conv.py\u001b[0m:446: AssertionError\r\n",
      "----------------------------- Captured stdout call -----------------------------\r\n",
      "out_grad shape: (3, 12, 12, 16)\r\n",
      "stride: 1\r\n",
      "_out_grad shape: (3, 12, 12, 16)\r\n",
      "W shape: (3, 3, 8, 16)\r\n",
      "_W shape: (3, 3, 16, 8)\r\n",
      "X_grad shape: (3, 14, 14, 8)\r\n",
      "X_shape: (3, 14, 14, 8)\r\n",
      "_X shape: (8, 14, 14, 3)\r\n",
      "_out_grad shape: (12, 12, 3, 16)\r\n",
      "W_grad shape: (8, 3, 3, 16)\r\n",
      "W_grad shape: (3, 3, 8, 16)\r\n",
      "\u001b[31m\u001b[1m_ test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape1-W_shape1-1-1] _\u001b[0m\r\n",
      "\r\n",
      "Z_shape = (3, 14, 14, 8), W_shape = (3, 3, 8, 16), stride = 1, padding = 1\r\n",
      "backward = True, device = cuda()\r\n",
      "\r\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\r\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\r\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\r\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\r\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\r\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\r\n",
      "        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\r\n",
      "        _Z = _Z.astype(np.float32)\r\n",
      "        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\r\n",
      "        _W = _W.astype(np.float32)\r\n",
      "        Z = ndl.Tensor(_Z, device=device)\r\n",
      "        W = ndl.Tensor(_W, device=device)\r\n",
      "        y = ndl.conv(Z, W, padding=padding, stride=stride)\r\n",
      "        y2 = y.sum()\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      "            y2.backward()\r\n",
      "        Ztch = torch.Tensor(_Z).float()\r\n",
      "        Ztch.requires_grad=\u001b[94mTrue\u001b[39;49;00m\r\n",
      "        Wtch = torch.Tensor(_W).float()\r\n",
      "        Wtch.requires_grad=\u001b[94mTrue\u001b[39;49;00m\r\n",
      "        out = torch.nn.functional.conv2d(Ztch.permute(\u001b[94m0\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m), Wtch.permute(\u001b[94m3\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m), padding=padding, stride=stride)\r\n",
      "        out2 = out.sum()\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      "            out2.backward()\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      "            err1 = np.linalg.norm(Ztch.grad.numpy() - Z.grad.numpy())\r\n",
      "            err2 = np.linalg.norm(Wtch.grad.numpy() - W.grad.numpy())\r\n",
      "        err3 = np.linalg.norm(out2.detach().numpy() - y2.numpy())\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      ">           \u001b[94massert\u001b[39;49;00m err1 < \u001b[94m1e-2\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33minput grads match\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "\u001b[1m\u001b[31mE           AssertionError: input grads match\u001b[0m\r\n",
      "\u001b[1m\u001b[31mE           assert 56936.973 < 0.01\u001b[0m\r\n",
      "\r\n",
      "W          = needle.Tensor([[[[  4.282669     8.133604    -4.6223483  ...   2.0798197\r\n",
      "      2.1704152   -0.38803983]\r\n",
      "   [  2.245844...2.618842     0.71487164]\r\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\r\n",
      "      0.03223436   2.0512383 ]]]])\r\n",
      "W_shape    = (3, 3, 8, 16)\r\n",
      "Wtch       = tensor([[[[  4.2827,   8.1336,  -4.6223,  ...,   2.0798,   2.1704,  -0.3880],\r\n",
      "          [  2.2458,  -0.6565,  -3.7547,...   0.7149],\r\n",
      "          [ -4.0937,   3.7292, -10.5518,  ...,   0.3174,   0.0322,   2.0512]]]],\r\n",
      "       requires_grad=True)\r\n",
      "Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\r\n",
      "     4.75044203e+00 -7.56786048e...3e+00]\r\n",
      "   [ 1.09905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\r\n",
      "    -8.48226726e-01 -2.29984760e+00]]]])\r\n",
      "Z_shape    = (3, 14, 14, 8)\r\n",
      "Ztch       = tensor([[[[ 8.8203e+00,  2.0008e+00,  4.8937e+00,  ..., -4.8864e+00,\r\n",
      "            4.7504e+00, -7.5679e-01],\r\n",
      "          [...[ 1.0991e+01,  2.2317e+00,  4.6206e+00,  ..., -6.2309e-01,\r\n",
      "           -8.4823e-01, -2.2998e+00]]]], requires_grad=True)\r\n",
      "_W         = array([[[[  4.282669  ,   8.133604  ,  -4.6223483 , ...,   2.0798197 ,\r\n",
      "            2.1704152 ,  -0.38803983],\r\n",
      "        ... [ -4.09374   ,   3.7291534 , -10.551835  , ...,   0.31741843,\r\n",
      "            0.03223436,   2.0512383 ]]]], dtype=float32)\r\n",
      "_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\r\n",
      "          -4.88638926e+00,  4.75044203e+00, -7.56786...166728e+00,  4.62055349e+00, ...,\r\n",
      "          -6.23090088e-01, -8.48226726e-01, -2.29984760e+00]]]],\r\n",
      "      dtype=float32)\r\n",
      "backward   = True\r\n",
      "device     = cuda()\r\n",
      "err1       = 56936.973\r\n",
      "err2       = 0.0020093916\r\n",
      "err3       = 0.009765625\r\n",
      "out        = tensor([[[[ 107.2545, -109.1249,   -9.3744,  ...,  118.7832,  127.2525,\r\n",
      "           -146.6358],\r\n",
      "          [-296.2552,  ...[-107.9026,   -8.9582,  137.1271,  ..., -355.2759,  128.8402,\r\n",
      "             28.4111]]]], grad_fn=<ConvolutionBackward0>)\r\n",
      "out2       = tensor(-12155.8203, grad_fn=<SumBackward0>)\r\n",
      "padding    = 1\r\n",
      "stride     = 1\r\n",
      "torch      = <module 'torch' from '/home/ubuntu/anaconda3/envs/mlsys_hw/lib/python3.8/site-packages/torch/__init__.py'>\r\n",
      "y          = needle.Tensor([[[[ 107.25448      31.246029   -213.92354    ...  -29.267391\r\n",
      "      90.55236      20.23182   ]\r\n",
      "   [-109....98     128.84021   ]\r\n",
      "   [ 243.11641     201.83783      41.88371    ...  121.594315\r\n",
      "     127.64035      28.411123  ]]]])\r\n",
      "y2         = needle.Tensor([-12155.83])\r\n",
      "\r\n",
      "\u001b[1m\u001b[31mtests/test_conv.py\u001b[0m:446: AssertionError\r\n",
      "----------------------------- Captured stdout call -----------------------------\r\n",
      "out_grad shape: (3, 14, 14, 16)\r\n",
      "stride: 1\r\n",
      "_out_grad shape: (3, 14, 14, 16)\r\n",
      "W shape: (3, 3, 8, 16)\r\n",
      "_W shape: (3, 3, 16, 8)\r\n",
      "X_grad shape: (3, 14, 14, 8)\r\n",
      "X_shape: (3, 14, 14, 8)\r\n",
      "_X shape: (8, 14, 14, 3)\r\n",
      "_out_grad shape: (14, 14, 3, 16)\r\n",
      "W_grad shape: (8, 3, 3, 16)\r\n",
      "W_grad shape: (3, 3, 8, 16)\r\n",
      "\u001b[31m\u001b[1m_ test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape2-W_shape2-1-2] _\u001b[0m\r\n",
      "\r\n",
      "Z_shape = (3, 16, 16, 8), W_shape = (3, 3, 8, 16), stride = 1, padding = 2\r\n",
      "backward = True, device = cuda()\r\n",
      "\r\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\r\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\r\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\r\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\r\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\r\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\r\n",
      "        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\r\n",
      "        _Z = _Z.astype(np.float32)\r\n",
      "        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\r\n",
      "        _W = _W.astype(np.float32)\r\n",
      "        Z = ndl.Tensor(_Z, device=device)\r\n",
      "        W = ndl.Tensor(_W, device=device)\r\n",
      "        y = ndl.conv(Z, W, padding=padding, stride=stride)\r\n",
      "        y2 = y.sum()\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      "            y2.backward()\r\n",
      "        Ztch = torch.Tensor(_Z).float()\r\n",
      "        Ztch.requires_grad=\u001b[94mTrue\u001b[39;49;00m\r\n",
      "        Wtch = torch.Tensor(_W).float()\r\n",
      "        Wtch.requires_grad=\u001b[94mTrue\u001b[39;49;00m\r\n",
      "        out = torch.nn.functional.conv2d(Ztch.permute(\u001b[94m0\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m), Wtch.permute(\u001b[94m3\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m), padding=padding, stride=stride)\r\n",
      "        out2 = out.sum()\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      "            out2.backward()\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      "            err1 = np.linalg.norm(Ztch.grad.numpy() - Z.grad.numpy())\r\n",
      "            err2 = np.linalg.norm(Wtch.grad.numpy() - W.grad.numpy())\r\n",
      "        err3 = np.linalg.norm(out2.detach().numpy() - y2.numpy())\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      ">           \u001b[94massert\u001b[39;49;00m err1 < \u001b[94m1e-2\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33minput grads match\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "\u001b[1m\u001b[31mE           AssertionError: input grads match\u001b[0m\r\n",
      "\u001b[1m\u001b[31mE           assert 63359.355 < 0.01\u001b[0m\r\n",
      "\r\n",
      "W          = needle.Tensor([[[[  0.40910086  -0.64982927  11.320294   ...  -0.12077556\r\n",
      "      1.428692    -5.5845156 ]\r\n",
      "   [ -6.89178...-1.5437248  -10.136281  ]\r\n",
      "   [  1.1511405    0.7441038   16.463472   ...  -6.8931007\r\n",
      "     -0.5833115   -2.504636  ]]]])\r\n",
      "W_shape    = (3, 3, 8, 16)\r\n",
      "Wtch       = tensor([[[[  0.4091,  -0.6498,  11.3203,  ...,  -0.1208,   1.4287,  -5.5845],\r\n",
      "          [ -6.8918,  -9.3836,  10.6335,... -10.1363],\r\n",
      "          [  1.1511,   0.7441,  16.4635,  ...,  -6.8931,  -0.5833,  -2.5046]]]],\r\n",
      "       requires_grad=True)\r\n",
      "Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\r\n",
      "     4.75044203e+00 -7.56786048e...5e+00]\r\n",
      "   [ 6.10367346e+00 -2.05434513e+00 -4.41384506e+00 ... -6.15245628e+00\r\n",
      "     3.33514667e+00 -1.35630560e+00]]]])\r\n",
      "Z_shape    = (3, 16, 16, 8)\r\n",
      "Ztch       = tensor([[[[ 8.8203e+00,  2.0008e+00,  4.8937e+00,  ..., -4.8864e+00,\r\n",
      "            4.7504e+00, -7.5679e-01],\r\n",
      "          [...[ 6.1037e+00, -2.0543e+00, -4.4138e+00,  ..., -6.1525e+00,\r\n",
      "            3.3351e+00, -1.3563e+00]]]], requires_grad=True)\r\n",
      "_W         = array([[[[  0.40910086,  -0.64982927,  11.320294  , ...,  -0.12077556,\r\n",
      "            1.428692  ,  -5.5845156 ],\r\n",
      "        ... [  1.1511405 ,   0.7441038 ,  16.463472  , ...,  -6.8931007 ,\r\n",
      "           -0.5833115 ,  -2.504636  ]]]], dtype=float32)\r\n",
      "_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\r\n",
      "          -4.88638926e+00,  4.75044203e+00, -7.56786...434513e+00, -4.41384506e+00, ...,\r\n",
      "          -6.15245628e+00,  3.33514667e+00, -1.35630560e+00]]]],\r\n",
      "      dtype=float32)\r\n",
      "backward   = True\r\n",
      "device     = cuda()\r\n",
      "err1       = 63359.355\r\n",
      "err2       = 0.003127568\r\n",
      "err3       = 0.056640625\r\n",
      "out        = tensor([[[[-6.3835e+01, -7.3393e+01,  7.0442e+01,  ..., -8.6601e+01,\r\n",
      "            2.9473e+01,  3.8723e+01],\r\n",
      "          [...,  8.9989e+01,  2.4763e+01,  ..., -2.3400e+02,\r\n",
      "            1.3548e+02, -2.8245e+01]]]], grad_fn=<ConvolutionBackward0>)\r\n",
      "out2       = tensor(23346.6914, grad_fn=<SumBackward0>)\r\n",
      "padding    = 2\r\n",
      "stride     = 1\r\n",
      "torch      = <module 'torch' from '/home/ubuntu/anaconda3/envs/mlsys_hw/lib/python3.8/site-packages/torch/__init__.py'>\r\n",
      "y          = needle.Tensor([[[[-6.38349190e+01  1.26404709e+02 -6.92413330e+01 ...  7.84971542e+01\r\n",
      "     5.02614059e+01 -1.00136520e...1e+02]\r\n",
      "   [-7.55530777e+01 -3.63743286e+01  5.69509811e+01 ...  6.93700714e+01\r\n",
      "    -3.12737598e+01 -2.82451649e+01]]]])\r\n",
      "y2         = needle.Tensor([23346.748])\r\n",
      "\r\n",
      "\u001b[1m\u001b[31mtests/test_conv.py\u001b[0m:446: AssertionError\r\n",
      "----------------------------- Captured stdout call -----------------------------\r\n",
      "out_grad shape: (3, 18, 18, 16)\r\n",
      "stride: 1\r\n",
      "_out_grad shape: (3, 18, 18, 16)\r\n",
      "W shape: (3, 3, 8, 16)\r\n",
      "_W shape: (3, 3, 16, 8)\r\n",
      "X_grad shape: (3, 16, 16, 8)\r\n",
      "X_shape: (3, 16, 16, 8)\r\n",
      "_X shape: (8, 16, 16, 3)\r\n",
      "_out_grad shape: (18, 18, 3, 16)\r\n",
      "W_grad shape: (8, 3, 3, 16)\r\n",
      "W_grad shape: (3, 3, 8, 16)\r\n",
      "\u001b[31m\u001b[1m_ test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape3-W_shape3-1-0] _\u001b[0m\r\n",
      "\r\n",
      "Z_shape = (3, 16, 16, 8), W_shape = (3, 3, 8, 14), stride = 1, padding = 0\r\n",
      "backward = True, device = cuda()\r\n",
      "\r\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\r\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\r\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\r\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\r\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\r\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\r\n",
      "        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\r\n",
      "        _Z = _Z.astype(np.float32)\r\n",
      "        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\r\n",
      "        _W = _W.astype(np.float32)\r\n",
      "        Z = ndl.Tensor(_Z, device=device)\r\n",
      "        W = ndl.Tensor(_W, device=device)\r\n",
      "        y = ndl.conv(Z, W, padding=padding, stride=stride)\r\n",
      "        y2 = y.sum()\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      "            y2.backward()\r\n",
      "        Ztch = torch.Tensor(_Z).float()\r\n",
      "        Ztch.requires_grad=\u001b[94mTrue\u001b[39;49;00m\r\n",
      "        Wtch = torch.Tensor(_W).float()\r\n",
      "        Wtch.requires_grad=\u001b[94mTrue\u001b[39;49;00m\r\n",
      "        out = torch.nn.functional.conv2d(Ztch.permute(\u001b[94m0\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m), Wtch.permute(\u001b[94m3\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m), padding=padding, stride=stride)\r\n",
      "        out2 = out.sum()\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      "            out2.backward()\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      "            err1 = np.linalg.norm(Ztch.grad.numpy() - Z.grad.numpy())\r\n",
      "            err2 = np.linalg.norm(Wtch.grad.numpy() - W.grad.numpy())\r\n",
      "        err3 = np.linalg.norm(out2.detach().numpy() - y2.numpy())\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      ">           \u001b[94massert\u001b[39;49;00m err1 < \u001b[94m1e-2\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33minput grads match\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "\u001b[1m\u001b[31mE           AssertionError: input grads match\u001b[0m\r\n",
      "\u001b[1m\u001b[31mE           assert 131554.44 < 0.01\u001b[0m\r\n",
      "\r\n",
      "W          = needle.Tensor([[[[ 4.09100860e-01 -6.49829268e-01  1.13202944e+01 ... -5.56089449e+00\r\n",
      "    -4.84385538e+00 -1.20775558e...5e+00]\r\n",
      "   [-1.10714078e+00  2.54448557e+00 -7.73867989e+00 ... -1.13336074e+00\r\n",
      "    -1.21658230e+00 -4.79330921e+00]]]])\r\n",
      "W_shape    = (3, 3, 8, 14)\r\n",
      "Wtch       = tensor([[[[ 4.0910e-01, -6.4983e-01,  1.1320e+01,  ..., -5.5609e+00,\r\n",
      "           -4.8439e+00, -1.2078e-01],\r\n",
      "          [...[-1.1071e+00,  2.5445e+00, -7.7387e+00,  ..., -1.1334e+00,\r\n",
      "           -1.2166e+00, -4.7933e+00]]]], requires_grad=True)\r\n",
      "Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\r\n",
      "     4.75044203e+00 -7.56786048e...5e+00]\r\n",
      "   [ 6.10367346e+00 -2.05434513e+00 -4.41384506e+00 ... -6.15245628e+00\r\n",
      "     3.33514667e+00 -1.35630560e+00]]]])\r\n",
      "Z_shape    = (3, 16, 16, 8)\r\n",
      "Ztch       = tensor([[[[ 8.8203e+00,  2.0008e+00,  4.8937e+00,  ..., -4.8864e+00,\r\n",
      "            4.7504e+00, -7.5679e-01],\r\n",
      "          [...[ 6.1037e+00, -2.0543e+00, -4.4138e+00,  ..., -6.1525e+00,\r\n",
      "            3.3351e+00, -1.3563e+00]]]], requires_grad=True)\r\n",
      "_W         = array([[[[ 4.09100860e-01, -6.49829268e-01,  1.13202944e+01, ...,\r\n",
      "          -5.56089449e+00, -4.84385538e+00, -1.20775...448557e+00, -7.73867989e+00, ...,\r\n",
      "          -1.13336074e+00, -1.21658230e+00, -4.79330921e+00]]]],\r\n",
      "      dtype=float32)\r\n",
      "_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\r\n",
      "          -4.88638926e+00,  4.75044203e+00, -7.56786...434513e+00, -4.41384506e+00, ...,\r\n",
      "          -6.15245628e+00,  3.33514667e+00, -1.35630560e+00]]]],\r\n",
      "      dtype=float32)\r\n",
      "backward   = True\r\n",
      "device     = cuda()\r\n",
      "err1       = 131554.44\r\n",
      "err2       = 0.0018837381\r\n",
      "err3       = 0.049804688\r\n",
      "out        = tensor([[[[ 5.7346e+00, -2.1858e+02, -5.5911e+01,  ..., -8.9695e+01,\r\n",
      "            2.2620e+02,  3.4342e+02],\r\n",
      "          [...,  2.3405e+01, -2.1477e+01,  ...,  1.2947e+02,\r\n",
      "            1.2442e+02, -4.6003e+02]]]], grad_fn=<ConvolutionBackward0>)\r\n",
      "out2       = tensor(16161.7031, grad_fn=<SumBackward0>)\r\n",
      "padding    = 0\r\n",
      "stride     = 1\r\n",
      "torch      = <module 'torch' from '/home/ubuntu/anaconda3/envs/mlsys_hw/lib/python3.8/site-packages/torch/__init__.py'>\r\n",
      "y          = needle.Tensor([[[[ 5.73456383e+00 -2.22225971e+01  2.64719360e+02 ... -6.97373962e+01\r\n",
      "    -3.79622650e+02  9.98186493e...8e+02]\r\n",
      "   [ 3.72021103e+01  8.33753281e+01 -4.17866564e+00 ... -1.67722427e+02\r\n",
      "    -1.28576309e+02 -4.60030090e+02]]]])\r\n",
      "y2         = needle.Tensor([16161.753])\r\n",
      "\r\n",
      "\u001b[1m\u001b[31mtests/test_conv.py\u001b[0m:446: AssertionError\r\n",
      "----------------------------- Captured stdout call -----------------------------\r\n",
      "out_grad shape: (3, 14, 14, 14)\r\n",
      "stride: 1\r\n",
      "_out_grad shape: (3, 14, 14, 14)\r\n",
      "W shape: (3, 3, 8, 14)\r\n",
      "_W shape: (3, 3, 14, 8)\r\n",
      "X_grad shape: (3, 16, 16, 8)\r\n",
      "X_shape: (3, 16, 16, 8)\r\n",
      "_X shape: (8, 16, 16, 3)\r\n",
      "_out_grad shape: (14, 14, 3, 14)\r\n",
      "W_grad shape: (8, 3, 3, 14)\r\n",
      "W_grad shape: (3, 3, 8, 14)\r\n",
      "\u001b[31m\u001b[1m_ test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape4-W_shape4-1-0] _\u001b[0m\r\n",
      "\r\n",
      "Z_shape = (3, 16, 16, 2), W_shape = (3, 3, 2, 14), stride = 1, padding = 0\r\n",
      "backward = True, device = cuda()\r\n",
      "\r\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\r\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\r\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\r\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\r\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\r\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\r\n",
      "        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\r\n",
      "        _Z = _Z.astype(np.float32)\r\n",
      "        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\r\n",
      "        _W = _W.astype(np.float32)\r\n",
      "        Z = ndl.Tensor(_Z, device=device)\r\n",
      "        W = ndl.Tensor(_W, device=device)\r\n",
      "        y = ndl.conv(Z, W, padding=padding, stride=stride)\r\n",
      "        y2 = y.sum()\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      "            y2.backward()\r\n",
      "        Ztch = torch.Tensor(_Z).float()\r\n",
      "        Ztch.requires_grad=\u001b[94mTrue\u001b[39;49;00m\r\n",
      "        Wtch = torch.Tensor(_W).float()\r\n",
      "        Wtch.requires_grad=\u001b[94mTrue\u001b[39;49;00m\r\n",
      "        out = torch.nn.functional.conv2d(Ztch.permute(\u001b[94m0\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m), Wtch.permute(\u001b[94m3\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m), padding=padding, stride=stride)\r\n",
      "        out2 = out.sum()\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      "            out2.backward()\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      "            err1 = np.linalg.norm(Ztch.grad.numpy() - Z.grad.numpy())\r\n",
      "            err2 = np.linalg.norm(Wtch.grad.numpy() - W.grad.numpy())\r\n",
      "        err3 = np.linalg.norm(out2.detach().numpy() - y2.numpy())\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      ">           \u001b[94massert\u001b[39;49;00m err1 < \u001b[94m1e-2\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33minput grads match\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "\u001b[1m\u001b[31mE           AssertionError: input grads match\u001b[0m\r\n",
      "\u001b[1m\u001b[31mE           assert 149034.6 < 0.01\u001b[0m\r\n",
      "\r\n",
      "W          = needle.Tensor([[[[ -8.0968      -2.5552022    8.703147    -1.4674252    4.5861077\r\n",
      "     -0.28521433   4.383634    -9.13...727333   0.22582927   9.296732    -8.13161     -0.67411226\r\n",
      "     -2.9204676    1.675528   -12.187821     5.5746226 ]]]])\r\n",
      "W_shape    = (3, 3, 2, 14)\r\n",
      "Wtch       = tensor([[[[ -8.0968,  -2.5552,   8.7031,  -1.4674,   4.5861,  -0.2852,   4.3836,\r\n",
      "            -9.1346,  -2.0159,   4.74...0.2258,\r\n",
      "             9.2967,  -8.1316,  -0.6741,  -2.9205,   1.6755, -12.1878,   5.5746]]]],\r\n",
      "       requires_grad=True)\r\n",
      "Z          = needle.Tensor([[[[  8.820262     2.000786  ]\r\n",
      "   [  4.89369     11.204466  ]\r\n",
      "   [  9.33779     -4.8863893 ]\r\n",
      "   ...\r\n",
      "   [...   -5.112822  ]\r\n",
      "   ...\r\n",
      "   [ -7.922368     4.2222714 ]\r\n",
      "   [ -6.064339     1.4188478 ]\r\n",
      "   [ -1.4109794   -5.791016  ]]]])\r\n",
      "Z_shape    = (3, 16, 16, 2)\r\n",
      "Ztch       = tensor([[[[  8.8203,   2.0008],\r\n",
      "          [  4.8937,  11.2045],\r\n",
      "          [  9.3378,  -4.8864],\r\n",
      "          ...,\r\n",
      "       ...\r\n",
      "          [ -7.9224,   4.2223],\r\n",
      "          [ -6.0643,   1.4188],\r\n",
      "          [ -1.4110,  -5.7910]]]], requires_grad=True)\r\n",
      "_W         = array([[[[ -8.0968    ,  -2.5552022 ,   8.703147  ,  -1.4674252 ,\r\n",
      "            4.5861077 ,  -0.28521433,   4.383634  , ...        -8.13161   ,  -0.67411226,  -2.9204676 ,   1.675528  ,\r\n",
      "          -12.187821  ,   5.5746226 ]]]], dtype=float32)\r\n",
      "_Z         = array([[[[  8.820262  ,   2.000786  ],\r\n",
      "         [  4.89369   ,  11.204466  ],\r\n",
      "         [  9.33779   ,  -4.8863893 ],\r\n",
      " ...22368  ,   4.2222714 ],\r\n",
      "         [ -6.064339  ,   1.4188478 ],\r\n",
      "         [ -1.4109794 ,  -5.791016  ]]]], dtype=float32)\r\n",
      "backward   = True\r\n",
      "device     = cuda()\r\n",
      "err1       = 149034.6\r\n",
      "err2       = 0.0006921925\r\n",
      "err3       = 0.0014648438\r\n",
      "out        = tensor([[[[-346.0716,  -28.1759,   63.5063,  ..., -158.1952,  -57.8746,\r\n",
      "             63.6776],\r\n",
      "          [  73.3426,  ...[  58.9731,    3.5575, -155.4996,  ...,  123.7366,   54.5836,\r\n",
      "             92.4735]]]], grad_fn=<ConvolutionBackward0>)\r\n",
      "out2       = tensor(-6321.1787, grad_fn=<SumBackward0>)\r\n",
      "padding    = 0\r\n",
      "stride     = 1\r\n",
      "torch      = <module 'torch' from '/home/ubuntu/anaconda3/envs/mlsys_hw/lib/python3.8/site-packages/torch/__init__.py'>\r\n",
      "y          = needle.Tensor([[[[-346.07162     -1.1081867  420.15115   ...  -58.583153\r\n",
      "     -66.19938     33.158627 ]\r\n",
      "   [ -28.17595... -8.201235    54.58357  ]\r\n",
      "   [ -17.069721  -119.112015    24.423635  ...   47.367493\r\n",
      "     -38.16554     92.47348  ]]]])\r\n",
      "y2         = needle.Tensor([-6321.18])\r\n",
      "\r\n",
      "\u001b[1m\u001b[31mtests/test_conv.py\u001b[0m:446: AssertionError\r\n",
      "----------------------------- Captured stdout call -----------------------------\r\n",
      "out_grad shape: (3, 14, 14, 14)\r\n",
      "stride: 1\r\n",
      "_out_grad shape: (3, 14, 14, 14)\r\n",
      "W shape: (3, 3, 2, 14)\r\n",
      "_W shape: (3, 3, 14, 2)\r\n",
      "X_grad shape: (3, 16, 16, 2)\r\n",
      "X_shape: (3, 16, 16, 2)\r\n",
      "_X shape: (2, 16, 16, 3)\r\n",
      "_out_grad shape: (14, 14, 3, 14)\r\n",
      "W_grad shape: (2, 3, 3, 14)\r\n",
      "W_grad shape: (3, 3, 2, 14)\r\n",
      "\u001b[31m\u001b[1m_ test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape5-W_shape5-2-0] _\u001b[0m\r\n",
      "\r\n",
      "Z_shape = (3, 14, 14, 8), W_shape = (3, 3, 8, 16), stride = 2, padding = 0\r\n",
      "backward = True, device = cuda()\r\n",
      "\r\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\r\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\r\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\r\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\r\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\r\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\r\n",
      "        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\r\n",
      "        _Z = _Z.astype(np.float32)\r\n",
      "        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\r\n",
      "        _W = _W.astype(np.float32)\r\n",
      "        Z = ndl.Tensor(_Z, device=device)\r\n",
      "        W = ndl.Tensor(_W, device=device)\r\n",
      "        y = ndl.conv(Z, W, padding=padding, stride=stride)\r\n",
      "        y2 = y.sum()\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      "            y2.backward()\r\n",
      "        Ztch = torch.Tensor(_Z).float()\r\n",
      "        Ztch.requires_grad=\u001b[94mTrue\u001b[39;49;00m\r\n",
      "        Wtch = torch.Tensor(_W).float()\r\n",
      "        Wtch.requires_grad=\u001b[94mTrue\u001b[39;49;00m\r\n",
      "        out = torch.nn.functional.conv2d(Ztch.permute(\u001b[94m0\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m), Wtch.permute(\u001b[94m3\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m), padding=padding, stride=stride)\r\n",
      "        out2 = out.sum()\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      "            out2.backward()\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      "            err1 = np.linalg.norm(Ztch.grad.numpy() - Z.grad.numpy())\r\n",
      "            err2 = np.linalg.norm(Wtch.grad.numpy() - W.grad.numpy())\r\n",
      "        err3 = np.linalg.norm(out2.detach().numpy() - y2.numpy())\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      ">           \u001b[94massert\u001b[39;49;00m err1 < \u001b[94m1e-2\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33minput grads match\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "\u001b[1m\u001b[31mE           AssertionError: input grads match\u001b[0m\r\n",
      "\u001b[1m\u001b[31mE           assert 66302.79 < 0.01\u001b[0m\r\n",
      "\r\n",
      "W          = needle.Tensor([[[[  4.282669     8.133604    -4.6223483  ...   2.0798197\r\n",
      "      2.1704152   -0.38803983]\r\n",
      "   [  2.245844...2.618842     0.71487164]\r\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\r\n",
      "      0.03223436   2.0512383 ]]]])\r\n",
      "W_shape    = (3, 3, 8, 16)\r\n",
      "Wtch       = tensor([[[[  4.2827,   8.1336,  -4.6223,  ...,   2.0798,   2.1704,  -0.3880],\r\n",
      "          [  2.2458,  -0.6565,  -3.7547,...   0.7149],\r\n",
      "          [ -4.0937,   3.7292, -10.5518,  ...,   0.3174,   0.0322,   2.0512]]]],\r\n",
      "       requires_grad=True)\r\n",
      "Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\r\n",
      "     4.75044203e+00 -7.56786048e...3e+00]\r\n",
      "   [ 1.09905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\r\n",
      "    -8.48226726e-01 -2.29984760e+00]]]])\r\n",
      "Z_shape    = (3, 14, 14, 8)\r\n",
      "Ztch       = tensor([[[[ 8.8203e+00,  2.0008e+00,  4.8937e+00,  ..., -4.8864e+00,\r\n",
      "            4.7504e+00, -7.5679e-01],\r\n",
      "          [...[ 1.0991e+01,  2.2317e+00,  4.6206e+00,  ..., -6.2309e-01,\r\n",
      "           -8.4823e-01, -2.2998e+00]]]], requires_grad=True)\r\n",
      "_W         = array([[[[  4.282669  ,   8.133604  ,  -4.6223483 , ...,   2.0798197 ,\r\n",
      "            2.1704152 ,  -0.38803983],\r\n",
      "        ... [ -4.09374   ,   3.7291534 , -10.551835  , ...,   0.31741843,\r\n",
      "            0.03223436,   2.0512383 ]]]], dtype=float32)\r\n",
      "_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\r\n",
      "          -4.88638926e+00,  4.75044203e+00, -7.56786...166728e+00,  4.62055349e+00, ...,\r\n",
      "          -6.23090088e-01, -8.48226726e-01, -2.29984760e+00]]]],\r\n",
      "      dtype=float32)\r\n",
      "backward   = True\r\n",
      "device     = cuda()\r\n",
      "err1       = 66302.79\r\n",
      "err2       = 1798.237\r\n",
      "err3       = 0.0029296875\r\n",
      "out        = tensor([[[[ -21.8508, -294.9621, -318.0497,   -4.7934,   59.4825, -104.8043],\r\n",
      "          [  14.3621,  303.2643, -221.53...          [ 183.8510, -115.7199,  115.7395, -105.5507,   75.9456, -171.5427]]]],\r\n",
      "       grad_fn=<ConvolutionBackward0>)\r\n",
      "out2       = tensor(-4902.2178, grad_fn=<SumBackward0>)\r\n",
      "padding    = 0\r\n",
      "stride     = 2\r\n",
      "torch      = <module 'torch' from '/home/ubuntu/anaconda3/envs/mlsys_hw/lib/python3.8/site-packages/torch/__init__.py'>\r\n",
      "y          = needle.Tensor([[[[ -21.850786    71.05673   -219.25307   ...  122.64386\r\n",
      "     109.58178     52.959953 ]\r\n",
      "   [-294.9621  ...-120.10991     75.945595 ]\r\n",
      "   [ 372.3654    -347.1187     253.12747   ...  220.83806\r\n",
      "     113.929146  -171.54272  ]]]])\r\n",
      "y2         = needle.Tensor([-4902.2207])\r\n",
      "\r\n",
      "\u001b[1m\u001b[31mtests/test_conv.py\u001b[0m:446: AssertionError\r\n",
      "----------------------------- Captured stdout call -----------------------------\r\n",
      "out_grad shape: (3, 6, 6, 16)\r\n",
      "stride: 2\r\n",
      "_out_grad shape: (3, 12, 12, 16)\r\n",
      "W shape: (3, 3, 8, 16)\r\n",
      "_W shape: (3, 3, 16, 8)\r\n",
      "X_grad shape: (3, 14, 14, 8)\r\n",
      "X_shape: (3, 14, 14, 8)\r\n",
      "_X shape: (8, 14, 14, 3)\r\n",
      "_out_grad shape: (12, 12, 3, 16)\r\n",
      "W_grad shape: (8, 3, 3, 16)\r\n",
      "W_grad shape: (3, 3, 8, 16)\r\n",
      "\u001b[31m\u001b[1m_ test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape6-W_shape6-2-1] _\u001b[0m\r\n",
      "\r\n",
      "Z_shape = (3, 14, 14, 8), W_shape = (3, 3, 8, 16), stride = 2, padding = 1\r\n",
      "backward = True, device = cuda()\r\n",
      "\r\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\r\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\r\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\r\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\r\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\r\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\r\n",
      "        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\r\n",
      "        _Z = _Z.astype(np.float32)\r\n",
      "        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\r\n",
      "        _W = _W.astype(np.float32)\r\n",
      "        Z = ndl.Tensor(_Z, device=device)\r\n",
      "        W = ndl.Tensor(_W, device=device)\r\n",
      "        y = ndl.conv(Z, W, padding=padding, stride=stride)\r\n",
      "        y2 = y.sum()\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      "            y2.backward()\r\n",
      "        Ztch = torch.Tensor(_Z).float()\r\n",
      "        Ztch.requires_grad=\u001b[94mTrue\u001b[39;49;00m\r\n",
      "        Wtch = torch.Tensor(_W).float()\r\n",
      "        Wtch.requires_grad=\u001b[94mTrue\u001b[39;49;00m\r\n",
      "        out = torch.nn.functional.conv2d(Ztch.permute(\u001b[94m0\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m), Wtch.permute(\u001b[94m3\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m), padding=padding, stride=stride)\r\n",
      "        out2 = out.sum()\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      "            out2.backward()\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      "            err1 = np.linalg.norm(Ztch.grad.numpy() - Z.grad.numpy())\r\n",
      "            err2 = np.linalg.norm(Wtch.grad.numpy() - W.grad.numpy())\r\n",
      "        err3 = np.linalg.norm(out2.detach().numpy() - y2.numpy())\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      ">           \u001b[94massert\u001b[39;49;00m err1 < \u001b[94m1e-2\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33minput grads match\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "\u001b[1m\u001b[31mE           AssertionError: input grads match\u001b[0m\r\n",
      "\u001b[1m\u001b[31mE           assert 37368.434 < 0.01\u001b[0m\r\n",
      "\r\n",
      "W          = needle.Tensor([[[[  4.282669     8.133604    -4.6223483  ...   2.0798197\r\n",
      "      2.1704152   -0.38803983]\r\n",
      "   [  2.245844...2.618842     0.71487164]\r\n",
      "   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\r\n",
      "      0.03223436   2.0512383 ]]]])\r\n",
      "W_shape    = (3, 3, 8, 16)\r\n",
      "Wtch       = tensor([[[[  4.2827,   8.1336,  -4.6223,  ...,   2.0798,   2.1704,  -0.3880],\r\n",
      "          [  2.2458,  -0.6565,  -3.7547,...   0.7149],\r\n",
      "          [ -4.0937,   3.7292, -10.5518,  ...,   0.3174,   0.0322,   2.0512]]]],\r\n",
      "       requires_grad=True)\r\n",
      "Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\r\n",
      "     4.75044203e+00 -7.56786048e...3e+00]\r\n",
      "   [ 1.09905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\r\n",
      "    -8.48226726e-01 -2.29984760e+00]]]])\r\n",
      "Z_shape    = (3, 14, 14, 8)\r\n",
      "Ztch       = tensor([[[[ 8.8203e+00,  2.0008e+00,  4.8937e+00,  ..., -4.8864e+00,\r\n",
      "            4.7504e+00, -7.5679e-01],\r\n",
      "          [...[ 1.0991e+01,  2.2317e+00,  4.6206e+00,  ..., -6.2309e-01,\r\n",
      "           -8.4823e-01, -2.2998e+00]]]], requires_grad=True)\r\n",
      "_W         = array([[[[  4.282669  ,   8.133604  ,  -4.6223483 , ...,   2.0798197 ,\r\n",
      "            2.1704152 ,  -0.38803983],\r\n",
      "        ... [ -4.09374   ,   3.7291534 , -10.551835  , ...,   0.31741843,\r\n",
      "            0.03223436,   2.0512383 ]]]], dtype=float32)\r\n",
      "_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\r\n",
      "          -4.88638926e+00,  4.75044203e+00, -7.56786...166728e+00,  4.62055349e+00, ...,\r\n",
      "          -6.23090088e-01, -8.48226726e-01, -2.29984760e+00]]]],\r\n",
      "      dtype=float32)\r\n",
      "backward   = True\r\n",
      "device     = cuda()\r\n",
      "err1       = 37368.434\r\n",
      "err2       = 2246.7993\r\n",
      "err3       = 0.0034179688\r\n",
      "out        = tensor([[[[ 1.0725e+02, -9.3744e+00,  2.7834e+02,  ..., -9.7155e+01,\r\n",
      "            1.3371e+02,  1.2725e+02],\r\n",
      "          [...,  5.7277e+02,  2.3476e+01,  ..., -2.1788e+02,\r\n",
      "           -1.8747e+02,  2.0660e+02]]]], grad_fn=<ConvolutionBackward0>)\r\n",
      "out2       = tensor(4313.9385, grad_fn=<SumBackward0>)\r\n",
      "padding    = 1\r\n",
      "stride     = 2\r\n",
      "torch      = <module 'torch' from '/home/ubuntu/anaconda3/envs/mlsys_hw/lib/python3.8/site-packages/torch/__init__.py'>\r\n",
      "y          = needle.Tensor([[[[ 1.07254478e+02  3.12460289e+01 -2.13923538e+02 ... -2.92673912e+01\r\n",
      "     9.05523605e+01  2.02318192e...9e+02]\r\n",
      "   [ 1.20957283e+02 -4.53491516e+02  7.81254578e+01 ...  1.71220749e+02\r\n",
      "    -1.71809418e+02  2.06600098e+02]]]])\r\n",
      "y2         = needle.Tensor([4313.935])\r\n",
      "\r\n",
      "\u001b[1m\u001b[31mtests/test_conv.py\u001b[0m:446: AssertionError\r\n",
      "----------------------------- Captured stdout call -----------------------------\r\n",
      "out_grad shape: (3, 7, 7, 16)\r\n",
      "stride: 2\r\n",
      "_out_grad shape: (3, 14, 14, 16)\r\n",
      "W shape: (3, 3, 8, 16)\r\n",
      "_W shape: (3, 3, 16, 8)\r\n",
      "X_grad shape: (3, 14, 14, 8)\r\n",
      "X_shape: (3, 14, 14, 8)\r\n",
      "_X shape: (8, 14, 14, 3)\r\n",
      "_out_grad shape: (14, 14, 3, 16)\r\n",
      "W_grad shape: (8, 3, 3, 16)\r\n",
      "W_grad shape: (3, 3, 8, 16)\r\n",
      "\u001b[31m\u001b[1m_ test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape7-W_shape7-2-2] _\u001b[0m\r\n",
      "\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z_shape = (3, 16, 16, 8), W_shape = (3, 3, 8, 16), stride = 2, padding = 2\r\n",
      "backward = True, device = cuda()\r\n",
      "\r\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\r\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\r\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\r\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\r\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\r\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\r\n",
      "        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\r\n",
      "        _Z = _Z.astype(np.float32)\r\n",
      "        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\r\n",
      "        _W = _W.astype(np.float32)\r\n",
      "        Z = ndl.Tensor(_Z, device=device)\r\n",
      "        W = ndl.Tensor(_W, device=device)\r\n",
      "        y = ndl.conv(Z, W, padding=padding, stride=stride)\r\n",
      "        y2 = y.sum()\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      "            y2.backward()\r\n",
      "        Ztch = torch.Tensor(_Z).float()\r\n",
      "        Ztch.requires_grad=\u001b[94mTrue\u001b[39;49;00m\r\n",
      "        Wtch = torch.Tensor(_W).float()\r\n",
      "        Wtch.requires_grad=\u001b[94mTrue\u001b[39;49;00m\r\n",
      "        out = torch.nn.functional.conv2d(Ztch.permute(\u001b[94m0\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m), Wtch.permute(\u001b[94m3\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m), padding=padding, stride=stride)\r\n",
      "        out2 = out.sum()\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      "            out2.backward()\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      "            err1 = np.linalg.norm(Ztch.grad.numpy() - Z.grad.numpy())\r\n",
      "            err2 = np.linalg.norm(Wtch.grad.numpy() - W.grad.numpy())\r\n",
      "        err3 = np.linalg.norm(out2.detach().numpy() - y2.numpy())\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      ">           \u001b[94massert\u001b[39;49;00m err1 < \u001b[94m1e-2\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33minput grads match\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "\u001b[1m\u001b[31mE           AssertionError: input grads match\u001b[0m\r\n",
      "\u001b[1m\u001b[31mE           assert 130971.21 < 0.01\u001b[0m\r\n",
      "\r\n",
      "W          = needle.Tensor([[[[  0.40910086  -0.64982927  11.320294   ...  -0.12077556\r\n",
      "      1.428692    -5.5845156 ]\r\n",
      "   [ -6.89178...-1.5437248  -10.136281  ]\r\n",
      "   [  1.1511405    0.7441038   16.463472   ...  -6.8931007\r\n",
      "     -0.5833115   -2.504636  ]]]])\r\n",
      "W_shape    = (3, 3, 8, 16)\r\n",
      "Wtch       = tensor([[[[  0.4091,  -0.6498,  11.3203,  ...,  -0.1208,   1.4287,  -5.5845],\r\n",
      "          [ -6.8918,  -9.3836,  10.6335,... -10.1363],\r\n",
      "          [  1.1511,   0.7441,  16.4635,  ...,  -6.8931,  -0.5833,  -2.5046]]]],\r\n",
      "       requires_grad=True)\r\n",
      "Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\r\n",
      "     4.75044203e+00 -7.56786048e...5e+00]\r\n",
      "   [ 6.10367346e+00 -2.05434513e+00 -4.41384506e+00 ... -6.15245628e+00\r\n",
      "     3.33514667e+00 -1.35630560e+00]]]])\r\n",
      "Z_shape    = (3, 16, 16, 8)\r\n",
      "Ztch       = tensor([[[[ 8.8203e+00,  2.0008e+00,  4.8937e+00,  ..., -4.8864e+00,\r\n",
      "            4.7504e+00, -7.5679e-01],\r\n",
      "          [...[ 6.1037e+00, -2.0543e+00, -4.4138e+00,  ..., -6.1525e+00,\r\n",
      "            3.3351e+00, -1.3563e+00]]]], requires_grad=True)\r\n",
      "_W         = array([[[[  0.40910086,  -0.64982927,  11.320294  , ...,  -0.12077556,\r\n",
      "            1.428692  ,  -5.5845156 ],\r\n",
      "        ... [  1.1511405 ,   0.7441038 ,  16.463472  , ...,  -6.8931007 ,\r\n",
      "           -0.5833115 ,  -2.504636  ]]]], dtype=float32)\r\n",
      "_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\r\n",
      "          -4.88638926e+00,  4.75044203e+00, -7.56786...434513e+00, -4.41384506e+00, ...,\r\n",
      "          -6.15245628e+00,  3.33514667e+00, -1.35630560e+00]]]],\r\n",
      "      dtype=float32)\r\n",
      "backward   = True\r\n",
      "device     = cuda()\r\n",
      "err1       = 130971.21\r\n",
      "err2       = 2713.6147\r\n",
      "err3       = 0.0078125\r\n",
      "out        = tensor([[[[-6.3835e+01,  7.0442e+01, -1.5787e+02,  ..., -1.1571e+02,\r\n",
      "            2.7167e+02,  2.9473e+01],\r\n",
      "          [...,  1.5725e+02, -4.1424e+01,  ...,  1.6422e+02,\r\n",
      "            1.0298e+02,  8.0763e+01]]]], grad_fn=<ConvolutionBackward0>)\r\n",
      "out2       = tensor(-8988.4932, grad_fn=<SumBackward0>)\r\n",
      "padding    = 2\r\n",
      "stride     = 2\r\n",
      "torch      = <module 'torch' from '/home/ubuntu/anaconda3/envs/mlsys_hw/lib/python3.8/site-packages/torch/__init__.py'>\r\n",
      "y          = needle.Tensor([[[[-6.38349190e+01  1.26404709e+02 -6.92413330e+01 ...  7.84971542e+01\r\n",
      "     5.02614059e+01 -1.00136520e...3e+02]\r\n",
      "   [-8.42394543e+00 -1.02299728e+02 -4.70671501e+01 ...  7.48113098e+01\r\n",
      "     7.02560806e+01  8.07628021e+01]]]])\r\n",
      "y2         = needle.Tensor([-8988.485])\r\n",
      "\r\n",
      "\u001b[1m\u001b[31mtests/test_conv.py\u001b[0m:446: AssertionError\r\n",
      "----------------------------- Captured stdout call -----------------------------\r\n",
      "out_grad shape: (3, 9, 9, 16)\r\n",
      "stride: 2\r\n",
      "_out_grad shape: (3, 18, 18, 16)\r\n",
      "W shape: (3, 3, 8, 16)\r\n",
      "_W shape: (3, 3, 16, 8)\r\n",
      "X_grad shape: (3, 16, 16, 8)\r\n",
      "X_shape: (3, 16, 16, 8)\r\n",
      "_X shape: (8, 16, 16, 3)\r\n",
      "_out_grad shape: (18, 18, 3, 16)\r\n",
      "W_grad shape: (8, 3, 3, 16)\r\n",
      "W_grad shape: (3, 3, 8, 16)\r\n",
      "\u001b[31m\u001b[1m_ test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape8-W_shape8-2-0] _\u001b[0m\r\n",
      "\r\n",
      "Z_shape = (3, 16, 16, 8), W_shape = (3, 3, 8, 14), stride = 2, padding = 0\r\n",
      "backward = True, device = cuda()\r\n",
      "\r\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\r\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\r\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\r\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\r\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\r\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\r\n",
      "        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\r\n",
      "        _Z = _Z.astype(np.float32)\r\n",
      "        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\r\n",
      "        _W = _W.astype(np.float32)\r\n",
      "        Z = ndl.Tensor(_Z, device=device)\r\n",
      "        W = ndl.Tensor(_W, device=device)\r\n",
      "        y = ndl.conv(Z, W, padding=padding, stride=stride)\r\n",
      "        y2 = y.sum()\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      "            y2.backward()\r\n",
      "        Ztch = torch.Tensor(_Z).float()\r\n",
      "        Ztch.requires_grad=\u001b[94mTrue\u001b[39;49;00m\r\n",
      "        Wtch = torch.Tensor(_W).float()\r\n",
      "        Wtch.requires_grad=\u001b[94mTrue\u001b[39;49;00m\r\n",
      "        out = torch.nn.functional.conv2d(Ztch.permute(\u001b[94m0\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m), Wtch.permute(\u001b[94m3\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m), padding=padding, stride=stride)\r\n",
      "        out2 = out.sum()\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      "            out2.backward()\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      "            err1 = np.linalg.norm(Ztch.grad.numpy() - Z.grad.numpy())\r\n",
      "            err2 = np.linalg.norm(Wtch.grad.numpy() - W.grad.numpy())\r\n",
      "        err3 = np.linalg.norm(out2.detach().numpy() - y2.numpy())\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      ">           \u001b[94massert\u001b[39;49;00m err1 < \u001b[94m1e-2\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33minput grads match\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "\u001b[1m\u001b[31mE           AssertionError: input grads match\u001b[0m\r\n",
      "\u001b[1m\u001b[31mE           assert 308386.5 < 0.01\u001b[0m\r\n",
      "\r\n",
      "W          = needle.Tensor([[[[ 4.09100860e-01 -6.49829268e-01  1.13202944e+01 ... -5.56089449e+00\r\n",
      "    -4.84385538e+00 -1.20775558e...5e+00]\r\n",
      "   [-1.10714078e+00  2.54448557e+00 -7.73867989e+00 ... -1.13336074e+00\r\n",
      "    -1.21658230e+00 -4.79330921e+00]]]])\r\n",
      "W_shape    = (3, 3, 8, 14)\r\n",
      "Wtch       = tensor([[[[ 4.0910e-01, -6.4983e-01,  1.1320e+01,  ..., -5.5609e+00,\r\n",
      "           -4.8439e+00, -1.2078e-01],\r\n",
      "          [...[-1.1071e+00,  2.5445e+00, -7.7387e+00,  ..., -1.1334e+00,\r\n",
      "           -1.2166e+00, -4.7933e+00]]]], requires_grad=True)\r\n",
      "Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\r\n",
      "     4.75044203e+00 -7.56786048e...5e+00]\r\n",
      "   [ 6.10367346e+00 -2.05434513e+00 -4.41384506e+00 ... -6.15245628e+00\r\n",
      "     3.33514667e+00 -1.35630560e+00]]]])\r\n",
      "Z_shape    = (3, 16, 16, 8)\r\n",
      "Ztch       = tensor([[[[ 8.8203e+00,  2.0008e+00,  4.8937e+00,  ..., -4.8864e+00,\r\n",
      "            4.7504e+00, -7.5679e-01],\r\n",
      "          [...[ 6.1037e+00, -2.0543e+00, -4.4138e+00,  ..., -6.1525e+00,\r\n",
      "            3.3351e+00, -1.3563e+00]]]], requires_grad=True)\r\n",
      "_W         = array([[[[ 4.09100860e-01, -6.49829268e-01,  1.13202944e+01, ...,\r\n",
      "          -5.56089449e+00, -4.84385538e+00, -1.20775...448557e+00, -7.73867989e+00, ...,\r\n",
      "          -1.13336074e+00, -1.21658230e+00, -4.79330921e+00]]]],\r\n",
      "      dtype=float32)\r\n",
      "_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\r\n",
      "          -4.88638926e+00,  4.75044203e+00, -7.56786...434513e+00, -4.41384506e+00, ...,\r\n",
      "          -6.15245628e+00,  3.33514667e+00, -1.35630560e+00]]]],\r\n",
      "      dtype=float32)\r\n",
      "backward   = True\r\n",
      "device     = cuda()\r\n",
      "err1       = 308386.5\r\n",
      "err2       = 2255.245\r\n",
      "err3       = 0.00390625\r\n",
      "out        = tensor([[[[   5.7346,  -55.9106,  320.3619,  ..., -472.6719,  -63.6798,\r\n",
      "            226.2005],\r\n",
      "          [ -39.7949, -...[ -34.6254, -262.0989, -192.8079,  ..., -480.3307,  172.4242,\r\n",
      "            163.7107]]]], grad_fn=<ConvolutionBackward0>)\r\n",
      "out2       = tensor(10302.4697, grad_fn=<SumBackward0>)\r\n",
      "padding    = 0\r\n",
      "stride     = 2\r\n",
      "torch      = <module 'torch' from '/home/ubuntu/anaconda3/envs/mlsys_hw/lib/python3.8/site-packages/torch/__init__.py'>\r\n",
      "y          = needle.Tensor([[[[   5.734564   -22.222597   264.71936   ...  -69.7374\r\n",
      "    -379.62265     99.81865  ]\r\n",
      "   [ -55.910625 ... 143.49785    172.42424  ]\r\n",
      "   [ 140.06458    -33.92796    128.48518   ...  228.07355\r\n",
      "     153.4408     163.71072  ]]]])\r\n",
      "y2         = needle.Tensor([10302.474])\r\n",
      "\r\n",
      "\u001b[1m\u001b[31mtests/test_conv.py\u001b[0m:446: AssertionError\r\n",
      "----------------------------- Captured stdout call -----------------------------\r\n",
      "out_grad shape: (3, 7, 7, 14)\r\n",
      "stride: 2\r\n",
      "_out_grad shape: (3, 14, 14, 14)\r\n",
      "W shape: (3, 3, 8, 14)\r\n",
      "_W shape: (3, 3, 14, 8)\r\n",
      "X_grad shape: (3, 16, 16, 8)\r\n",
      "X_shape: (3, 16, 16, 8)\r\n",
      "_X shape: (8, 16, 16, 3)\r\n",
      "_out_grad shape: (14, 14, 3, 14)\r\n",
      "W_grad shape: (8, 3, 3, 14)\r\n",
      "W_grad shape: (3, 3, 8, 14)\r\n",
      "\u001b[31m\u001b[1m_ test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape9-W_shape9-2-0] _\u001b[0m\r\n",
      "\r\n",
      "Z_shape = (3, 16, 16, 2), W_shape = (3, 3, 2, 14), stride = 2, padding = 0\r\n",
      "backward = True, device = cuda()\r\n",
      "\r\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\r\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\r\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\r\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\r\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\r\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\r\n",
      "        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\r\n",
      "        _Z = _Z.astype(np.float32)\r\n",
      "        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\r\n",
      "        _W = _W.astype(np.float32)\r\n",
      "        Z = ndl.Tensor(_Z, device=device)\r\n",
      "        W = ndl.Tensor(_W, device=device)\r\n",
      "        y = ndl.conv(Z, W, padding=padding, stride=stride)\r\n",
      "        y2 = y.sum()\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      "            y2.backward()\r\n",
      "        Ztch = torch.Tensor(_Z).float()\r\n",
      "        Ztch.requires_grad=\u001b[94mTrue\u001b[39;49;00m\r\n",
      "        Wtch = torch.Tensor(_W).float()\r\n",
      "        Wtch.requires_grad=\u001b[94mTrue\u001b[39;49;00m\r\n",
      "        out = torch.nn.functional.conv2d(Ztch.permute(\u001b[94m0\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m), Wtch.permute(\u001b[94m3\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m), padding=padding, stride=stride)\r\n",
      "        out2 = out.sum()\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      "            out2.backward()\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      "            err1 = np.linalg.norm(Ztch.grad.numpy() - Z.grad.numpy())\r\n",
      "            err2 = np.linalg.norm(Wtch.grad.numpy() - W.grad.numpy())\r\n",
      "        err3 = np.linalg.norm(out2.detach().numpy() - y2.numpy())\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      ">           \u001b[94massert\u001b[39;49;00m err1 < \u001b[94m1e-2\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33minput grads match\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "\u001b[1m\u001b[31mE           AssertionError: input grads match\u001b[0m\r\n",
      "\u001b[1m\u001b[31mE           assert 42426.348 < 0.01\u001b[0m\r\n",
      "\r\n",
      "W          = needle.Tensor([[[[ -8.0968      -2.5552022    8.703147    -1.4674252    4.5861077\r\n",
      "     -0.28521433   4.383634    -9.13...727333   0.22582927   9.296732    -8.13161     -0.67411226\r\n",
      "     -2.9204676    1.675528   -12.187821     5.5746226 ]]]])\r\n",
      "W_shape    = (3, 3, 2, 14)\r\n",
      "Wtch       = tensor([[[[ -8.0968,  -2.5552,   8.7031,  -1.4674,   4.5861,  -0.2852,   4.3836,\r\n",
      "            -9.1346,  -2.0159,   4.74...0.2258,\r\n",
      "             9.2967,  -8.1316,  -0.6741,  -2.9205,   1.6755, -12.1878,   5.5746]]]],\r\n",
      "       requires_grad=True)\r\n",
      "Z          = needle.Tensor([[[[  8.820262     2.000786  ]\r\n",
      "   [  4.89369     11.204466  ]\r\n",
      "   [  9.33779     -4.8863893 ]\r\n",
      "   ...\r\n",
      "   [...   -5.112822  ]\r\n",
      "   ...\r\n",
      "   [ -7.922368     4.2222714 ]\r\n",
      "   [ -6.064339     1.4188478 ]\r\n",
      "   [ -1.4109794   -5.791016  ]]]])\r\n",
      "Z_shape    = (3, 16, 16, 2)\r\n",
      "Ztch       = tensor([[[[  8.8203,   2.0008],\r\n",
      "          [  4.8937,  11.2045],\r\n",
      "          [  9.3378,  -4.8864],\r\n",
      "          ...,\r\n",
      "       ...\r\n",
      "          [ -7.9224,   4.2223],\r\n",
      "          [ -6.0643,   1.4188],\r\n",
      "          [ -1.4110,  -5.7910]]]], requires_grad=True)\r\n",
      "_W         = array([[[[ -8.0968    ,  -2.5552022 ,   8.703147  ,  -1.4674252 ,\r\n",
      "            4.5861077 ,  -0.28521433,   4.383634  , ...        -8.13161   ,  -0.67411226,  -2.9204676 ,   1.675528  ,\r\n",
      "          -12.187821  ,   5.5746226 ]]]], dtype=float32)\r\n",
      "_Z         = array([[[[  8.820262  ,   2.000786  ],\r\n",
      "         [  4.89369   ,  11.204466  ],\r\n",
      "         [  9.33779   ,  -4.8863893 ],\r\n",
      " ...22368  ,   4.2222714 ],\r\n",
      "         [ -6.064339  ,   1.4188478 ],\r\n",
      "         [ -1.4109794 ,  -5.791016  ]]]], dtype=float32)\r\n",
      "backward   = True\r\n",
      "device     = cuda()\r\n",
      "err1       = 42426.348\r\n",
      "err2       = 1072.2002\r\n",
      "err3       = 0.005126953\r\n",
      "out        = tensor([[[[-3.4607e+02,  6.3506e+01, -3.3188e+01,  ...,  1.9525e+02,\r\n",
      "            1.2715e+02, -5.7875e+01],\r\n",
      "          [..., -1.1399e+02, -6.9067e+01,  ..., -1.4515e+02,\r\n",
      "            5.3236e+01,  3.2069e+00]]]], grad_fn=<ConvolutionBackward0>)\r\n",
      "out2       = tensor(-2297.1741, grad_fn=<SumBackward0>)\r\n",
      "padding    = 0\r\n",
      "stride     = 2\r\n",
      "torch      = <module 'torch' from '/home/ubuntu/anaconda3/envs/mlsys_hw/lib/python3.8/site-packages/torch/__init__.py'>\r\n",
      "y          = needle.Tensor([[[[-3.46071625e+02 -1.10818672e+00  4.20151154e+02 ... -5.85831528e+01\r\n",
      "    -6.61993790e+01  3.31586266e...3e+01]\r\n",
      "   [-9.30875702e+01 -4.32016563e+01  1.87175808e+01 ... -3.38230171e+01\r\n",
      "    -5.38563805e+01  3.20688462e+00]]]])\r\n",
      "y2         = needle.Tensor([-2297.1792])\r\n",
      "\r\n",
      "\u001b[1m\u001b[31mtests/test_conv.py\u001b[0m:446: AssertionError\r\n",
      "----------------------------- Captured stdout call -----------------------------\r\n",
      "out_grad shape: (3, 7, 7, 14)\r\n",
      "stride: 2\r\n",
      "_out_grad shape: (3, 14, 14, 14)\r\n",
      "W shape: (3, 3, 2, 14)\r\n",
      "_W shape: (3, 3, 14, 2)\r\n",
      "X_grad shape: (3, 16, 16, 2)\r\n",
      "X_shape: (3, 16, 16, 2)\r\n",
      "_X shape: (2, 16, 16, 3)\r\n",
      "_out_grad shape: (14, 14, 3, 14)\r\n",
      "W_grad shape: (2, 3, 3, 14)\r\n",
      "W_grad shape: (3, 3, 2, 14)\r\n",
      "\u001b[31m\u001b[1m_ test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape10-W_shape10-1-0] _\u001b[0m\r\n",
      "\r\n",
      "Z_shape = (3, 16, 16, 24), W_shape = (3, 3, 24, 14), stride = 1, padding = 0\r\n",
      "backward = True, device = cuda()\r\n",
      "\r\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\r\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\r\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\r\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\r\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\r\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\r\n",
      "        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\r\n",
      "        _Z = _Z.astype(np.float32)\r\n",
      "        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\r\n",
      "        _W = _W.astype(np.float32)\r\n",
      "        Z = ndl.Tensor(_Z, device=device)\r\n",
      "        W = ndl.Tensor(_W, device=device)\r\n",
      "        y = ndl.conv(Z, W, padding=padding, stride=stride)\r\n",
      "        y2 = y.sum()\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      "            y2.backward()\r\n",
      "        Ztch = torch.Tensor(_Z).float()\r\n",
      "        Ztch.requires_grad=\u001b[94mTrue\u001b[39;49;00m\r\n",
      "        Wtch = torch.Tensor(_W).float()\r\n",
      "        Wtch.requires_grad=\u001b[94mTrue\u001b[39;49;00m\r\n",
      "        out = torch.nn.functional.conv2d(Ztch.permute(\u001b[94m0\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m), Wtch.permute(\u001b[94m3\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m), padding=padding, stride=stride)\r\n",
      "        out2 = out.sum()\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      "            out2.backward()\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      "            err1 = np.linalg.norm(Ztch.grad.numpy() - Z.grad.numpy())\r\n",
      "            err2 = np.linalg.norm(Wtch.grad.numpy() - W.grad.numpy())\r\n",
      "        err3 = np.linalg.norm(out2.detach().numpy() - y2.numpy())\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      ">           \u001b[94massert\u001b[39;49;00m err1 < \u001b[94m1e-2\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33minput grads match\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "\u001b[1m\u001b[31mE           AssertionError: input grads match\u001b[0m\r\n",
      "\u001b[1m\u001b[31mE           assert 205625.61 < 0.01\u001b[0m\r\n",
      "\r\n",
      "W          = needle.Tensor([[[[ -1.3245817    1.0111231   -1.9236585  ...  -2.5907016\r\n",
      "     -0.36175704   4.6234684 ]\r\n",
      "   [  5.279258... -4.82095      4.98517   ]\r\n",
      "   [  3.2300415   -2.4161792   -3.420825   ...  -9.444658\r\n",
      "      8.537833     2.3818388 ]]]])\r\n",
      "W_shape    = (3, 3, 24, 14)\r\n",
      "Wtch       = tensor([[[[ -1.3246,   1.0111,  -1.9237,  ...,  -2.5907,  -0.3618,   4.6235],\r\n",
      "          [  5.2793,  -1.3090,   0.0500,...   4.9852],\r\n",
      "          [  3.2300,  -2.4162,  -3.4208,  ...,  -9.4447,   8.5378,   2.3818]]]],\r\n",
      "       requires_grad=True)\r\n",
      "Z          = needle.Tensor([[[[  8.820262     2.000786     4.89369    ...   3.2680929\r\n",
      "      4.322181    -3.7108252 ]\r\n",
      "   [ 11.348773...4.207389     8.935435  ]\r\n",
      "   [  2.5084004    3.1399443   -0.12958507 ...   0.13740699\r\n",
      "     -1.400853     0.07475674]]]])\r\n",
      "Z_shape    = (3, 16, 16, 24)\r\n",
      "Ztch       = tensor([[[[  8.8203,   2.0008,   4.8937,  ...,   3.2681,   4.3222,  -3.7108],\r\n",
      "          [ 11.3488,  -7.2718,   0.2288,...   8.9354],\r\n",
      "          [  2.5084,   3.1399,  -0.1296,  ...,   0.1374,  -1.4009,   0.0748]]]],\r\n",
      "       requires_grad=True)\r\n",
      "_W         = array([[[[ -1.3245817 ,   1.0111231 ,  -1.9236585 , ...,  -2.5907016 ,\r\n",
      "           -0.36175704,   4.6234684 ],\r\n",
      "        ... [  3.2300415 ,  -2.4161792 ,  -3.420825  , ...,  -9.444658  ,\r\n",
      "            8.537833  ,   2.3818388 ]]]], dtype=float32)\r\n",
      "_Z         = array([[[[  8.820262  ,   2.000786  ,   4.89369   , ...,   3.2680929 ,\r\n",
      "            4.322181  ,  -3.7108252 ],\r\n",
      "        ... [  2.5084004 ,   3.1399443 ,  -0.12958507, ...,   0.13740699,\r\n",
      "           -1.400853  ,   0.07475674]]]], dtype=float32)\r\n",
      "backward   = True\r\n",
      "device     = cuda()\r\n",
      "err1       = 205625.61\r\n",
      "err2       = 0.002582637\r\n",
      "err3       = 0.032226562\r\n",
      "out        = tensor([[[[ 5.9644e+02,  1.7622e+01, -3.8665e+02,  ..., -5.6437e+02,\r\n",
      "           -1.4383e+02, -1.2310e+02],\r\n",
      "          [..., -6.6977e+02,  7.6359e+00,  ...,  3.7344e+02,\r\n",
      "           -8.9822e+01,  1.5355e+02]]]], grad_fn=<ConvolutionBackward0>)\r\n",
      "out2       = tensor(-5941.3750, grad_fn=<SumBackward0>)\r\n",
      "padding    = 0\r\n",
      "stride     = 1\r\n",
      "torch      = <module 'torch' from '/home/ubuntu/anaconda3/envs/mlsys_hw/lib/python3.8/site-packages/torch/__init__.py'>\r\n",
      "y          = needle.Tensor([[[[ 5.96437805e+02 -2.17912796e+02 -2.09804783e+01 ... -4.44460907e+02\r\n",
      "    -2.74169159e+02 -6.12073181e...4e+01]\r\n",
      "   [-5.03057983e+02 -1.26949539e+02 -1.28574600e+01 ...  4.08393738e+02\r\n",
      "    -1.71167480e+02  1.53551315e+02]]]])\r\n",
      "y2         = needle.Tensor([-5941.407])\r\n",
      "\r\n",
      "\u001b[1m\u001b[31mtests/test_conv.py\u001b[0m:446: AssertionError\r\n",
      "----------------------------- Captured stdout call -----------------------------\r\n",
      "out_grad shape: (3, 14, 14, 14)\r\n",
      "stride: 1\r\n",
      "_out_grad shape: (3, 14, 14, 14)\r\n",
      "W shape: (3, 3, 24, 14)\r\n",
      "_W shape: (3, 3, 14, 24)\r\n",
      "X_grad shape: (3, 16, 16, 24)\r\n",
      "X_shape: (3, 16, 16, 24)\r\n",
      "_X shape: (24, 16, 16, 3)\r\n",
      "_out_grad shape: (14, 14, 3, 14)\r\n",
      "W_grad shape: (24, 3, 3, 14)\r\n",
      "W_grad shape: (3, 3, 24, 14)\r\n",
      "\u001b[31m\u001b[1m_ test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape11-W_shape11-1-0] _\u001b[0m\r\n",
      "\r\n",
      "Z_shape = (3, 14, 14, 8), W_shape = (5, 5, 8, 16), stride = 1, padding = 0\r\n",
      "backward = True, device = cuda()\r\n",
      "\r\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\r\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\r\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\r\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\r\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\r\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\r\n",
      "        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\r\n",
      "        _Z = _Z.astype(np.float32)\r\n",
      "        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\r\n",
      "        _W = _W.astype(np.float32)\r\n",
      "        Z = ndl.Tensor(_Z, device=device)\r\n",
      "        W = ndl.Tensor(_W, device=device)\r\n",
      "        y = ndl.conv(Z, W, padding=padding, stride=stride)\r\n",
      "        y2 = y.sum()\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      "            y2.backward()\r\n",
      "        Ztch = torch.Tensor(_Z).float()\r\n",
      "        Ztch.requires_grad=\u001b[94mTrue\u001b[39;49;00m\r\n",
      "        Wtch = torch.Tensor(_W).float()\r\n",
      "        Wtch.requires_grad=\u001b[94mTrue\u001b[39;49;00m\r\n",
      "        out = torch.nn.functional.conv2d(Ztch.permute(\u001b[94m0\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m), Wtch.permute(\u001b[94m3\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m), padding=padding, stride=stride)\r\n",
      "        out2 = out.sum()\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      "            out2.backward()\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      "            err1 = np.linalg.norm(Ztch.grad.numpy() - Z.grad.numpy())\r\n",
      "            err2 = np.linalg.norm(Wtch.grad.numpy() - W.grad.numpy())\r\n",
      "        err3 = np.linalg.norm(out2.detach().numpy() - y2.numpy())\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      ">           \u001b[94massert\u001b[39;49;00m err1 < \u001b[94m1e-2\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33minput grads match\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "\u001b[1m\u001b[31mE           AssertionError: input grads match\u001b[0m\r\n",
      "\u001b[1m\u001b[31mE           assert 223271.78 < 0.01\u001b[0m\r\n",
      "\r\n",
      "W          = needle.Tensor([[[[ 4.28266907e+00  8.13360405e+00 -4.62234831e+00 ...  2.07981968e+00\r\n",
      "     2.17041516e+00 -3.88039827e...7e+00]\r\n",
      "   [-7.86027253e-01  4.30947495e+00 -4.97804356e+00 ... -5.69430470e-01\r\n",
      "     4.27775383e-01 -3.84437203e+00]]]])\r\n",
      "W_shape    = (5, 5, 8, 16)\r\n",
      "Wtch       = tensor([[[[ 4.2827e+00,  8.1336e+00, -4.6223e+00,  ...,  2.0798e+00,\r\n",
      "            2.1704e+00, -3.8804e-01],\r\n",
      "          [...[-7.8603e-01,  4.3095e+00, -4.9780e+00,  ..., -5.6943e-01,\r\n",
      "            4.2778e-01, -3.8444e+00]]]], requires_grad=True)\r\n",
      "Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\r\n",
      "     4.75044203e+00 -7.56786048e...3e+00]\r\n",
      "   [ 1.09905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\r\n",
      "    -8.48226726e-01 -2.29984760e+00]]]])\r\n",
      "Z_shape    = (3, 14, 14, 8)\r\n",
      "Ztch       = tensor([[[[ 8.8203e+00,  2.0008e+00,  4.8937e+00,  ..., -4.8864e+00,\r\n",
      "            4.7504e+00, -7.5679e-01],\r\n",
      "          [...[ 1.0991e+01,  2.2317e+00,  4.6206e+00,  ..., -6.2309e-01,\r\n",
      "           -8.4823e-01, -2.2998e+00]]]], requires_grad=True)\r\n",
      "_W         = array([[[[ 4.28266907e+00,  8.13360405e+00, -4.62234831e+00, ...,\r\n",
      "           2.07981968e+00,  2.17041516e+00, -3.88039...947495e+00, -4.97804356e+00, ...,\r\n",
      "          -5.69430470e-01,  4.27775383e-01, -3.84437203e+00]]]],\r\n",
      "      dtype=float32)\r\n",
      "_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\r\n",
      "          -4.88638926e+00,  4.75044203e+00, -7.56786...166728e+00,  4.62055349e+00, ...,\r\n",
      "          -6.23090088e-01, -8.48226726e-01, -2.29984760e+00]]]],\r\n",
      "      dtype=float32)\r\n",
      "backward   = True\r\n",
      "device     = cuda()\r\n",
      "err1       = 223271.78\r\n",
      "err2       = 0.0019506718\r\n",
      "err3       = 0.0068359375\r\n",
      "out        = tensor([[[[ 1.7557e+02,  1.3950e+02,  4.5144e+02,  ...,  1.9506e+01,\r\n",
      "           -5.7378e+01, -2.2967e+02],\r\n",
      "          [...,  3.1106e+02,  4.1425e+02,  ...,  3.3462e+02,\r\n",
      "           -1.0345e+02,  2.4403e+02]]]], grad_fn=<ConvolutionBackward0>)\r\n",
      "out2       = tensor(-10705.7383, grad_fn=<SumBackward0>)\r\n",
      "padding    = 0\r\n",
      "stride     = 1\r\n",
      "torch      = <module 'torch' from '/home/ubuntu/anaconda3/envs/mlsys_hw/lib/python3.8/site-packages/torch/__init__.py'>\r\n",
      "y          = needle.Tensor([[[[ 1.75570267e+02  2.39688141e+02  5.25702209e+02 ...  4.85040955e+02\r\n",
      "    -2.37886475e+02 -2.36008549e...4e+02]\r\n",
      "   [-4.85305603e+02 -1.46666489e+02 -3.62995392e+02 ...  1.50065781e+02\r\n",
      "    -5.45707825e+02  2.44026398e+02]]]])\r\n",
      "y2         = needle.Tensor([-10705.731])\r\n",
      "\r\n",
      "\u001b[1m\u001b[31mtests/test_conv.py\u001b[0m:446: AssertionError\r\n",
      "----------------------------- Captured stdout call -----------------------------\r\n",
      "out_grad shape: (3, 10, 10, 16)\r\n",
      "stride: 1\r\n",
      "_out_grad shape: (3, 10, 10, 16)\r\n",
      "W shape: (5, 5, 8, 16)\r\n",
      "_W shape: (5, 5, 16, 8)\r\n",
      "X_grad shape: (3, 14, 14, 8)\r\n",
      "X_shape: (3, 14, 14, 8)\r\n",
      "_X shape: (8, 14, 14, 3)\r\n",
      "_out_grad shape: (10, 10, 3, 16)\r\n",
      "W_grad shape: (8, 5, 5, 16)\r\n",
      "W_grad shape: (5, 5, 8, 16)\r\n",
      "\u001b[31m\u001b[1m_ test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape12-W_shape12-1-0] _\u001b[0m\r\n",
      "\r\n",
      "Z_shape = (3, 17, 17, 8), W_shape = (5, 5, 8, 16), stride = 1, padding = 0\r\n",
      "backward = True, device = cuda()\r\n",
      "\r\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\r\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\r\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\r\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\r\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\r\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\r\n",
      "        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\r\n",
      "        _Z = _Z.astype(np.float32)\r\n",
      "        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\r\n",
      "        _W = _W.astype(np.float32)\r\n",
      "        Z = ndl.Tensor(_Z, device=device)\r\n",
      "        W = ndl.Tensor(_W, device=device)\r\n",
      "        y = ndl.conv(Z, W, padding=padding, stride=stride)\r\n",
      "        y2 = y.sum()\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      "            y2.backward()\r\n",
      "        Ztch = torch.Tensor(_Z).float()\r\n",
      "        Ztch.requires_grad=\u001b[94mTrue\u001b[39;49;00m\r\n",
      "        Wtch = torch.Tensor(_W).float()\r\n",
      "        Wtch.requires_grad=\u001b[94mTrue\u001b[39;49;00m\r\n",
      "        out = torch.nn.functional.conv2d(Ztch.permute(\u001b[94m0\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m), Wtch.permute(\u001b[94m3\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m), padding=padding, stride=stride)\r\n",
      "        out2 = out.sum()\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      "            out2.backward()\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      "            err1 = np.linalg.norm(Ztch.grad.numpy() - Z.grad.numpy())\r\n",
      "            err2 = np.linalg.norm(Wtch.grad.numpy() - W.grad.numpy())\r\n",
      "        err3 = np.linalg.norm(out2.detach().numpy() - y2.numpy())\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      ">           \u001b[94massert\u001b[39;49;00m err1 < \u001b[94m1e-2\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33minput grads match\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "\u001b[1m\u001b[31mE           AssertionError: input grads match\u001b[0m\r\n",
      "\u001b[1m\u001b[31mE           assert 169034.64 < 0.01\u001b[0m\r\n",
      "\r\n",
      "W          = needle.Tensor([[[[ 5.93117094e+00 -4.61600447e+00 -7.43411541e+00 ...  9.49053860e+00\r\n",
      "     6.47611380e+00 -3.38525438e...7e+00]\r\n",
      "   [-3.93575430e+00 -6.98609781e+00  1.63659811e+00 ...  8.22474575e+00\r\n",
      "     2.56555057e+00  1.63004756e+00]]]])\r\n",
      "W_shape    = (5, 5, 8, 16)\r\n",
      "Wtch       = tensor([[[[ 5.9312e+00, -4.6160e+00, -7.4341e+00,  ...,  9.4905e+00,\r\n",
      "            6.4761e+00, -3.3853e+00],\r\n",
      "          [...[-3.9358e+00, -6.9861e+00,  1.6366e+00,  ...,  8.2247e+00,\r\n",
      "            2.5656e+00,  1.6300e+00]]]], requires_grad=True)\r\n",
      "Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\r\n",
      "     4.75044203e+00 -7.56786048e...8e+00]\r\n",
      "   [ 2.80155873e+00  3.11298299e+00 -3.24756050e+00 ... -7.96233535e-01\r\n",
      "     4.72636795e+00 -5.75466204e+00]]]])\r\n",
      "Z_shape    = (3, 17, 17, 8)\r\n",
      "Ztch       = tensor([[[[ 8.8203e+00,  2.0008e+00,  4.8937e+00,  ..., -4.8864e+00,\r\n",
      "            4.7504e+00, -7.5679e-01],\r\n",
      "          [...[ 2.8016e+00,  3.1130e+00, -3.2476e+00,  ..., -7.9623e-01,\r\n",
      "            4.7264e+00, -5.7547e+00]]]], requires_grad=True)\r\n",
      "_W         = array([[[[ 5.93117094e+00, -4.61600447e+00, -7.43411541e+00, ...,\r\n",
      "           9.49053860e+00,  6.47611380e+00, -3.38525...609781e+00,  1.63659811e+00, ...,\r\n",
      "           8.22474575e+00,  2.56555057e+00,  1.63004756e+00]]]],\r\n",
      "      dtype=float32)\r\n",
      "_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\r\n",
      "          -4.88638926e+00,  4.75044203e+00, -7.56786...298299e+00, -3.24756050e+00, ...,\r\n",
      "          -7.96233535e-01,  4.72636795e+00, -5.75466204e+00]]]],\r\n",
      "      dtype=float32)\r\n",
      "backward   = True\r\n",
      "device     = cuda()\r\n",
      "err1       = 169034.64\r\n",
      "err2       = 0.0029745598\r\n",
      "err3       = 0.0703125\r\n",
      "out        = tensor([[[[ 5.8605e+02,  6.5851e+02,  3.1471e+01,  ..., -2.0572e+02,\r\n",
      "           -5.6982e+02, -7.7784e+02],\r\n",
      "          [...,  1.3106e+02, -3.2224e+02,  ...,  7.1755e+01,\r\n",
      "           -5.2006e+02, -6.5310e+01]]]], grad_fn=<ConvolutionBackward0>)\r\n",
      "out2       = tensor(50567.7344, grad_fn=<SumBackward0>)\r\n",
      "padding    = 0\r\n",
      "stride     = 1\r\n",
      "torch      = <module 'torch' from '/home/ubuntu/anaconda3/envs/mlsys_hw/lib/python3.8/site-packages/torch/__init__.py'>\r\n",
      "y          = needle.Tensor([[[[ 5.86050903e+02 -5.11887695e+02  5.65467911e+01 ...  7.48446426e+01\r\n",
      "    -3.56891510e+02 -3.76215637e...2e+02]\r\n",
      "   [-6.95190125e+02 -8.10500183e+01  2.38304077e+02 ... -1.27415886e+02\r\n",
      "     8.60565063e+02 -6.53100510e+01]]]])\r\n",
      "y2         = needle.Tensor([50567.664])\r\n",
      "\r\n",
      "\u001b[1m\u001b[31mtests/test_conv.py\u001b[0m:446: AssertionError\r\n",
      "----------------------------- Captured stdout call -----------------------------\r\n",
      "out_grad shape: (3, 13, 13, 16)\r\n",
      "stride: 1\r\n",
      "_out_grad shape: (3, 13, 13, 16)\r\n",
      "W shape: (5, 5, 8, 16)\r\n",
      "_W shape: (5, 5, 16, 8)\r\n",
      "X_grad shape: (3, 17, 17, 8)\r\n",
      "X_shape: (3, 17, 17, 8)\r\n",
      "_X shape: (8, 17, 17, 3)\r\n",
      "_out_grad shape: (13, 13, 3, 16)\r\n",
      "W_grad shape: (8, 5, 5, 16)\r\n",
      "W_grad shape: (5, 5, 8, 16)\r\n",
      "\u001b[31m\u001b[1m_ test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape13-W_shape13-1-0] _\u001b[0m\r\n",
      "\r\n",
      "Z_shape = (3, 17, 17, 1), W_shape = (5, 5, 1, 16), stride = 1, padding = 0\r\n",
      "backward = True, device = cuda()\r\n",
      "\r\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\r\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\r\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\r\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\r\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\r\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\r\n",
      "        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\r\n",
      "        _Z = _Z.astype(np.float32)\r\n",
      "        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\r\n",
      "        _W = _W.astype(np.float32)\r\n",
      "        Z = ndl.Tensor(_Z, device=device)\r\n",
      "        W = ndl.Tensor(_W, device=device)\r\n",
      "        y = ndl.conv(Z, W, padding=padding, stride=stride)\r\n",
      "        y2 = y.sum()\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      "            y2.backward()\r\n",
      "        Ztch = torch.Tensor(_Z).float()\r\n",
      "        Ztch.requires_grad=\u001b[94mTrue\u001b[39;49;00m\r\n",
      "        Wtch = torch.Tensor(_W).float()\r\n",
      "        Wtch.requires_grad=\u001b[94mTrue\u001b[39;49;00m\r\n",
      "        out = torch.nn.functional.conv2d(Ztch.permute(\u001b[94m0\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m), Wtch.permute(\u001b[94m3\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m), padding=padding, stride=stride)\r\n",
      "        out2 = out.sum()\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      "            out2.backward()\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      "            err1 = np.linalg.norm(Ztch.grad.numpy() - Z.grad.numpy())\r\n",
      "            err2 = np.linalg.norm(Wtch.grad.numpy() - W.grad.numpy())\r\n",
      "        err3 = np.linalg.norm(out2.detach().numpy() - y2.numpy())\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      ">           \u001b[94massert\u001b[39;49;00m err1 < \u001b[94m1e-2\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33minput grads match\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "\u001b[1m\u001b[31mE           AssertionError: input grads match\u001b[0m\r\n",
      "\u001b[1m\u001b[31mE           assert 27556150.0 < 0.01\u001b[0m\r\n",
      "\r\n",
      "W          = needle.Tensor([[[[  1.5437562   -6.8538       4.3282647    5.4068804   -3.15688\r\n",
      "     -1.206689    -4.3909516    3.4969...448   -2.242325     0.6578698\r\n",
      "     -7.0278      -1.7489109   10.11736      2.5269346    1.7962458\r\n",
      "     -7.9124722 ]]]])\r\n",
      "W_shape    = (5, 5, 1, 16)\r\n",
      "Wtch       = tensor([[[[  1.5438,  -6.8538,   4.3283,   5.4069,  -3.1569,  -1.2067,  -4.3910,\r\n",
      "             3.4969,  -5.3061,  -1.11...461,  -2.2423,   0.6579,  -7.0278,  -1.7489,  10.1174,   2.5269,\r\n",
      "             1.7962,  -7.9125]]]], requires_grad=True)\r\n",
      "Z          = needle.Tensor([[[[ 8.82026196e+00]\r\n",
      "   [ 2.00078607e+00]\r\n",
      "   [ 4.89369011e+00]\r\n",
      "   [ 1.12044659e+01]\r\n",
      "   [ 9.33778954e+00]...22066e-01]\r\n",
      "   [ 1.05310105e-01]\r\n",
      "   [ 4.97272283e-01]\r\n",
      "   [ 1.13696384e+00]\r\n",
      "   [-5.08369303e+00]\r\n",
      "   [-5.73876619e-01]]]])\r\n",
      "Z_shape    = (3, 17, 17, 1)\r\n",
      "Ztch       = tensor([[[[ 8.8203e+00],\r\n",
      "          [ 2.0008e+00],\r\n",
      "          [ 4.8937e+00],\r\n",
      "          [ 1.1204e+01],\r\n",
      "          [ 9.3378...       [ 4.9727e-01],\r\n",
      "          [ 1.1370e+00],\r\n",
      "          [-5.0837e+00],\r\n",
      "          [-5.7388e-01]]]], requires_grad=True)\r\n",
      "_W         = array([[[[  1.5437562 ,  -6.8538    ,   4.3282647 ,   5.4068804 ,\r\n",
      "           -3.15688   ,  -1.206689  ,  -4.3909516 , ...  -7.0278    ,  -1.7489109 ,\r\n",
      "           10.11736   ,   2.5269346 ,   1.7962458 ,  -7.9124722 ]]]],\r\n",
      "      dtype=float32)\r\n",
      "_Z         = array([[[[ 8.82026196e+00],\r\n",
      "         [ 2.00078607e+00],\r\n",
      "         [ 4.89369011e+00],\r\n",
      "         [ 1.12044659e+01],\r\n",
      "      ... 4.97272283e-01],\r\n",
      "         [ 1.13696384e+00],\r\n",
      "         [-5.08369303e+00],\r\n",
      "         [-5.73876619e-01]]]], dtype=float32)\r\n",
      "backward   = True\r\n",
      "device     = cuda()\r\n",
      "err1       = 27556150.0\r\n",
      "err2       = 0.0012842789\r\n",
      "err3       = 0.029296875\r\n",
      "out        = tensor([[[[ 4.1444e+01,  7.2539e+01, -1.2076e+02,  ...,  5.7938e+01,\r\n",
      "           -1.5660e+01, -6.3388e+00],\r\n",
      "          [...,  1.4962e+02,  3.9613e+01,  ...,  6.5264e+01,\r\n",
      "            1.5128e+01, -1.2588e+02]]]], grad_fn=<ConvolutionBackward0>)\r\n",
      "out2       = tensor(-27459.7266, grad_fn=<SumBackward0>)\r\n",
      "padding    = 0\r\n",
      "stride     = 1\r\n",
      "torch      = <module 'torch' from '/home/ubuntu/anaconda3/envs/mlsys_hw/lib/python3.8/site-packages/torch/__init__.py'>\r\n",
      "y          = needle.Tensor([[[[ 4.14443283e+01  1.16950035e+01 -4.72139893e+01 ...  1.41469421e+02\r\n",
      "    -2.21567459e+01 -3.79012985e...8e+01]\r\n",
      "   [ 6.41885986e+01  7.46171875e+01  3.42209511e+01 ...  1.39405457e+02\r\n",
      "    -1.01194525e+01 -1.25883507e+02]]]])\r\n",
      "y2         = needle.Tensor([-27459.697])\r\n",
      "\r\n",
      "\u001b[1m\u001b[31mtests/test_conv.py\u001b[0m:446: AssertionError\r\n",
      "----------------------------- Captured stdout call -----------------------------\r\n",
      "out_grad shape: (3, 13, 13, 16)\r\n",
      "stride: 1\r\n",
      "_out_grad shape: (3, 13, 13, 16)\r\n",
      "W shape: (5, 5, 1, 16)\r\n",
      "_W shape: (5, 5, 16, 1)\r\n",
      "X_grad shape: (3, 17, 17, 1)\r\n",
      "X_shape: (3, 17, 17, 1)\r\n",
      "_X shape: (1, 17, 17, 3)\r\n",
      "_out_grad shape: (13, 13, 3, 16)\r\n",
      "W_grad shape: (1, 5, 5, 16)\r\n",
      "W_grad shape: (5, 5, 1, 16)\r\n",
      "\u001b[31m\u001b[1m_ test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape16-W_shape16-1-0] _\u001b[0m\r\n",
      "\r\n",
      "Z_shape = (1, 14, 14, 2), W_shape = (3, 3, 2, 2), stride = 1, padding = 0\r\n",
      "backward = True, device = cuda()\r\n",
      "\r\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\r\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\r\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\r\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\r\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\r\n",
      "        \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96mtorch\u001b[39;49;00m\r\n",
      "        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\r\n",
      "        _Z = _Z.astype(np.float32)\r\n",
      "        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\r\n",
      "        _W = _W.astype(np.float32)\r\n",
      "        Z = ndl.Tensor(_Z, device=device)\r\n",
      "        W = ndl.Tensor(_W, device=device)\r\n",
      "        y = ndl.conv(Z, W, padding=padding, stride=stride)\r\n",
      "        y2 = y.sum()\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      "            y2.backward()\r\n",
      "        Ztch = torch.Tensor(_Z).float()\r\n",
      "        Ztch.requires_grad=\u001b[94mTrue\u001b[39;49;00m\r\n",
      "        Wtch = torch.Tensor(_W).float()\r\n",
      "        Wtch.requires_grad=\u001b[94mTrue\u001b[39;49;00m\r\n",
      "        out = torch.nn.functional.conv2d(Ztch.permute(\u001b[94m0\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m), Wtch.permute(\u001b[94m3\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m), padding=padding, stride=stride)\r\n",
      "        out2 = out.sum()\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      "            out2.backward()\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      "            err1 = np.linalg.norm(Ztch.grad.numpy() - Z.grad.numpy())\r\n",
      "            err2 = np.linalg.norm(Wtch.grad.numpy() - W.grad.numpy())\r\n",
      "        err3 = np.linalg.norm(out2.detach().numpy() - y2.numpy())\r\n",
      "        \u001b[94mif\u001b[39;49;00m backward:\r\n",
      ">           \u001b[94massert\u001b[39;49;00m err1 < \u001b[94m1e-2\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33minput grads match\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "\u001b[1m\u001b[31mE           AssertionError: input grads match\u001b[0m\r\n",
      "\u001b[1m\u001b[31mE           assert 319.2101 < 0.01\u001b[0m\r\n",
      "\r\n",
      "W          = needle.Tensor([[[[ -1.7671587   -8.082371  ]\r\n",
      "   [ -1.4591868   -3.807461  ]]\r\n",
      "\r\n",
      "  [[  4.2896194    5.705509  ]\r\n",
      "   [  7.3...7526     2.911123  ]\r\n",
      "   [-10.473016     0.61860955]]\r\n",
      "\r\n",
      "  [[ -0.65053475   0.46976614]\r\n",
      "   [  4.7152305  -13.698386  ]]]])\r\n",
      "W_shape    = (3, 3, 2, 2)\r\n",
      "Wtch       = tensor([[[[ -1.7672,  -8.0824],\r\n",
      "          [ -1.4592,  -3.8075]],\r\n",
      "\r\n",
      "         [[  4.2896,   5.7055],\r\n",
      "          [  7.3329,...         [-10.4730,   0.6186]],\r\n",
      "\r\n",
      "         [[ -0.6505,   0.4698],\r\n",
      "          [  4.7152, -13.6984]]]], requires_grad=True)\r\n",
      "Z          = needle.Tensor([[[[  8.820262     2.000786  ]\r\n",
      "   [  4.89369     11.204466  ]\r\n",
      "   [  9.33779     -4.8863893 ]\r\n",
      "   [  4.750...19315276  -8.283575  ]\r\n",
      "   [ -4.9275537   -7.359175  ]\r\n",
      "   [  8.240675     0.8211388 ]\r\n",
      "   [  2.8364513   -1.1133755 ]]]])\r\n",
      "Z_shape    = (1, 14, 14, 2)\r\n",
      "Ztch       = tensor([[[[  8.8203,   2.0008],\r\n",
      "          [  4.8937,  11.2045],\r\n",
      "          [  9.3378,  -4.8864],\r\n",
      "          [  4.7504,  ...\r\n",
      "          [ -4.9276,  -7.3592],\r\n",
      "          [  8.2407,   0.8211],\r\n",
      "          [  2.8365,  -1.1134]]]], requires_grad=True)\r\n",
      "_W         = array([[[[ -1.7671587 ,  -8.082371  ],\r\n",
      "         [ -1.4591868 ,  -3.807461  ]],\r\n",
      "\r\n",
      "        [[  4.2896194 ,   5.705509  ],...016  ,   0.61860955]],\r\n",
      "\r\n",
      "        [[ -0.65053475,   0.46976614],\r\n",
      "         [  4.7152305 , -13.698386  ]]]], dtype=float32)\r\n",
      "_Z         = array([[[[  8.820262  ,   2.000786  ],\r\n",
      "         [  4.89369   ,  11.204466  ],\r\n",
      "         [  9.33779   ,  -4.8863893 ],\r\n",
      " ...275537 ,  -7.359175  ],\r\n",
      "         [  8.240675  ,   0.8211388 ],\r\n",
      "         [  2.8364513 ,  -1.1133755 ]]]], dtype=float32)\r\n",
      "backward   = True\r\n",
      "device     = cuda()\r\n",
      "err1       = 319.2101\r\n",
      "err2       = 0.0\r\n",
      "err3       = 0.0017089844\r\n",
      "out        = tensor([[[[ -83.1438,   -6.3602,   93.8760,  -41.2035,  -96.8909,  152.8543,\r\n",
      "           -101.4654,  259.1811,    7.015...            -60.5771,   13.6480,  228.9975,    8.4413, -121.9792,   26.1601]]]],\r\n",
      "       grad_fn=<ConvolutionBackward0>)\r\n",
      "out2       = tensor(2243.9304, grad_fn=<SumBackward0>)\r\n",
      "padding    = 0\r\n",
      "stride     = 1\r\n",
      "torch      = <module 'torch' from '/home/ubuntu/anaconda3/envs/mlsys_hw/lib/python3.8/site-packages/torch/__init__.py'>\r\n",
      "y          = needle.Tensor([[[[ -83.14382     -46.58788   ]\r\n",
      "   [  -6.3601565    -5.1046524 ]\r\n",
      "   [  93.87601     -20.797268  ]\r\n",
      "   [ ...   228.99751   ]\r\n",
      "   [  41.437016      8.441346  ]\r\n",
      "   [ -26.093277   -121.97922   ]\r\n",
      "   [  22.205511     26.16009   ]]]])\r\n",
      "y2         = needle.Tensor([2243.9321])\r\n",
      "\r\n",
      "\u001b[1m\u001b[31mtests/test_conv.py\u001b[0m:446: AssertionError\r\n",
      "----------------------------- Captured stdout call -----------------------------\r\n",
      "out_grad shape: (1, 12, 12, 2)\r\n",
      "stride: 1\r\n",
      "_out_grad shape: (1, 12, 12, 2)\r\n",
      "W shape: (3, 3, 2, 2)\r\n",
      "_W shape: (3, 3, 2, 2)\r\n",
      "X_grad shape: (1, 14, 14, 2)\r\n",
      "X_shape: (1, 14, 14, 2)\r\n",
      "_X shape: (2, 14, 14, 1)\r\n",
      "_out_grad shape: (12, 12, 1, 2)\r\n",
      "W_grad shape: (2, 3, 3, 2)\r\n",
      "W_grad shape: (3, 3, 2, 2)\r\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\r\n",
      "\u001b[31mFAILED\u001b[0m tests/test_conv.py::\u001b[1mtest_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape0-W_shape0-1-0]\u001b[0m - AssertionError: input grads match\r\n",
      "\u001b[31mFAILED\u001b[0m tests/test_conv.py::\u001b[1mtest_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape1-W_shape1-1-1]\u001b[0m - AssertionError: input grads match\r\n",
      "\u001b[31mFAILED\u001b[0m tests/test_conv.py::\u001b[1mtest_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape2-W_shape2-1-2]\u001b[0m - AssertionError: input grads match\r\n",
      "\u001b[31mFAILED\u001b[0m tests/test_conv.py::\u001b[1mtest_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape3-W_shape3-1-0]\u001b[0m - AssertionError: input grads match\r\n",
      "\u001b[31mFAILED\u001b[0m tests/test_conv.py::\u001b[1mtest_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape4-W_shape4-1-0]\u001b[0m - AssertionError: input grads match\r\n",
      "\u001b[31mFAILED\u001b[0m tests/test_conv.py::\u001b[1mtest_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape5-W_shape5-2-0]\u001b[0m - AssertionError: input grads match\r\n",
      "\u001b[31mFAILED\u001b[0m tests/test_conv.py::\u001b[1mtest_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape6-W_shape6-2-1]\u001b[0m - AssertionError: input grads match\r\n",
      "\u001b[31mFAILED\u001b[0m tests/test_conv.py::\u001b[1mtest_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape7-W_shape7-2-2]\u001b[0m - AssertionError: input grads match\r\n",
      "\u001b[31mFAILED\u001b[0m tests/test_conv.py::\u001b[1mtest_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape8-W_shape8-2-0]\u001b[0m - AssertionError: input grads match\r\n",
      "\u001b[31mFAILED\u001b[0m tests/test_conv.py::\u001b[1mtest_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape9-W_shape9-2-0]\u001b[0m - AssertionError: input grads match\r\n",
      "\u001b[31mFAILED\u001b[0m tests/test_conv.py::\u001b[1mtest_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape10-W_shape10-1-0]\u001b[0m - AssertionError: input grads match\r\n",
      "\u001b[31mFAILED\u001b[0m tests/test_conv.py::\u001b[1mtest_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape11-W_shape11-1-0]\u001b[0m - AssertionError: input grads match\r\n",
      "\u001b[31mFAILED\u001b[0m tests/test_conv.py::\u001b[1mtest_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape12-W_shape12-1-0]\u001b[0m - AssertionError: input grads match\r\n",
      "\u001b[31mFAILED\u001b[0m tests/test_conv.py::\u001b[1mtest_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape13-W_shape13-1-0]\u001b[0m - AssertionError: input grads match\r\n",
      "\u001b[31mFAILED\u001b[0m tests/test_conv.py::\u001b[1mtest_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape16-W_shape16-1-0]\u001b[0m - AssertionError: input grads match\r\n",
      "\u001b[31mFAILED\u001b[0m tests/test_conv.py::\u001b[1mtest_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape0-W_shape0-1-0]\u001b[0m - AssertionError: input grads match\r\n",
      "\u001b[31mFAILED\u001b[0m tests/test_conv.py::\u001b[1mtest_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape1-W_shape1-1-1]\u001b[0m - AssertionError: input grads match\r\n",
      "\u001b[31mFAILED\u001b[0m tests/test_conv.py::\u001b[1mtest_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape2-W_shape2-1-2]\u001b[0m - AssertionError: input grads match\r\n",
      "\u001b[31mFAILED\u001b[0m tests/test_conv.py::\u001b[1mtest_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape3-W_shape3-1-0]\u001b[0m - AssertionError: input grads match\r\n",
      "\u001b[31mFAILED\u001b[0m tests/test_conv.py::\u001b[1mtest_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape4-W_shape4-1-0]\u001b[0m - AssertionError: input grads match\r\n",
      "\u001b[31mFAILED\u001b[0m tests/test_conv.py::\u001b[1mtest_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape5-W_shape5-2-0]\u001b[0m - AssertionError: input grads match\r\n",
      "\u001b[31mFAILED\u001b[0m tests/test_conv.py::\u001b[1mtest_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape6-W_shape6-2-1]\u001b[0m - AssertionError: input grads match\r\n",
      "\u001b[31mFAILED\u001b[0m tests/test_conv.py::\u001b[1mtest_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape7-W_shape7-2-2]\u001b[0m - AssertionError: input grads match\r\n",
      "\u001b[31mFAILED\u001b[0m tests/test_conv.py::\u001b[1mtest_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape8-W_shape8-2-0]\u001b[0m - AssertionError: input grads match\r\n",
      "\u001b[31mFAILED\u001b[0m tests/test_conv.py::\u001b[1mtest_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape9-W_shape9-2-0]\u001b[0m - AssertionError: input grads match\r\n",
      "\u001b[31mFAILED\u001b[0m tests/test_conv.py::\u001b[1mtest_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape10-W_shape10-1-0]\u001b[0m - AssertionError: input grads match\r\n",
      "\u001b[31mFAILED\u001b[0m tests/test_conv.py::\u001b[1mtest_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape11-W_shape11-1-0]\u001b[0m - AssertionError: input grads match\r\n",
      "\u001b[31mFAILED\u001b[0m tests/test_conv.py::\u001b[1mtest_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape12-W_shape12-1-0]\u001b[0m - AssertionError: input grads match\r\n",
      "\u001b[31mFAILED\u001b[0m tests/test_conv.py::\u001b[1mtest_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape13-W_shape13-1-0]\u001b[0m - AssertionError: input grads match\r\n",
      "\u001b[31mFAILED\u001b[0m tests/test_conv.py::\u001b[1mtest_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape16-W_shape16-1-0]\u001b[0m - AssertionError: input grads match\r\n",
      "\u001b[31m================ \u001b[31m\u001b[1m30 failed\u001b[0m, \u001b[32m4 passed\u001b[0m, \u001b[33m1769 deselected\u001b[0m\u001b[31m in 3.68s\u001b[0m\u001b[31m =================\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pytest -l -v -k \"op_conv and backward\"\n",
    "# !python3 -m pytest -l -v -k \"test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape0-W_shape0-1-0]\"\n",
    "# !python3 -m pytest -l -v -k \"test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape0-W_shape0-1-0]\"\n",
    "# !python3 -m pytest -l -v -k \"test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape1-W_shape1-1-1]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Conv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Fixing init._calculate_fans for convolution\n",
    "Previously, we have implemented Kaiming uniform/normal initializations, where we essentially assigned `fan_in = input_size` and `fan_out = output_size`.\n",
    "For convolution, this becomes somewhat more detailed, in that you should multiply both of these by the \"receptive field size\", which is in this case just the product of the kernel sizes -- which in our case are always going to be the same, i.e., $k\\times k$ kernels.\n",
    "\n",
    "**You will need to edit your `kaiming_uniform`, etc. init functions to support multidimensional arrays.** In particular, you should add a new `shape` argument which is then passed to, e.g., the underlying `rand` function.\n",
    "\n",
    "You can test this below; though it is not _directly_ graded, it must match ours to pass the nn.Conv mugrade tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -l -v -k \"kaiming_uniform\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing nn.Conv\n",
    "\n",
    "Essentially, nn.Conv is just a wrapper of the convolution operator we previously implemented\n",
    "which adds a bias term, initializes the weight and bias, and ensures that the padding is set so that the input and output dimensions are the same (in the `stride=1` case, anyways). \n",
    "\n",
    "Importantly, nn.Conv should support NCHW format instead of NHWC format. In particular, we think this makes more sense given our current BatchNorm implementation. You can implement this by applying `transpose` twice to both the input and output.  \n",
    "\n",
    "- Ensure nn.Conv works for (N, C, H, W) tensors even though we implemented the conv op for (N, H, W, C) tensors\n",
    "- Initialize the (k, k, i, o) weight tensor using Kaiming uniform initialization with default settings\n",
    "- Initialize the (o,) bias tensor using uniform initialization on the interval $\\pm$`1.0/(in_channels * kernel_size**2)**0.5`\n",
    "- Calculate the appropriate padding to ensure input and output dimensions are the same\n",
    "- Calculate the convolution, then add the properly-broadcasted bias term if present\n",
    "\n",
    "You can now test your nn.Conv against PyTorch's nn.Conv2d with the two PyTest calls below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -l -v -k \"nn_conv_forward\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -l -v -k \"nn_conv_backward\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit nn.Conv to mugrade [20 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit \"YOUR KEY HERE\" -k \"conv_forward\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit \"YOUR KEY HERE\" -k \"conv_backward\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Implementing \"ResNet9\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now use your convolutional layer to implement a model similar to _ResNet9_, which is known to be a reasonable model for getting good accuracy on CIFAR-10 quickly (see [here](https://github.com/davidcpage/cifar10-fast)). Our main change is that we used striding instead of pooling and divided all of the channels by 4 for the sake of performance (as our framework is not as well-optimized as industry-grade frameworks).\n",
    "\n",
    "In the figure below, before the linear layer, you should \"flatten\" the tensor. We have added a module called `Flatten` in `nn.py` that you can complete and use, or you can simply use `.reshape` in the `forward()` method of your ResNet9.\n",
    "\n",
    "Make sure that you pass the device to all modules in your model; otherwise, you will get errors about mismatched devices when trying to run with CUDA.\n",
    "\n",
    "<center><img src=\"https://github.com/dlsyscourse/hw4/blob/main/ResNet9.png?raw=true\" alt=\"ResNet9\" style=\"width: 400px;\" /></center>\n",
    "\n",
    "We have tried to make it easier to pass the tests here than for previous assignments where you have implemented models. In particular, we are just going to make sure it has the right number of parameters and similar accuracy and loss after 1 or 2 batches of CIFAR-10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python3 -m pytest -l -v -k \"resnet9\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit ResNet9 to mugrade [10 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit \"YOUR KEY HERE\" -k \"resnet9\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can train your model on CIFAR-10 using the following code. Note that this is likely going to be quite slow, and also  not all that accurate due to the lack of data augmentation. You should expect it to take around 500s per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./python')\n",
    "sys.path.append('./apps')\n",
    "import needle as ndl\n",
    "from models import ResNet9\n",
    "from simple_training import train_cifar10, evaluate_cifar10\n",
    "\n",
    "device = ndl.cpu()\n",
    "dataset = ndl.data.CIFAR10Dataset(\"data/cifar-10-batches-py\", train=True)\n",
    "dataloader = ndl.data.DataLoader(\\\n",
    "         dataset=dataset,\n",
    "         batch_size=128,\n",
    "         shuffle=True,\n",
    "         collate_fn=ndl.data.collate_ndarray,\n",
    "         device=device,\n",
    "         dtype=\"float32\")\n",
    "model = ResNet9(device=device, dtype=\"float32\")\n",
    "train_cifar10(model, dataloader, n_epochs=10, optimizer=ndl.optim.Adam,\n",
    "      lr=0.001, weight_decay=0.001)\n",
    "evaluate_cifar10(model, dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Recurrent neural network [10 points]\n",
    "\n",
    "**Note:** In the following sections, you may find yourself wanting to index into tensors, i.e., to use getitem or setitem. However, we have not implemented these for tensors in our library; instead, you should use `stack` and `split` operations.\n",
    "\n",
    "In `python/needle/nn.py`, implement `RNNCell`.\n",
    "\n",
    "$h^\\prime = \\text{tanh}(xW_{ih} + b_{ih} + hW_{hh} + b_{hh})$. If nonlinearity is 'relu', then ReLU is used in place of tanh.\n",
    "\n",
    "All weights and biases should be initialized from $\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})$ where $k=\\frac{1}{\\text{hidden_size}}$.\n",
    "\n",
    "In `python/needle/nn.py`, implement `RNN`.\n",
    "\n",
    "For each element in the input sequence, each layer computes the following function:\n",
    "\n",
    "$h_t = \\text{tanh}(x_tW_{ih} + b_{ih} + h_{(t-1)}W_{hh} + b_{hh})$\n",
    "\n",
    "where $h_t$ is the hidden state at time $t$, $x_t$ is the input at time $t$, and $h_{(t-1)}$ is the hidden state of the previous layer at time $t-1$ or the initial hidden state at time $0$. If nonlinearity is 'relu', then ReLU is used in place of tanh.\n",
    "\n",
    "In a multi-layer RNN, the input $x_t^{(l)}$ of the $l$-th layer ($l \\ge 2$) is the hidden state $h_t^{(l-1)}$ of the previous layer. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -l -v -k \"test_rnn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit \"YOUR KEY HERE\" -k \"rnn\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Long short-term memory network [10 points]\n",
    "Implement - `Sigmoid`\n",
    "\n",
    "$\\sigma(x) = \\frac{1}{1 + \\text{exp}(-x)}$\n",
    "\n",
    "In `python/needle/nn.py`, implement `Sigmoid`, `LSTMCell` and `LSTM`.\n",
    "\n",
    "\\begin{align}\n",
    "i &= \\sigma(xW_{ii} + b_{ii} + hW_{hi} + b_{hi}) \\\\\n",
    "f &= \\sigma(xW_{if} + b_{if} + hW_{hf} + b_{hf}) \\\\\n",
    "g &= \\text{tanh}(xW_{ig} + b_{ig} + hW_{hg} + b_{hg}) \\\\\n",
    "o &= \\sigma(xW_{io} + b_{io} + hW_{ho} + b_{ho}) \\\\\n",
    "c^\\prime &= f * c + i * g \\\\\n",
    "h^\\prime &= o * \\text{tanh}(c^\\prime)\n",
    "\\end{align}\n",
    "\n",
    "where $\\sigma$ is the sigmoid function, and $i$, $f$, $g$, $o$ are the input, forget, cell, and output gates, respectively. \n",
    "\n",
    "All weights and biases should be initialized from $\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})$ where $k=\\frac{1}{\\text{hidden_size}}$.\n",
    "\n",
    "Now implement `LSTM` in `python/needle/nn.py`, which applies a multi-layer LSTM RNN to an input sequence. For each element in the input sequence, each layer computes the following function:\n",
    "\n",
    "\\begin{align}\n",
    "i_t &= \\sigma(x_tW_{ii} + b_{ii} + h_{(t-1)}W_{hi} + b_{hi}) \\\\\n",
    "f_t &= \\sigma(x_tW_{if} + b_{if} + h_{(t-1)}W_{hf} + b_{hf}) \\\\\n",
    "g_t &= \\text{tanh}(x_tW_{ig} + b_{ig} + h_{(t-1)}W_{hg} + b_{hg}) \\\\\n",
    "o_t &= \\sigma(x_tW_{io} + b_{io} + h_{(t-1)}W_{ho} + b_{ho}) \\\\\n",
    "c_t &= f * c_{(t-1)} + i * g \\\\\n",
    "h_t &= o * \\text{tanh}(c_t)\n",
    "\\end{align},\n",
    "where $h_t$ is the hidden state at time $t$, $c_t$ is the cell state at time $t$, $x_t$ is the input at time $t$, $h_{(t-1)}$ is the hidden state of the layer at time $t-1$ or the initial hidden state at time $0$, and $i_t$, $f_t$, $g_t$, $o_t$ are the input, forget, cell, and output gates at time $t$ respectively. \n",
    "\n",
    "In a multi-layer LSTM, the input $x_t^{(l)}$ of the $l$-th layer ($l \\ge 2$) is the hidden state $h_t^{(l-1)}$ of the previous layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -l -v -k \"test_lstm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit \"YOUR KEY HERE\" -k \"lstm\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Penn Treebank dataset [10 points]\n",
    "\n",
    "In word-level language modeling tasks, the model predicts the probability of the next word in the sequence, based on the words already observed in the sequence. You will write support for the Penn Treebank dataset, which consists of stories from the Wall Street Journal, to train and evaluate a language model on word-level prediction.\n",
    "\n",
    "In `python/needle/data.py`, start by implementing the `Dictionary` class, which creates a dictionary from a list of words, mapping each word to a unique integer.\n",
    "\n",
    "Next, we will use this `Dictionary` class to create a corpus from the train and test txt files in the Penn Treebank dataset that you downloaded at the beginning of the notebook. Implement the `tokenize` function in the `Corpus` class to do this.\n",
    "\n",
    "In order to prepare the data for training and evaluation, you will next implement the `batchify` function. Starting from sequential data, batchify arranges the dataset into columns. For instance, with the alphabet as the sequence and batch size 4, we'd get\n",
    "\n",
    "```\n",
    "┌ a g m s ┐\n",
    "│ b h n t │\n",
    "│ c i o u │\n",
    "│ d j p v │\n",
    "│ e k q w │\n",
    "└ f l r x ┘\n",
    "```\n",
    "\n",
    "These columns are treated as independent by the model, which means that the dependence of e. g. 'g' on 'f' cannot be learned, but allows more efficient batch processing.\n",
    "\n",
    "Next, implement the `get_batch` function. `get_batch` subdivides the source data into chunks of length `bptt`. If source is equal to the example output of the batchify function, with a bptt-limit of 2, we'd get the following two Variables for i = 0:\n",
    "```\n",
    "┌ a g m s ┐ ┌ b h n t ┐\n",
    "└ b h n t ┘ └ c i o u ┘\n",
    "```\n",
    "Note that despite the name of the function, the subdivison of data is not done along the batch dimension (i.e. dimension 1), since that was handled by the batchify function. The chunks are along dimension 0, corresponding to the seq_len dimension in the LSTM or RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -l -v -k \"ptb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit \"YOUR KEY HERE\" -k \"ptb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Training a word-level language model [10 points]\n",
    "\n",
    "Finally, you will use the `RNN` and `LSTM` components you have written to construct a language model that we will train on the Penn Treebank dataset.\n",
    "\n",
    "First, in `python/needle/nn.py` implement `Embedding`. Consider we have a dictionary with 1000 words. Then for a word which indexes into this dictionary, we can represent this word as a one-hot vector of size 1000, and then use a linear layer to project this to a vector of some embedding size.\n",
    "\n",
    "In `apps/models.py`, you can now implement `LanguageModel`. Your language model should consist of \n",
    "\n",
    "- An embedding layer (which maps word IDs to embeddings) \n",
    "- A sequence model (either RNN or LSTM)\n",
    "- A linear layer (which outputs probabilities of the next word)\n",
    "\n",
    "In `apps/simple_training.py` implement `epoch_general_ptb`, `train_ptb`, and `evaluate_ptb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -l -v -k \"language_model_implementation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -l -v -k \"language_model_training\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit \"YOUR KEY HERE\" -k \"language_model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can train your language model on the Penn Treebank dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import needle as ndl\n",
    "sys.path.append('./apps')\n",
    "from models import LanguageModel\n",
    "from simple_training import train_ptb, evaluate_ptb\n",
    "\n",
    "device = ndl.cpu()\n",
    "corpus = ndl.data.Corpus(\"data/ptb\")\n",
    "train_data = ndl.data.batchify(corpus.train, batch_size=16, device=ndl.cpu(), dtype=\"float32\")\n",
    "model = LanguageModel(30, len(corpus.dictionary), hidden_size=10, num_layers=2, seq_model='rnn', device=ndl.cpu())\n",
    "train_ptb(model, train_data, seq_len=1, n_epochs=1, device=device)\n",
    "evaluate_ptb(model, train_data, seq_len=40, device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
