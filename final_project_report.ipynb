{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ba6b023",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117997e0",
   "metadata": {},
   "source": [
    "We implement distributed training based on the Needle framework in our final project. In distributed training, the workload to train a model is split up and shared among multiple devices like GPUs, called nodes. These nodes work in parallel to speed up model training. The two main types of distributed training are data parallelism and model parallelism. In short, data parallelism divides the training data into partitions; model parallelism segments the model into different parts that can run concurrently in different nodes [1]. This project implmements the data parallism apporach. We'll elaborate a bit more about data parallelism in the following sections.\n",
    "\n",
    "In data parallelism, the training data is divided into partitions, where the number of partitions is equal to the total number of available nodes. The partitions are assigned to the available nodes.\n",
    "The model is copied in each of these nodes and each nodes operates on its own subset of the partition. Each node calculates the gradients of the model parameters independently. The calculated gradients of the nodes are aggragated to obtain the average gradients. Finally, each node updates the model parameters using the average gradients. \n",
    "\n",
    "Here we also give a brief explanation of the mathematical theory of data parallelism. Let $w$ be the parameters of the model; $\\frac{\\delta{L}}{\\delta{w}}$ is the original gradients of the batch of size $n$; $l_i$ is the loss for data point $i$ and $k$ is the number of nodes. Then we have\n",
    "$$\n",
    "\\frac{\\delta{L}}{\\delta{w}}=\\frac{\\delta[\\frac{1}{n}\\sum_{i=1}^{n}l_i]}{\\delta{w}} \\\\\n",
    "                              =\\frac{1}{n}\\sum_{i=1}^{n}\\frac{\\delta{l_i}}{\\delta{w}} \\\\\n",
    "                              =\\frac{m_1}{n}\\frac{\\frac{1}{m_1}\\sum_{i=1}^{m_1}l_i}{\\delta{w}} \n",
    "                               +\\frac{m_2}{n}\\frac{\\frac{1}{m_2}\\sum_{i=m_1+1}^{m_1+m2}l_i}{\\delta{w}}\n",
    "                               + \\dots\n",
    "                               + \\frac{m_k}{n}\\frac{\\frac{1}{m_k}\\sum_{i=m_{k-1}+1}^{m_{k-1}+m_{k}}l_i} {\\delta{w}} \\\\\n",
    "                              =\\frac{m_1}{n}\\frac{\\delta{l_1}}{\\delta{w}}+\\frac{m_2}{n}\\frac{\\delta{l_2}}{\\delta{w}}\n",
    "                              +\\dots+\\frac{m_k}{n}\\frac{\\delta{l_k}}{\\delta{w}}\n",
    "$$\n",
    "where $m_k$ is the number of data points assigned to node $k$, and \n",
    "$$\n",
    "m_1+m_2+\\dots+m_{k}=n\n",
    "$$\n",
    "If $m_1=m_2=\\dots=m_k=\\frac{n}{k}$, we have\n",
    "$$\n",
    "\\frac{\\delta{L}}{\\delta{w}}=\\frac{1}{k}[\\frac{\\delta{l_1}}{\\delta{w}}+\\frac{\\delta{l_2}}{\\delta{w}}+\\dots+\\frac{\\delta{l_k}}{\\delta{w}}]\n",
    "$$\n",
    "where $\\frac{\\delta{l_k}}{\\delta{w}}$ means the gradients calculated by node $k$ based on the data points $\\{m_{k-1}+1,m_{k-1}+2,\\dots,m_{k-1}+m_k\\}$.\n",
    "According to the above equation, we could know that the average gradients of all the nodes are equal to the original gradients [2]. \n",
    "\n",
    "The source code of the project can be found here: [TODO]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5bb6a7",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e875077",
   "metadata": {},
   "source": [
    "[1] Distributed training. https://learn.microsoft.com/en-us/azure/machine-learning/concept-distributed-training\n",
    "\n",
    "[2] Data Parallelism VS Model Parallelism in Distributed Deep Learning Training. https://leimao.github.io/blog/Data-Parallelism-vs-Model-Paralelism/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fab82d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
