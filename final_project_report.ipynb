{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ba6b023",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117997e0",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We implement distributed training based on the Needle framework in our final project. In distributed training, the workload to train a model is split up and shared among multiple devices like GPUs, called nodes. These nodes work in parallel to speed up model training. The two main types of distributed training are data parallelism and model parallelism. In short, data parallelism divides the training data into partitions; model parallelism segments the model into different parts that can run concurrently in different nodes [1]. This project implmements the data parallism apporach. We'll elaborate a bit more about data parallelism in the following sections.\n",
    "\n",
    "In data parallelism, the training data is divided into partitions, where the number of partitions is equal to the total number of available nodes. The partitions are assigned to the available nodes.\n",
    "The model is copied in each of these nodes and each nodes operates on its own subset of the partition. Each node calculates the gradients of the model parameters independently. The calculated gradients of the nodes are aggragated to obtain the average gradients. Finally, each node updates the model parameters using the average gradients. \n",
    "\n",
    "Here we also give a brief explanation of the mathematical theory of data parallelism. Let $w$ be the parameters of the model; $\\frac{\\delta{L}}{\\delta{w}}$ is the original gradients of the batch of size $n$; $l_i$ is the loss for data point $i$ and $k$ is the number of nodes. Then we have\n",
    "$$\n",
    "\\frac{\\delta{L}}{\\delta{w}}=\\frac{\\delta[\\frac{1}{n}\\sum_{i=1}^{n}l_i]}{\\delta{w}} \\\\\n",
    "                              =\\frac{1}{n}\\sum_{i=1}^{n}\\frac{\\delta{l_i}}{\\delta{w}} \\\\\n",
    "                              =\\frac{m_1}{n}\\frac{\\frac{1}{m_1}\\sum_{i=1}^{m_1}l_i}{\\delta{w}} \n",
    "                               +\\frac{m_2}{n}\\frac{\\frac{1}{m_2}\\sum_{i=m_1+1}^{m_1+m2}l_i}{\\delta{w}}\n",
    "                               + \\dots\n",
    "                               + \\frac{m_k}{n}\\frac{\\frac{1}{m_k}\\sum_{i=m_{k-1}+1}^{m_{k-1}+m_{k}}l_i} {\\delta{w}} \\\\\n",
    "                              =\\frac{m_1}{n}\\frac{\\delta{l_1}}{\\delta{w}}+\\frac{m_2}{n}\\frac{\\delta{l_2}}{\\delta{w}}\n",
    "                              +\\dots+\\frac{m_k}{n}\\frac{\\delta{l_k}}{\\delta{w}}\n",
    "$$\n",
    "where $m_k$ is the number of data points assigned to node $k$, and \n",
    "$$\n",
    "m_1+m_2+\\dots+m_{k}=n\n",
    "$$\n",
    "If $m_1=m_2=\\dots=m_k=\\frac{n}{k}$, we have\n",
    "$$\n",
    "\\frac{\\delta{L}}{\\delta{w}}=\\frac{1}{k}[\\frac{\\delta{l_1}}{\\delta{w}}+\\frac{\\delta{l_2}}{\\delta{w}}+\\dots+\\frac{\\delta{l_k}}{\\delta{w}}]\n",
    "$$\n",
    "where $\\frac{\\delta{l_k}}{\\delta{w}}$ means the gradients calculated by node $k$ based on the data points $\\{m_{k-1}+1,m_{k-1}+2,\\dots,m_{k-1}+m_k\\}$.\n",
    "According to the above equation, we could know that the average gradients of all the nodes are equal to the original gradients [2]. \n",
    "\n",
    "The source code of the project can be found here: [TODO]"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Usage"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this project, we tried to create the process similar to what horovod provides. The training process will take place in different process at the same time, and each process would communicate with each other through Message Passing Interface (MPI) protocol.\n",
    "\n",
    "Let's see how it works."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following lines of code should be inside a python file.\n",
    "Now we are trying to apply the distributed training for a ResNet9 model."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "sys.path.append('./python')\n",
    "import needle as ndl\n",
    "from apps.simple_training import train_cifar10, evaluate_cifar10\n",
    "from apps.models import ResNet9"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "After importing what we need from the basic needle framework, we now can import the ddp (distributed data parallel) from apps"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import apps.ddp as ddp"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here, we are going to initialize everything we need"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# this function initialize the ddp functionality\n",
    "# and return a desired cuda device\n",
    "rank, device = ddp.init()\n",
    "\n",
    "dataset = ndl.data.CIFAR10Dataset(\"data/cifar-10-batches-py\", train=True)\n",
    "\n",
    "#  this function do the partition for dataset and\n",
    "#  returns a dataloader and batch_size for the current process\n",
    "train_dataloader, bsz = ddp.partition_dataset(\n",
    "    dataset=dataset, batch_size=128, device=device, dtype='float32')\n",
    "\n",
    "model = ResNet9(device=device, dtype=\"float32\")\n",
    "#  Before training, we must broadcast the parameters to different process\n",
    "ddp.broadcast_parameters(model)\n",
    "\n",
    "model.train()\n",
    "opt = ndl.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.001)\n",
    "#  After defining the optimizer, we need to call this to\n",
    "#  make the optimizer work for distributed class\n",
    "opt  = ddp.DistributedOptimizer(opt)\n",
    "\n",
    "loss_fn = ndl.nn.SoftmaxLoss()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "After the initialization, the training step is very simple. Here we can see that the training process is similar to what we normally do in needle framework"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n_epochs = 1\n",
    "for i in range(n_epochs):\n",
    "    if rank == 0:\n",
    "        print(f'epoch: {i+1}/{n_epochs}')\n",
    "    for batch in train_dataloader:\n",
    "        opt.reset_grad()\n",
    "        X, y = batch\n",
    "        out = model(X)\n",
    "        correct = np.sum(np.argmax(out.numpy(), axis=1) == y.numpy())\n",
    "        loss = loss_fn(out, y)\n",
    "        loss.backward()\n",
    "        opt.step()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To run the script, we can do:\n",
    "mpiexec -np NUM_GPU python train_script.py"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we put the demo code into a python file"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "my_file = open(\"train_script.py\",\"w+\")\n",
    "my_file.write('''import sys\n",
    "import time\n",
    "from random import Random\n",
    "import numpy as np\n",
    "from mpi4py import MPI\n",
    "sys.path.append('./python')\n",
    "sys.path.append('./apps')\n",
    "from simple_training import train_cifar10, evaluate_cifar10\n",
    "from models import ResNet9\n",
    "import needle as ndl\n",
    "import ddp\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    np.random.seed(0)\n",
    "    rank, device = ddp.init()\n",
    "\n",
    "    dataset = ndl.data.CIFAR10Dataset(\"data/cifar-10-batches-py\", train=True)\n",
    "\n",
    "    train_set, bsz = ddp.partition_dataset(\n",
    "        dataset, 128, device=device, dtype='float32')\n",
    "    print(f'orignal dataset length: {len(dataset)}')\n",
    "\n",
    "    model = ResNet9(device=device, dtype=\"float32\")\n",
    "    ddp.broadcast_parameters(model)\n",
    "\n",
    "    model.train()\n",
    "    correct, total_loss = 0, 0\n",
    "    opt = ndl.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.001)\n",
    "    opt = ddp.DistributedOptimizer(opt)\n",
    "    loss_fn = ndl.nn.SoftmaxLoss()\n",
    "    n_epochs = 1\n",
    "    begin = time.time()\n",
    "    for i in range(n_epochs):\n",
    "        if rank == 0:\n",
    "            print(f'epoch: {i+1}/{n_epochs}')\n",
    "        count = 0\n",
    "        for batch in train_set:\n",
    "            opt.reset_grad()\n",
    "            X, y = batch\n",
    "            out = model(X)\n",
    "            correct = np.sum(np.argmax(out.numpy(), axis=1) == y.numpy())\n",
    "            loss = loss_fn(out, y)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            acc = correct/(y.shape[0])\n",
    "            if rank == 0 and count % 100 == 0:\n",
    "                print(f'acc: {acc}; avg_loss: {loss.data.numpy()}')\n",
    "            count += 1\n",
    "    end = time.time()\n",
    "    if rank == 0:\n",
    "        print(f'Training Time: {end-begin}')\n",
    "''')\n",
    "my_file.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using pytorch to find how many gpu available"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "num_of_gpus = torch.cuda.device_count()\n",
    "print(num_of_gpus)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we using this number to run the script"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: mpiexec\r\n"
     ]
    }
   ],
   "source": [
    "!mpiexec -np {num_of_gpus} python train_script.py"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# References"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "[1] Distributed training. https://learn.microsoft.com/en-us/azure/machine-learning/concept-distributed-training\n",
    "\n",
    "[2] Data Parallelism VS Model Parallelism in Distributed Deep Learning Training. https://leimao.github.io/blog/Data-Parallelism-vs-Model-Paralelism/\n",
    "\n",
    "[3] Horovod Framework: https://horovod.ai"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}